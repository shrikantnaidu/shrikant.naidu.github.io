<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>Dog Breed Classifier with CNN</title>

  <!-- mobile responsive meta -->
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <!-- theme meta -->
  <meta name="theme-name" content="parsa-jekyll" />
  
  <!-- ** Plugins Needed for the Project ** -->
  <!-- Bootstrap -->
  <link rel="stylesheet" href="/assets/plugins/bootstrap/bootstrap.min.css">
  <!-- slick slider -->
  <link rel="stylesheet" href="/assets/plugins/slick/slick.css">
  <!-- themefy-icon -->
  <link rel="stylesheet" href="/assets/plugins/themify-icons/themify-icons.css">

  <!-- Main Stylesheet -->
  <link href="/assets/css/style.css" rel="stylesheet">
  
  <!--Favicon-->
  <link rel="shortcut icon" href="/assets/images/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/assets/images/favicon.ico" type="image/x-icon">

</head>


<body>
  

  
  <!-- preloader -->
  <div class="preloader">
    <div class="loader">
      <span class="dot"></span>
      <div class="dots">
        <span></span>
        <span></span>
        <span></span>
      </div>
    </div>
  </div>
  <!-- /preloader -->
  

  <header class="navigation">
  <nav class="navbar navbar-expand-lg navbar-light">
    
    <a class="navbar-brand" href="http://localhost:4000/"><img class="img-fluid" src="/assets/images/s-logo.png" alt="Shrikant Naidu"></a>
    
    <button class="navbar-toggler border-0" type="button" data-toggle="collapse" data-target="#navogation"
      aria-controls="navogation" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse text-center" id="navogation">
      <ul class="navbar-nav ml-auto">
        
        
        <li class="nav-item">
          <a class="nav-link text-uppercase text-dark" href="/">Home</a>
        </li>
        
        
        
        <li class="nav-item dropdown">
          <a class="nav-link text-uppercase text-dark dropdown-toggle" href="#" id="navbarDropdown"
            role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
            Projects
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdown">
            
            <a class="dropdown-item" href="/DL/">Deep Learning</a>
            
          </div>
        </li>
        
        
        
        <li class="nav-item">
          <a class="nav-link text-uppercase text-dark" href="/about/">About</a>
        </li>
        
        
        
        <li class="nav-item">
          <a class="nav-link text-uppercase text-dark" href="/contact/">Contact</a>
        </li>
        
        
      </ul>
      <form class="form-inline position-relative ml-lg-4" action="/search.html" method="get">
        <input class="form-control px-0 w-100" type="search" id="search-box" name="query" placeholder="Search">
        <button class="search-icon" type="submit"><i class="ti-search text-dark"></i></button>
      </form>
    </div>
  </nav>
</header>

    <!-- page-title -->
<section class="section bg-secondary">
	<div class="container">
		<div class="row">
			<div class="col-lg-12">
				<h4>Dog Breed Classifier with CNN</h4>
			</div>
		</div>
	</div>
</section>
<!-- /page-title -->

<!-- blog single -->
<section>
	<div class="container">
		<div class="row">
			<div class="col-lg-8">
				<ul class="list-inline d-flex justify-content-between py-3">
					<li class="list-inline-item"><i class="ti-user mr-2"></i>Post by Shrikant Naidu</li>
					<li class="list-inline-item"><i class="ti-calendar mr-2"></i>Jun 04, 2020</li>
				</ul>
				
				<img src="/assets/images/masonary-post/dog-breed.jpg" alt="Dog Breed Classifier with CNN" class="w-100 img-fluid mb-4">
				
				<div class="content">
					<h1 id="convolutional-neural-networks">Convolutional Neural Networks</h1>

<hr />
<h3 id="why-were-here">Why We’re Here</h3>

<p>In this notebook, you will make the first steps towards developing an algorithm that could be used as part of a mobile or web app.  At the end of this project, your code will accept any user-supplied image as input.  If a dog is detected in the image, it will provide an estimate of the dog’s breed.  If a human is detected, it will provide an estimate of the dog breed that is most resembling.  The image below displays potential sample output of your finished project (… but we expect that each student’s algorithm will behave differently!).</p>

<p><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/sample_dog_output.png" alt="Sample Dog Output" /></p>

<p>In this real-world setting, you will need to piece together a series of models to perform different tasks; for instance, the algorithm that detects humans in an image will be different from the CNN that infers dog breed.  There are many points of possible failure, and no perfect algorithm exists.  Your imperfect solution will nonetheless create a fun user experience!</p>

<h3 id="the-road-ahead">The Road Ahead</h3>

<p>We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.</p>

<ul>
  <li><a href="#step0">Step 0</a>: Import Datasets</li>
  <li><a href="#step1">Step 1</a>: Detect Humans</li>
  <li><a href="#step2">Step 2</a>: Detect Dogs</li>
  <li><a href="#step3">Step 3</a>: Create a CNN to Classify Dog Breeds (from Scratch)</li>
  <li><a href="#step4">Step 4</a>: Create a CNN to Classify Dog Breeds (using Transfer Learning)</li>
  <li><a href="#step5">Step 5</a>: Write your Algorithm</li>
  <li><a href="#step6">Step 6</a>: Test Your Algorithm</li>
</ul>

<hr />
<p><a id="step0"></a></p>
<h2 id="step-0-import-datasets">Step 0: Import Datasets</h2>

<p>Make sure that you’ve downloaded the required human and dog datasets:</p>

<p><strong>Note: if you are using the Udacity workspace, you <em>DO NOT</em> need to re-download these - they can be found in the <code class="language-plaintext highlighter-rouge">/data</code> folder as noted in the cell below.</strong></p>

<ul>
  <li>
    <p>Download the <a href="https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip">dog dataset</a>.  Unzip the folder and place it in this project’s home directory, at the location <code class="language-plaintext highlighter-rouge">/dog_images</code>.</p>
  </li>
  <li>
    <p>Download the <a href="https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/lfw.zip">human dataset</a>.  Unzip the folder and place it in the home directory, at location <code class="language-plaintext highlighter-rouge">/lfw</code>.</p>
  </li>
</ul>

<p><em>Note: If you are using a Windows machine, you are encouraged to use <a href="http://www.7-zip.org/">7zip</a> to extract the folder.</em></p>

<p>In the code cell below, we save the file paths for both the human (LFW) dataset and dog dataset in the numpy arrays <code class="language-plaintext highlighter-rouge">human_files</code> and <code class="language-plaintext highlighter-rouge">dog_files</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">glob</span> <span class="kn">import</span> <span class="n">glob</span>

<span class="c1"># load filenames for human and dog images
</span><span class="n">human_files</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nf">glob</span><span class="p">(</span><span class="s">"/data/lfw/*/*"</span><span class="p">))</span>
<span class="n">dog_files</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nf">glob</span><span class="p">(</span><span class="s">"/data/dog_images/*/*/*"</span><span class="p">))</span>

<span class="c1"># print number of images in each dataset
</span><span class="nf">print</span><span class="p">(</span><span class="s">'There are %d total human images.'</span> <span class="o">%</span> <span class="nf">len</span><span class="p">(</span><span class="n">human_files</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="s">'There are %d total dog images.'</span> <span class="o">%</span> <span class="nf">len</span><span class="p">(</span><span class="n">dog_files</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>There are 13233 total human images.
There are 8351 total dog images.
</code></pre></div></div>

<p><a id="step1"></a></p>
<h2 id="step-1-detect-humans">Step 1: Detect Humans</h2>

<p>In this section, we use OpenCV’s implementation of <a href="http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html">Haar feature-based cascade classifiers</a> to detect human faces in images.</p>

<p>OpenCV provides many pre-trained face detectors, stored as XML files on <a href="https://github.com/opencv/opencv/tree/master/data/haarcascades">github</a>.  We have downloaded one of these detectors and stored it in the <code class="language-plaintext highlighter-rouge">haarcascades</code> directory.  In the next code cell, we demonstrate how to use this detector to find human faces in a sample image.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">cv2</span>                
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>                        
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>                               

<span class="c1"># extract pre-trained face detector
</span><span class="n">face_cascade</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nc">CascadeClassifier</span><span class="p">(</span><span class="s">'haarcascades/haarcascade_frontalface_alt.xml'</span><span class="p">)</span>

<span class="c1"># load color (BGR) image
</span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">imread</span><span class="p">(</span><span class="n">human_files</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="c1"># convert BGR image to grayscale
</span><span class="n">gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>

<span class="c1"># find faces in image
</span><span class="n">faces</span> <span class="o">=</span> <span class="n">face_cascade</span><span class="p">.</span><span class="nf">detectMultiScale</span><span class="p">(</span><span class="n">gray</span><span class="p">)</span>

<span class="c1"># print number of faces detected in the image
</span><span class="nf">print</span><span class="p">(</span><span class="s">'Number of faces detected:'</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">faces</span><span class="p">))</span>

<span class="c1"># get bounding box for each detected face
</span><span class="nf">for </span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">h</span><span class="p">)</span> <span class="ow">in</span> <span class="n">faces</span><span class="p">:</span>
    <span class="c1"># add bounding box to color image
</span>    <span class="n">cv2</span><span class="p">.</span><span class="nf">rectangle</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">),(</span><span class="n">x</span><span class="o">+</span><span class="n">w</span><span class="p">,</span><span class="n">y</span><span class="o">+</span><span class="n">h</span><span class="p">),(</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="mi">2</span><span class="p">)</span>
    
<span class="c1"># convert BGR image to RGB for plotting
</span><span class="n">cv_rgb</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>

<span class="c1"># display the image, along with bounding box
</span><span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">cv_rgb</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Number of faces detected: 1
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/output_3_1.png" alt="png" /></p>

<p>Before using any of the face detectors, it is standard procedure to convert the images to grayscale.  The <code class="language-plaintext highlighter-rouge">detectMultiScale</code> function executes the classifier stored in <code class="language-plaintext highlighter-rouge">face_cascade</code> and takes the grayscale image as a parameter.</p>

<p>In the above code, <code class="language-plaintext highlighter-rouge">faces</code> is a numpy array of detected faces, where each row corresponds to a detected face.  Each detected face is a 1D array with four entries that specifies the bounding box of the detected face.  The first two entries in the array (extracted in the above code as <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code>) specify the horizontal and vertical positions of the top left corner of the bounding box.  The last two entries in the array (extracted here as <code class="language-plaintext highlighter-rouge">w</code> and <code class="language-plaintext highlighter-rouge">h</code>) specify the width and height of the box.</p>

<h3 id="write-a-human-face-detector">Write a Human Face Detector</h3>

<p>We can use this procedure to write a function that returns <code class="language-plaintext highlighter-rouge">True</code> if a human face is detected in an image and <code class="language-plaintext highlighter-rouge">False</code> otherwise.  This function, aptly named <code class="language-plaintext highlighter-rouge">face_detector</code>, takes a string-valued file path to an image as input and appears in the code block below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># returns "True" if face is detected in image stored at img_path
</span><span class="k">def</span> <span class="nf">face_detector</span><span class="p">(</span><span class="n">img_path</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">imread</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>
    <span class="n">gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv2</span><span class="p">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>
    <span class="n">faces</span> <span class="o">=</span> <span class="n">face_cascade</span><span class="p">.</span><span class="nf">detectMultiScale</span><span class="p">(</span><span class="n">gray</span><span class="p">)</span>
    <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">faces</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
</code></pre></div></div>

<h3 id="implementation-assess-the-human-face-detector">(IMPLEMENTATION) Assess the Human Face Detector</h3>

<p><strong>Question 1:</strong> Use the code cell below to test the performance of the <code class="language-plaintext highlighter-rouge">face_detector</code> function.</p>
<ul>
  <li>What percentage of the first 100 images in <code class="language-plaintext highlighter-rouge">human_files</code> have a detected human face?</li>
  <li>What percentage of the first 100 images in <code class="language-plaintext highlighter-rouge">dog_files</code> have a detected human face?</li>
</ul>

<p>Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face.  You will see that our algorithm falls short of this goal, but still gives acceptable performance.  We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays <code class="language-plaintext highlighter-rouge">human_files_short</code> and <code class="language-plaintext highlighter-rouge">dog_files_short</code>.</p>

<p><strong>Answer:</strong> Percentage of Human Faces detected in human_files is 98% whereas Percentage of Human Faces detected in dog_files is 17%</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="n">human_files_short</span> <span class="o">=</span> <span class="n">human_files</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span>
<span class="n">dog_files_short</span> <span class="o">=</span> <span class="n">dog_files</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span>

<span class="c1">#-#-# Do NOT modify the code above this line. #-#-#
</span>
<span class="c1">## TODO: Test the performance of the face_detector algorithm 
## on the images in human_files_short and dog_files_short.
</span><span class="n">face_detected_in_human_files</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">face_detected_in_dog_files</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">images</span> <span class="ow">in</span> <span class="n">human_files_short</span><span class="p">:</span>
    <span class="k">if</span> <span class="nf">face_detector</span><span class="p">(</span><span class="n">images</span><span class="p">):</span>
        <span class="n">face_detected_in_human_files</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="k">for</span> <span class="n">images</span> <span class="ow">in</span> <span class="n">dog_files_short</span><span class="p">:</span>
    <span class="k">if</span> <span class="nf">face_detector</span><span class="p">(</span><span class="n">images</span><span class="p">):</span>
        <span class="n">face_detected_in_dog_files</span> <span class="o">+=</span> <span class="mi">1</span>

        
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Percentage of Human Faces detected in human_files : </span><span class="si">{</span><span class="n">face_detected_in_human_files</span><span class="si">}</span><span class="s">%"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Percentage of Human Faces detected in dog_files : </span><span class="si">{</span><span class="n">face_detected_in_dog_files</span><span class="si">}</span><span class="s">%"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Percentage of Human Faces detected in human_files : 98%
Percentage of Human Faces detected in dog_files : 17%
</code></pre></div></div>

<p>We suggest the face detector from OpenCV as a potential way to detect human images in your algorithm, but you are free to explore other approaches, especially approaches that make use of deep learning :).  Please use the code cell below to design and test your own face detection algorithm.  If you decide to pursue this <em>optional</em> task, report performance on <code class="language-plaintext highlighter-rouge">human_files_short</code> and <code class="language-plaintext highlighter-rouge">dog_files_short</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">### (Optional) 
### TODO: Test performance of anotherface detection algorithm.
### Feel free to use as many code cells as needed.
</span></code></pre></div></div>

<hr />
<p><a id="step2"></a></p>
<h2 id="step-2-detect-dogs">Step 2: Detect Dogs</h2>

<p>In this section, we use a <a href="http://pytorch.org/docs/master/torchvision/models.html">pre-trained model</a> to detect dogs in images.</p>

<h3 id="obtain-pre-trained-vgg-16-model">Obtain Pre-trained VGG-16 Model</h3>

<p>The code cell below downloads the VGG-16 model, along with weights that have been trained on <a href="http://www.image-net.org/">ImageNet</a>, a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of <a href="https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a">1000 categories</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torchvision.models</span> <span class="k">as</span> <span class="n">models</span>

<span class="c1"># define VGG16 model
</span><span class="n">VGG16</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="nf">vgg16</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># check if CUDA is available
</span><span class="n">use_cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>

<span class="c1"># move model to GPU if CUDA is available
</span><span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span>
    <span class="n">VGG16</span> <span class="o">=</span> <span class="n">VGG16</span><span class="p">.</span><span class="nf">cuda</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Downloading: "https://download.pytorch.org/models/vgg16-397923af.pth" to /root/.torch/models/vgg16-397923af.pth
100%|██████████| 553433881/553433881 [00:05&lt;00:00, 99176335.73it/s] 
</code></pre></div></div>

<p>Given an image, this pre-trained VGG-16 model returns a prediction (derived from the 1000 possible categories in ImageNet) for the object that is contained in the image.</p>

<h3 id="implementation-making-predictions-with-a-pre-trained-model">(IMPLEMENTATION) Making Predictions with a Pre-trained Model</h3>

<p>In the next code cell, you will write a function that accepts a path to an image (such as <code class="language-plaintext highlighter-rouge">'dogImages/train/001.Affenpinscher/Affenpinscher_00001.jpg'</code>) as input and returns the index corresponding to the ImageNet class that is predicted by the pre-trained VGG-16 model.  The output should always be an integer between 0 and 999, inclusive.</p>

<p>Before writing the function, make sure that you take the time to learn  how to appropriately pre-process tensors for pre-trained models in the <a href="http://pytorch.org/docs/stable/torchvision/models.html">PyTorch documentation</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="n">torchvision.transforms</span> <span class="k">as</span> <span class="n">transforms</span>

<span class="k">def</span> <span class="nf">VGG16_predict</span><span class="p">(</span><span class="n">img_path</span><span class="p">):</span>
    <span class="s">'''
    Use pre-trained VGG-16 model to obtain index corresponding to 
    predicted ImageNet class for image at specified path
    
    Args:
        img_path: path to an image
        
    Returns:
        Index corresponding to VGG-16 model's prediction
    '''</span>
    
    <span class="c1">## TODO: Complete the function.
</span>    <span class="c1">## Load and pre-process an image from the given img_path
</span>    <span class="c1">## Return the *index* of the predicted class for that image
</span>    <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">img_path</span><span class="p">).</span><span class="nf">convert</span><span class="p">(</span><span class="s">'RGB'</span><span class="p">)</span>
    
    <span class="n">data_transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="p">.</span><span class="nc">Resize</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
                                         <span class="n">transforms</span><span class="p">.</span><span class="nc">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
                                         <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
                                         <span class="n">transforms</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
                                                              <span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])])</span>
    
    <span class="n">image</span> <span class="o">=</span> <span class="nf">data_transform</span><span class="p">(</span><span class="n">image</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="nf">cuda</span><span class="p">()</span>
    <span class="n">predict</span> <span class="o">=</span> <span class="nc">VGG16</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="n">predicted_class</span> <span class="o">=</span> <span class="n">predict</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">argmax</span><span class="p">()</span>
    
    
    <span class="k">return</span> <span class="nf">int</span><span class="p">(</span><span class="n">predicted_class</span><span class="p">)</span> <span class="c1"># predicted class index
</span></code></pre></div></div>

<h3 id="implementation-write-a-dog-detector">(IMPLEMENTATION) Write a Dog Detector</h3>

<p>While looking at the <a href="https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a">dictionary</a>, you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from <code class="language-plaintext highlighter-rouge">'Chihuahua'</code> to <code class="language-plaintext highlighter-rouge">'Mexican hairless'</code>.  Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained VGG-16 model, we need only check if the pre-trained model predicts an index between 151 and 268 (inclusive).</p>

<p>Use these ideas to complete the <code class="language-plaintext highlighter-rouge">dog_detector</code> function below, which returns <code class="language-plaintext highlighter-rouge">True</code> if a dog is detected in an image (and <code class="language-plaintext highlighter-rouge">False</code> if not).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">### returns "True" if a dog is detected in the image stored at img_path
</span><span class="k">def</span> <span class="nf">dog_detector</span><span class="p">(</span><span class="n">img_path</span><span class="p">):</span>
    <span class="c1">## TODO: Complete the function.
</span>    <span class="n">class_label</span> <span class="o">=</span> <span class="nc">VGG16_predict</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>
    
    <span class="nf">return </span><span class="p">(</span> <span class="p">(</span><span class="n">class_label</span> <span class="o">&gt;=</span> <span class="mi">151</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">class_label</span> <span class="o">&lt;=</span>  <span class="mi">268</span><span class="p">))</span>  <span class="c1"># true/false
</span></code></pre></div></div>

<h3 id="implementation-assess-the-dog-detector">(IMPLEMENTATION) Assess the Dog Detector</h3>

<p><strong>Question 2:</strong> Use the code cell below to test the performance of your <code class="language-plaintext highlighter-rouge">dog_detector</code> function.</p>
<ul>
  <li>What percentage of the images in <code class="language-plaintext highlighter-rouge">human_files_short</code> have a detected dog?</li>
  <li>What percentage of the images in <code class="language-plaintext highlighter-rouge">dog_files_short</code> have a detected dog?</li>
</ul>

<p><strong>Answer:</strong> Dog Images in Human dataset is 1% whereas Dog Images in Dog Dataset is 100%</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">### TODO: Test the performance of the dog_detector function
### on the images in human_files_short and dog_files_short.
</span><span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>


<span class="c1"># Initiallizing:
</span>
<span class="n">dog_images_in_human_files</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">dog_images_in_dog_files</span> <span class="o">=</span> <span class="mi">0</span>


<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">human_files_short</span><span class="p">))):</span>
    <span class="k">if</span> <span class="nf">dog_detector</span><span class="p">(</span><span class="n">human_files_short</span><span class="p">[</span><span class="n">i</span><span class="p">]):</span>
        <span class="n">dog_images_in_human_files</span> <span class="o">+=</span> <span class="mi">1</span>
        
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">dog_files_short</span><span class="p">))):</span>
    <span class="k">if</span> <span class="nf">dog_detector</span><span class="p">(</span><span class="n">dog_files_short</span><span class="p">[</span><span class="n">i</span><span class="p">]):</span>
        <span class="n">dog_images_in_dog_files</span> <span class="o">+=</span> <span class="mi">1</span>

    
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Dog Images in Human dataset: </span><span class="si">{</span><span class="n">dog_images_in_human_files</span><span class="si">}</span><span class="s">%'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Dog Images in Dog Dataset: </span><span class="si">{</span><span class="n">dog_images_in_dog_files</span><span class="si">}</span><span class="s">%'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 100/100 [00:03&lt;00:00, 28.38it/s]
100%|██████████| 100/100 [00:04&lt;00:00, 25.46it/s]

Dog Images in Human dataset: 1%
Dog Images in Dog Dataset: 100%
</code></pre></div></div>

<p>We suggest VGG-16 as a potential network to detect dog images in your algorithm, but you are free to explore other pre-trained networks (such as <a href="http://pytorch.org/docs/master/torchvision/models.html#inception-v3">Inception-v3</a>, <a href="http://pytorch.org/docs/master/torchvision/models.html#id3">ResNet-50</a>, etc).  Please use the code cell below to test other pre-trained PyTorch models.  If you decide to pursue this <em>optional</em> task, report performance on <code class="language-plaintext highlighter-rouge">human_files_short</code> and <code class="language-plaintext highlighter-rouge">dog_files_short</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">### (Optional) 
### TODO: Report the performance of another pre-trained network.
### Feel free to use as many code cells as needed.
</span></code></pre></div></div>

<hr />
<p><a id="step3"></a></p>
<h2 id="step-3-create-a-cnn-to-classify-dog-breeds-from-scratch">Step 3: Create a CNN to Classify Dog Breeds (from Scratch)</h2>

<p>Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images.  In this step, you will create a CNN that classifies dog breeds.  You must create your CNN <em>from scratch</em> (so, you can’t use transfer learning <em>yet</em>!), and you must attain a test accuracy of at least 10%.  In Step 4 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.</p>

<p>We mention that the task of assigning breed to dogs from images is considered exceptionally challenging.  To see why, consider that <em>even a human</em> would have trouble distinguishing between a Brittany and a Welsh Springer Spaniel.</p>

<table>
  <thead>
    <tr>
      <th>Brittany</th>
      <th>Welsh Springer Spaniel</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/Brittany_02625.jpg" width="100" /></td>
      <td><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/Welsh_springer_spaniel_08203.jpg" width="200" /></td>
    </tr>
  </tbody>
</table>

<p>It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).</p>

<table>
  <thead>
    <tr>
      <th>Curly-Coated Retriever</th>
      <th>American Water Spaniel</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/Curly-coated_retriever_03896.jpg" width="200" /></td>
      <td><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/American_water_spaniel_00648.jpg" width="200" /></td>
    </tr>
  </tbody>
</table>

<p>Likewise, recall that labradors come in yellow, chocolate, and black.  Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.</p>

<table>
  <thead>
    <tr>
      <th>Yellow Labrador</th>
      <th>Chocolate Labrador</th>
      <th>Black Labrador</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/Labrador_retriever_06457.jpg" width="150" /></td>
      <td><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/Labrador_retriever_06455.jpg" width="240" /></td>
      <td><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/Labrador_retriever_06449.jpg" width="220" /></td>
    </tr>
  </tbody>
</table>

<p>We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.</p>

<p>Remember that the practice is far ahead of the theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun!</p>

<h3 id="implementation-specify-data-loaders-for-the-dog-dataset">(IMPLEMENTATION) Specify Data Loaders for the Dog Dataset</h3>

<p>Use the code cell below to write three separate <a href="http://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">data loaders</a> for the training, validation, and test datasets of dog images (located at <code class="language-plaintext highlighter-rouge">dog_images/train</code>, <code class="language-plaintext highlighter-rouge">dog_images/valid</code>, and <code class="language-plaintext highlighter-rouge">dog_images/test</code>, respectively).  You may find <a href="http://pytorch.org/docs/stable/torchvision/datasets.html">this documentation on custom datasets</a> to be a useful resource.  If you are interested in augmenting your training and/or validation data, check out the wide variety of <a href="http://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transform">transforms</a>!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="n">PIL</span> <span class="kn">import</span> <span class="n">ImageFile</span>
<span class="n">ImageFile</span><span class="p">.</span><span class="n">LOAD_TRUNCATED_IMAGES</span> <span class="o">=</span> <span class="bp">True</span>

<span class="c1">### TODO: Write data loaders for training, validation, and test sets
## Specify appropriate transforms, and batch_sizes
</span><span class="n">num_workers</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">24</span>

<span class="n">data_dir</span> <span class="o">=</span> <span class="s">'/data/dog_images/'</span>
<span class="n">train_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s">'train/'</span><span class="p">)</span>
<span class="n">test_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s">'test/'</span><span class="p">)</span>
<span class="n">valid_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s">'valid/'</span><span class="p">)</span>


<span class="n">data_transforms</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s">'train'</span> <span class="p">:</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">RandomRotation</span><span class="p">(</span><span class="mi">30</span><span class="p">),</span>
                                                   <span class="n">transforms</span><span class="p">.</span><span class="nc">RandomResizedCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
                                                   <span class="n">transforms</span><span class="p">.</span><span class="nc">RandomHorizontalFlip</span><span class="p">(),</span>
                                                   <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
                                                   <span class="n">transforms</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
                                                                        <span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])]),</span>
                   
                    <span class="s">'test'</span> <span class="p">:</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Resize</span><span class="p">(</span><span class="mi">255</span><span class="p">),</span>
                                                  <span class="n">transforms</span><span class="p">.</span><span class="nc">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
                                                  <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
                                                  <span class="n">transforms</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
                                                                       <span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])]),</span>
                   <span class="s">'valid'</span> <span class="p">:</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Resize</span><span class="p">(</span><span class="mi">255</span><span class="p">),</span>
                                                  <span class="n">transforms</span><span class="p">.</span><span class="nc">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
                                                  <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
                                                  <span class="n">transforms</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
                                                                       <span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])])</span>
                  <span class="p">}</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="p">{</span>   
            <span class="s">'train_data'</span> <span class="p">:</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">ImageFolder</span><span class="p">(</span><span class="n">train_dir</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">data_transforms</span><span class="p">[</span><span class="s">'train'</span><span class="p">]),</span>
            <span class="s">'test_data'</span> <span class="p">:</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">ImageFolder</span><span class="p">(</span><span class="n">test_dir</span><span class="p">,</span><span class="n">transform</span><span class="o">=</span><span class="n">data_transforms</span><span class="p">[</span><span class="s">'test'</span><span class="p">]),</span>
            <span class="s">'val_data'</span> <span class="p">:</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">ImageFolder</span><span class="p">(</span><span class="n">valid_dir</span><span class="p">,</span><span class="n">transform</span><span class="o">=</span><span class="n">data_transforms</span><span class="p">[</span><span class="s">'valid'</span><span class="p">])</span>
            <span class="p">}</span>

<span class="n">loaders_scratch</span>  <span class="o">=</span> <span class="p">{</span> 
                        <span class="s">'train'</span> <span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s">'train_data'</span><span class="p">],</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
                                                               <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
                        <span class="s">'test'</span> <span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s">'test_data'</span><span class="p">],</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
                                                              <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
                        <span class="s">'valid'</span> <span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s">'val_data'</span><span class="p">],</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
                                                              <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                    <span class="p">}</span>
</code></pre></div></div>

<p><strong>Question 3:</strong> Describe your chosen procedure for preprocessing the data.</p>
<ul>
  <li>How does your code resize the images (by cropping, stretching, etc)?  What size did you pick for the input tensor, and why?</li>
  <li>Did you decide to augment the dataset?  If so, how (through translations, flips, rotations, etc)?  If not, why not?</li>
</ul>

<p><strong>Answer</strong>:</p>
<ol>
  <li>Since I have decided to keep the same data loader throughout the project,transforming the images to the input size that is accepted by a pre-trained model is essential so the size of the image is resized to 224 x 224.</li>
  <li>Data Augmentation improves the performance of models so I have used a Random Rotation and Horizontal Flip along with RandomResizedCrop which will randomly crop the original image and it’s aspect ratio and then resize it to the desired size mentioned.</li>
  <li>Since Normalization of the Input tensors speed the training as well as improve the performance I normalized the data with standard values used in transfer learning.</li>
</ol>

<h3 id="implementation-model-architecture">(IMPLEMENTATION) Model Architecture</h3>

<p>Create a CNN to classify dog breed.  Use the template in the code cell below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="c1"># define the CNN architecture
</span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="c1">### TODO: choose an architecture, and complete the class
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="c1">## Define layers of a CNN
</span>        <span class="c1"># convolutional layer (sees 32x32x3 image tensor)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># convolutional layer (sees 16x16x16 tensor)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># convolutional layer (sees 8x8x32 tensor)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># max pooling layer
</span>        <span class="n">self</span><span class="p">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># linear layer (64 * 4 * 4 -&gt; 500)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">64</span> <span class="o">*</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
        <span class="c1"># linear layer (500 -&gt; 10)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">133</span><span class="p">)</span>
        <span class="c1"># dropout layer (p=0.25)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1">## Define forward behavior
</span>        <span class="c1"># Convolution Layers
</span>                
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pool</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pool</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pool</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="c1"># flatten image input
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span> <span class="o">*</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">)</span>
        <span class="c1"># add dropout layer
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># add 1st hidden layer, with relu activation function
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="c1"># add dropout layer
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># add 2nd hidden layer, with relu activation function
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
      
        <span class="k">return</span> <span class="n">x</span>

<span class="c1">#-#-# You so NOT have to modify the code below this line. #-#-#
</span>
<span class="c1"># instantiate the CNN
</span><span class="n">model_scratch</span> <span class="o">=</span> <span class="nc">Net</span><span class="p">()</span>

<span class="c1"># move tensors to GPU if CUDA is available
</span><span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span>
    <span class="n">model_scratch</span><span class="p">.</span><span class="nf">cuda</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Question 4:</strong> Outline the steps you took to get to your final CNN architecture and your reasoning at each step.</p>

<p><strong>Answer:</strong></p>
<ol>
  <li>The class labels have a length of 133 which means the output of last dense layers will be 133.</li>
  <li>The input is a color image which is resized into a 224x224x3 pixels i.e a depth of 3 and x-y dimensions of 224x224, so input channel of the first convolution layer is 3(depth).</li>
  <li>As we computation in high dimensionality will slow the training process, Max Pooling layer that reduces the x-y dimensions is used along with a convolution kernel of size 3 i.e 3x3.This setting will half the x-y dimension in the forward pass.</li>
  <li>Each convolution layer had the depth more than the previous layer to capture complex patterns.</li>
  <li>Every convolution layer had a relu activation and used a max pooling layer after every activation.</li>
  <li>The first Dense layer was attached with a droput of 25% to avoid overfitting followed by an relu activation.</li>
  <li>The last dense layer did not follow an activation function since Cross Entropy loss function is used as the loss function.</li>
</ol>

<h3 id="implementation-specify-loss-function-and-optimizer">(IMPLEMENTATION) Specify Loss Function and Optimizer</h3>

<p>Use the next code cell to specify a <a href="http://pytorch.org/docs/stable/nn.html#loss-functions">loss function</a> and <a href="http://pytorch.org/docs/stable/optim.html">optimizer</a>.  Save the chosen loss function as <code class="language-plaintext highlighter-rouge">criterion_scratch</code>, and the optimizer as <code class="language-plaintext highlighter-rouge">optimizer_scratch</code> below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="c1">### TODO: select loss function
</span><span class="n">criterion_scratch</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>

<span class="c1">### TODO: select optimizer
</span><span class="n">optimizer_scratch</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">model_scratch</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.03</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="implementation-train-and-validate-the-model">(IMPLEMENTATION) Train and Validate the Model</h3>

<p>Train and validate your model in the code cell below.  <a href="http://pytorch.org/docs/master/notes/serialization.html">Save the final model parameters</a> at filepath <code class="language-plaintext highlighter-rouge">'model_scratch.pt'</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">,</span> <span class="n">loaders</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">use_cuda</span><span class="p">,</span> <span class="n">save_path</span><span class="p">):</span>
    <span class="s">"""returns trained model"""</span>
    <span class="c1"># initialize tracker for minimum validation loss
</span>    <span class="n">valid_loss_min</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">Inf</span> 
    
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="c1"># initialize variables to monitor training and validation loss
</span>        <span class="n">train_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">valid_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        
        <span class="c1">###################
</span>        <span class="c1"># train the model #
</span>        <span class="c1">###################
</span>        <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">loaders</span><span class="p">[</span><span class="s">'train'</span><span class="p">]):</span>
            <span class="c1"># move to GPU
</span>            <span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span>
                <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">cuda</span><span class="p">(),</span> <span class="n">target</span><span class="p">.</span><span class="nf">cuda</span><span class="p">()</span>
            <span class="c1">## find the loss and update the model parameters accordingly
</span>            <span class="c1">## record the average training loss, using something like
</span>            <span class="c1">## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))
</span>            <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
            <span class="c1"># forward pass: compute predicted outputs by passing inputs to the model
</span>            <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="c1"># calculate the batch loss
</span>            <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
            <span class="c1"># backward pass: compute gradient of the loss with respect to model parameters
</span>            <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
            <span class="c1"># perform a single optimization step (parameter update)
</span>            <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
            <span class="c1"># update training loss
</span>            <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="o">*</span><span class="n">data</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            
            
        <span class="c1">######################    
</span>        <span class="c1"># validate the model #
</span>        <span class="c1">######################
</span>        <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">loaders</span><span class="p">[</span><span class="s">'valid'</span><span class="p">]):</span>
            <span class="c1"># move to GPU
</span>            <span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span>
                <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">cuda</span><span class="p">(),</span> <span class="n">target</span><span class="p">.</span><span class="nf">cuda</span><span class="p">()</span>
            <span class="c1">## update the average validation loss
</span>            <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
            <span class="c1"># update average validation loss 
</span>            <span class="n">valid_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="o">*</span><span class="n">data</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># calculate average losses
</span>        <span class="n">train_loss</span> <span class="o">=</span> <span class="n">train_loss</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">loaders</span><span class="p">[</span><span class="s">'train'</span><span class="p">].</span><span class="n">dataset</span><span class="p">)</span>
        <span class="n">valid_loss</span> <span class="o">=</span> <span class="n">valid_loss</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">loaders</span><span class="p">[</span><span class="s">'valid'</span><span class="p">].</span><span class="n">dataset</span><span class="p">)</span>
        
        <span class="c1"># print training/validation statistics 
</span>        <span class="nf">print</span><span class="p">(</span><span class="s">'Epoch: {} </span><span class="se">\t</span><span class="s">Training Loss: {:.6f} </span><span class="se">\t</span><span class="s">Validation Loss: {:.6f}'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span>
            <span class="n">epoch</span><span class="p">,</span> 
            <span class="n">train_loss</span><span class="p">,</span>
            <span class="n">valid_loss</span>
            <span class="p">))</span>
        
        <span class="c1">## TODO: save the model if validation loss has decreased
</span>        <span class="k">if</span> <span class="n">valid_loss</span> <span class="o">&lt;=</span> <span class="n">valid_loss_min</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="s">'Validation loss decreased ({:.6f} --&gt; {:.6f}).  Saving model ...'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span>
            <span class="n">valid_loss_min</span><span class="p">,</span>
            <span class="n">valid_loss</span><span class="p">))</span>
            <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span> <span class="n">save_path</span><span class="p">)</span>
            <span class="n">valid_loss_min</span> <span class="o">=</span> <span class="n">valid_loss</span>   
    <span class="c1"># return trained model
</span>    <span class="k">return</span> <span class="n">model</span>


<span class="c1"># train the model
</span><span class="n">model_scratch</span> <span class="o">=</span> <span class="nf">train</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="n">loaders_scratch</span><span class="p">,</span> <span class="n">model_scratch</span><span class="p">,</span> <span class="n">optimizer_scratch</span><span class="p">,</span> 
                      <span class="n">criterion_scratch</span><span class="p">,</span> <span class="n">use_cuda</span><span class="p">,</span> <span class="s">'model_scratch.pt'</span><span class="p">)</span>

<span class="c1"># load the model that got the best validation accuracy
</span><span class="n">model_scratch</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="s">'model_scratch.pt'</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch: 1 	Training Loss: 4.876332 	Validation Loss: 4.816575
Validation loss decreased (inf --&gt; 4.816575).  Saving model ...
Epoch: 2 	Training Loss: 4.748259 	Validation Loss: 4.572024
Validation loss decreased (4.816575 --&gt; 4.572024).  Saving model ...
Epoch: 3 	Training Loss: 4.627809 	Validation Loss: 4.499690
Validation loss decreased (4.572024 --&gt; 4.499690).  Saving model ...
Epoch: 4 	Training Loss: 4.586300 	Validation Loss: 4.449453
Validation loss decreased (4.499690 --&gt; 4.449453).  Saving model ...
Epoch: 5 	Training Loss: 4.526802 	Validation Loss: 4.497794
Epoch: 6 	Training Loss: 4.492073 	Validation Loss: 4.417369
Validation loss decreased (4.449453 --&gt; 4.417369).  Saving model ...
Epoch: 7 	Training Loss: 4.443948 	Validation Loss: 4.287956
Validation loss decreased (4.417369 --&gt; 4.287956).  Saving model ...
Epoch: 8 	Training Loss: 4.408243 	Validation Loss: 4.230388
Validation loss decreased (4.287956 --&gt; 4.230388).  Saving model ...
Epoch: 9 	Training Loss: 4.376940 	Validation Loss: 4.178228
Validation loss decreased (4.230388 --&gt; 4.178228).  Saving model ...
Epoch: 10 	Training Loss: 4.344263 	Validation Loss: 4.196465
Epoch: 11 	Training Loss: 4.300478 	Validation Loss: 4.197212
Epoch: 12 	Training Loss: 4.262984 	Validation Loss: 4.088739
Validation loss decreased (4.178228 --&gt; 4.088739).  Saving model ...
Epoch: 13 	Training Loss: 4.233462 	Validation Loss: 4.032015
Validation loss decreased (4.088739 --&gt; 4.032015).  Saving model ...
Epoch: 14 	Training Loss: 4.188986 	Validation Loss: 4.074294
Epoch: 15 	Training Loss: 4.154463 	Validation Loss: 3.936569
Validation loss decreased (4.032015 --&gt; 3.936569).  Saving model ...
Epoch: 16 	Training Loss: 4.113894 	Validation Loss: 3.955761
Epoch: 17 	Training Loss: 4.070495 	Validation Loss: 3.964011
Epoch: 18 	Training Loss: 4.016574 	Validation Loss: 3.931126
Validation loss decreased (3.936569 --&gt; 3.931126).  Saving model ...
Epoch: 19 	Training Loss: 4.018847 	Validation Loss: 3.961809
Epoch: 20 	Training Loss: 3.985453 	Validation Loss: 3.931244
Epoch: 21 	Training Loss: 3.957856 	Validation Loss: 3.969822
Epoch: 22 	Training Loss: 3.932438 	Validation Loss: 4.001125
Epoch: 23 	Training Loss: 3.899744 	Validation Loss: 3.956952
Epoch: 24 	Training Loss: 3.873465 	Validation Loss: 3.933152
Epoch: 25 	Training Loss: 3.850253 	Validation Loss: 3.939559
</code></pre></div></div>

<h3 id="implementation-test-the-model">(IMPLEMENTATION) Test the Model</h3>

<p>Try out your model on the test dataset of dog images.  Use the code cell below to calculate and print the test loss and accuracy.  Ensure that your test accuracy is greater than 10%.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">loaders</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">use_cuda</span><span class="p">):</span>

    <span class="c1"># monitor test loss and accuracy
</span>    <span class="n">test_loss</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mf">0.</span>

    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">loaders</span><span class="p">[</span><span class="s">'test'</span><span class="p">]):</span>
        <span class="c1"># move to GPU
</span>        <span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span>
            <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">cuda</span><span class="p">(),</span> <span class="n">target</span><span class="p">.</span><span class="nf">cuda</span><span class="p">()</span>
        <span class="c1"># forward pass: compute predicted outputs by passing inputs to the model
</span>        <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="c1"># calculate the loss
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="c1"># update average test loss 
</span>        <span class="n">test_loss</span> <span class="o">=</span> <span class="n">test_loss</span> <span class="o">+</span> <span class="p">((</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="n">data</span> <span class="o">-</span> <span class="n">test_loss</span><span class="p">))</span>
        <span class="c1"># convert output probabilities to predicted class
</span>        <span class="n">pred</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># compare predictions to true label
</span>        <span class="n">correct</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="n">pred</span><span class="p">.</span><span class="nf">eq</span><span class="p">(</span><span class="n">target</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">view_as</span><span class="p">(</span><span class="n">pred</span><span class="p">))).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">())</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">data</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            
    <span class="nf">print</span><span class="p">(</span><span class="s">'Test Loss: {:.6f}</span><span class="se">\n</span><span class="s">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">test_loss</span><span class="p">))</span>

    <span class="nf">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">Test Accuracy: %2d%% (%2d/%2d)'</span> <span class="o">%</span> <span class="p">(</span>
        <span class="mf">100.</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span><span class="p">,</span> <span class="n">correct</span><span class="p">,</span> <span class="n">total</span><span class="p">))</span>

<span class="c1"># call test function    
</span><span class="nf">test</span><span class="p">(</span><span class="n">loaders_scratch</span><span class="p">,</span> <span class="n">model_scratch</span><span class="p">,</span> <span class="n">criterion_scratch</span><span class="p">,</span> <span class="n">use_cuda</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Test Loss: 3.923931


Test Accuracy: 11% (99/836)
</code></pre></div></div>

<hr />
<p><a id="step4"></a></p>
<h2 id="step-4-create-a-cnn-to-classify-dog-breeds-using-transfer-learning">Step 4: Create a CNN to Classify Dog Breeds (using Transfer Learning)</h2>

<p>You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.</p>

<h3 id="implementation-specify-data-loaders-for-the-dog-dataset-1">(IMPLEMENTATION) Specify Data Loaders for the Dog Dataset</h3>

<p>Use the code cell below to write three separate <a href="http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader">data loaders</a> for the training, validation, and test datasets of dog images (located at <code class="language-plaintext highlighter-rouge">dogImages/train</code>, <code class="language-plaintext highlighter-rouge">dogImages/valid</code>, and <code class="language-plaintext highlighter-rouge">dogImages/test</code>, respectively).</p>

<p>If you like, <strong>you are welcome to use the same data loaders from the previous step</strong>, when you created a CNN from scratch.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## TODO: Specify data loaders
</span><span class="n">loaders_transfer</span> <span class="o">=</span> <span class="n">loaders_scratch</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="n">VGG16</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="s">"Input Features:"</span><span class="p">,</span><span class="n">VGG16</span><span class="p">.</span><span class="n">classifier</span><span class="p">[</span><span class="mi">6</span><span class="p">].</span><span class="n">in_features</span><span class="p">)</span> 
<span class="nf">print</span><span class="p">(</span><span class="s">"Output Features: "</span><span class="p">,</span><span class="n">VGG16</span><span class="p">.</span><span class="n">classifier</span><span class="p">[</span><span class="mi">6</span><span class="p">].</span><span class="n">out_features</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace)
    (2): Dropout(p=0.5)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace)
    (5): Dropout(p=0.5)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
4096
1000
</code></pre></div></div>

<h3 id="implementation-model-architecture-1">(IMPLEMENTATION) Model Architecture</h3>

<p>Use transfer learning to create a CNN to classify dog breed.  Use the code cell below, and save your initialized model as the variable <code class="language-plaintext highlighter-rouge">model_transfer</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torchvision.models</span> <span class="k">as</span> <span class="n">models</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="c1">## TODO: Specify model architecture 
</span><span class="n">model_transfer</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="nf">vgg16</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Freeze training for all "features" layers
</span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model_transfer</span><span class="p">.</span><span class="n">features</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>

<span class="n">n_inputs</span> <span class="o">=</span> <span class="n">model_transfer</span><span class="p">.</span><span class="n">classifier</span><span class="p">[</span><span class="mi">6</span><span class="p">].</span><span class="n">in_features</span>

<span class="n">last_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_inputs</span><span class="p">,</span> <span class="mi">133</span><span class="p">)</span>

<span class="n">model_transfer</span><span class="p">.</span><span class="n">classifier</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span> <span class="o">=</span> <span class="n">last_layer</span>


<span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span>
    <span class="n">model_transfer</span> <span class="o">=</span> <span class="n">model_transfer</span><span class="p">.</span><span class="nf">cuda</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Question 5:</strong> Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem.</p>

<p><strong>Answer:</strong></p>
<ol>
  <li>VGG16 is trained on the ImageNet dataset and it can easily capture the features of our dog images.</li>
  <li>First step was to freeze all the layers except classifier.</li>
  <li>Now changing the output of the last dense layer to 133 makes the model ready for testing.</li>
</ol>

<h3 id="implementation-specify-loss-function-and-optimizer-1">(IMPLEMENTATION) Specify Loss Function and Optimizer</h3>

<p>Use the next code cell to specify a <a href="http://pytorch.org/docs/master/nn.html#loss-functions">loss function</a> and <a href="http://pytorch.org/docs/master/optim.html">optimizer</a>.  Save the chosen loss function as <code class="language-plaintext highlighter-rouge">criterion_transfer</code>, and the optimizer as <code class="language-plaintext highlighter-rouge">optimizer_transfer</code> below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">criterion_transfer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer_transfer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">model_transfer</span><span class="p">.</span><span class="n">classifier</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="implementation-train-and-validate-the-model-1">(IMPLEMENTATION) Train and Validate the Model</h3>

<p>Train and validate your model in the code cell below.  <a href="http://pytorch.org/docs/master/notes/serialization.html">Save the final model parameters</a> at filepath <code class="language-plaintext highlighter-rouge">'model_transfer.pt'</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">15</span>
<span class="c1"># train the model
</span><span class="n">model_transfer</span> <span class="o">=</span> <span class="nf">train</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">,</span> <span class="n">loaders_transfer</span><span class="p">,</span> <span class="n">model_transfer</span><span class="p">,</span> <span class="n">optimizer_transfer</span><span class="p">,</span> <span class="n">criterion_transfer</span><span class="p">,</span> <span class="n">use_cuda</span><span class="p">,</span> <span class="s">'model_transfer.pt'</span><span class="p">)</span>

<span class="c1"># load the model that got the best validation accuracy (uncomment the line below)
</span><span class="n">model_transfer</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="s">'model_transfer.pt'</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch: 1 	Training Loss: 4.395283 	Validation Loss: 3.219059
Validation loss decreased (inf --&gt; 3.219059).  Saving model ...
Epoch: 2 	Training Loss: 3.086744 	Validation Loss: 1.650656
Validation loss decreased (3.219059 --&gt; 1.650656).  Saving model ...
Epoch: 3 	Training Loss: 2.150576 	Validation Loss: 0.968228
Validation loss decreased (1.650656 --&gt; 0.968228).  Saving model ...
Epoch: 4 	Training Loss: 1.715927 	Validation Loss: 0.711452
Validation loss decreased (0.968228 --&gt; 0.711452).  Saving model ...
Epoch: 5 	Training Loss: 1.492721 	Validation Loss: 0.612683
Validation loss decreased (0.711452 --&gt; 0.612683).  Saving model ...
Epoch: 6 	Training Loss: 1.391120 	Validation Loss: 0.539134
Validation loss decreased (0.612683 --&gt; 0.539134).  Saving model ...
Epoch: 7 	Training Loss: 1.288818 	Validation Loss: 0.503064
Validation loss decreased (0.539134 --&gt; 0.503064).  Saving model ...
Epoch: 8 	Training Loss: 1.234156 	Validation Loss: 0.470367
Validation loss decreased (0.503064 --&gt; 0.470367).  Saving model ...
Epoch: 9 	Training Loss: 1.194840 	Validation Loss: 0.455888
Validation loss decreased (0.470367 --&gt; 0.455888).  Saving model ...
Epoch: 10 	Training Loss: 1.144792 	Validation Loss: 0.439842
Validation loss decreased (0.455888 --&gt; 0.439842).  Saving model ...
Epoch: 11 	Training Loss: 1.132506 	Validation Loss: 0.430309
Validation loss decreased (0.439842 --&gt; 0.430309).  Saving model ...
Epoch: 12 	Training Loss: 1.087484 	Validation Loss: 0.421268
Validation loss decreased (0.430309 --&gt; 0.421268).  Saving model ...
Epoch: 13 	Training Loss: 1.072485 	Validation Loss: 0.416770
Validation loss decreased (0.421268 --&gt; 0.416770).  Saving model ...
Epoch: 14 	Training Loss: 1.027749 	Validation Loss: 0.408649
Validation loss decreased (0.416770 --&gt; 0.408649).  Saving model ...
Epoch: 15 	Training Loss: 1.005084 	Validation Loss: 0.391689
Validation loss decreased (0.408649 --&gt; 0.391689).  Saving model ...
</code></pre></div></div>

<h3 id="implementation-test-the-model-1">(IMPLEMENTATION) Test the Model</h3>

<p>Try out your model on the test dataset of dog images. Use the code cell below to calculate and print the test loss and accuracy.  Ensure that your test accuracy is greater than 60%.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">test</span><span class="p">(</span><span class="n">loaders_transfer</span><span class="p">,</span> <span class="n">model_transfer</span><span class="p">,</span> <span class="n">criterion_transfer</span><span class="p">,</span> <span class="n">use_cuda</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Test Loss: 0.439616


Test Accuracy: 86% (725/836)
</code></pre></div></div>

<h3 id="implementation-predict-dog-breed-with-the-model">(IMPLEMENTATION) Predict Dog Breed with the Model</h3>

<p>Write a function that takes an image path as input and returns the dog breed (<code class="language-plaintext highlighter-rouge">Affenpinscher</code>, <code class="language-plaintext highlighter-rouge">Afghan hound</code>, etc) that is predicted by your model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">### TODO: Write a function that takes a path to an image as input
### and returns the dog breed that is predicted by the model.
</span>
<span class="c1"># list of class names by index, i.e. a name can be accessed like class_names[0]
</span><span class="n">class_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="mi">4</span><span class="p">:].</span><span class="nf">replace</span><span class="p">(</span><span class="s">"_"</span><span class="p">,</span> <span class="s">" "</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">[</span><span class="s">'train_data'</span><span class="p">].</span><span class="n">classes</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">predict_breed_transfer</span><span class="p">(</span><span class="n">img_path</span><span class="p">):</span>
    <span class="c1"># load the image and return the predicted breed
</span>    <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">img_path</span><span class="p">).</span><span class="nf">convert</span><span class="p">(</span><span class="s">'RGB'</span><span class="p">)</span>
    
    <span class="n">data_transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="p">.</span><span class="nc">Resize</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
                                         <span class="n">transforms</span><span class="p">.</span><span class="nc">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
                                         <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
                                         <span class="n">transforms</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
                                                              <span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])])</span>
    
    <span class="n">image</span> <span class="o">=</span> <span class="nf">data_transform</span><span class="p">(</span><span class="n">image</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="nf">cuda</span><span class="p">()</span>
    <span class="n">predict</span> <span class="o">=</span> <span class="nf">model_transfer</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="n">predict</span> <span class="o">=</span> <span class="n">predict</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">argmax</span><span class="p">()</span>
    
    <span class="k">return</span> <span class="n">class_names</span><span class="p">[</span><span class="n">predict</span><span class="p">]</span>
</code></pre></div></div>

<hr />
<p><a id="step5"></a></p>
<h2 id="step-5-write-your-algorithm">Step 5: Write your Algorithm</h2>

<p>Write an algorithm that accepts a file path to an image and first determines whether the image contains a human, dog, or neither.  Then,</p>
<ul>
  <li>if a <strong>dog</strong> is detected in the image, return the predicted breed.</li>
  <li>if a <strong>human</strong> is detected in the image, return the resembling dog breed.</li>
  <li>if <strong>neither</strong> is detected in the image, provide output that indicates an error.</li>
</ul>

<p>You are welcome to write your own functions for detecting humans and dogs in images, but feel free to use the <code class="language-plaintext highlighter-rouge">face_detector</code> and <code class="language-plaintext highlighter-rouge">human_detector</code> functions developed above.  You are <strong>required</strong> to use your CNN from Step 4 to predict dog breed.</p>

<p>Some sample output for our algorithm is provided below, but feel free to design your own user experience!</p>

<p><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/sample_human_output.png" alt="Sample Human Output" /></p>

<h3 id="implementation-write-your-algorithm">(IMPLEMENTATION) Write your Algorithm</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">### TODO: Write your algorithm.
### Feel free to use as many code cells as needed.
</span>
<span class="k">def</span> <span class="nf">display_image</span><span class="p">(</span><span class="n">img_path</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">run_app</span><span class="p">(</span><span class="n">img_path</span><span class="p">):</span>
    
    
    <span class="c1"># load and transform image
</span>    <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">img_path</span><span class="p">).</span><span class="nf">convert</span><span class="p">(</span><span class="s">'RGB'</span><span class="p">)</span>
    
    <span class="n">data_transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="p">.</span><span class="nc">Resize</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
                                         <span class="n">transforms</span><span class="p">.</span><span class="nc">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
                                         <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
                                         <span class="n">transforms</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">],</span>
                                                              <span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])])</span>
    
    <span class="n">image</span> <span class="o">=</span> <span class="nf">data_transform</span><span class="p">(</span><span class="n">image</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="c1">## handle cases for a human face, dog, and neither
</span>    <span class="n">breed_pred</span> <span class="o">=</span> <span class="nf">predict_breed_transfer</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="nf">dog_detector</span><span class="p">(</span><span class="n">img_path</span><span class="p">):</span>
        <span class="nf">print </span><span class="p">(</span><span class="s">'</span><span class="se">\n\n\n</span><span class="s">  Dog Detected'</span><span class="p">)</span>
        <span class="nf">display_image</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>
        <span class="k">return</span> <span class="nf">print </span><span class="p">(</span><span class="s">'The Predicted Breed:'</span><span class="p">,</span> <span class="n">breed_pred</span><span class="p">)</span>
    
    
    <span class="k">elif</span> <span class="nf">face_detector</span><span class="p">(</span><span class="n">img_path</span><span class="p">):</span>
        <span class="nf">print </span><span class="p">(</span><span class="s">'</span><span class="se">\n\n\n</span><span class="s"> Human Detected'</span><span class="p">)</span>
        <span class="nf">display_image</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>
        <span class="k">return</span> <span class="nf">print </span><span class="p">(</span><span class="s">'Closest Dog Breed:'</span><span class="p">,</span> <span class="n">breed_pred</span><span class="p">)</span>
    
    <span class="k">else</span><span class="p">:</span>
        <span class="nf">display_image</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>
        <span class="nf">print </span><span class="p">(</span><span class="s">'</span><span class="se">\n\n\n</span><span class="s"> Error: Neither Dog nor Human Detected'</span><span class="p">)</span>
</code></pre></div></div>

<hr />
<p><a id="step6"></a></p>
<h2 id="step-6-test-your-algorithm">Step 6: Test Your Algorithm</h2>

<p>In this section, you will take your new algorithm for a spin!  What kind of dog does the algorithm think that <em>you</em> look like?  If you have a dog, does it predict your dog’s breed accurately?  If you have a cat, does it mistakenly think that your cat is a dog?</p>

<h3 id="implementation-test-your-algorithm-on-sample-images">(IMPLEMENTATION) Test Your Algorithm on Sample Images!</h3>

<p>Test your algorithm at least six images on your computer.  Feel free to use any images you like.  Use at least two human and two dog images.</p>

<p><strong>Question 6:</strong> Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm.</p>

<p><strong>Answer:</strong></p>
<ol>
  <li>The ouput is just as expected which correctly classifies dog and somewhat classifies the closest remsemblance to a dog for a human.</li>
  <li>One thing that surprised me while testing on custom images was that the model worked even with random noise in the test images and gave almost correct results.</li>
  <li>Training on more data and augmenting this data can improve the accuracy of our model.</li>
  <li>Increasing the number of epochs can also improve the performance.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## TODO: Execute your algorithm from Step 6 on
## at least 6 images on your computer.
## Feel free to use as many code cells as needed.
</span>
<span class="c1">## suggested code, below
</span><span class="k">for</span> <span class="nb">file</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">human_files</span><span class="p">[:</span><span class="mi">3</span><span class="p">],</span> <span class="n">dog_files</span><span class="p">[:</span><span class="mi">3</span><span class="p">])):</span>
    <span class="nf">run_app</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> Human Detected
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/output_56_1.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Closest Dog Breed: American staffordshire terrier



 Human Detected
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/output_56_3.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Closest Dog Breed: Dachshund



 Human Detected
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/output_56_5.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Closest Dog Breed: Cocker spaniel



  Dog Detected
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/output_56_7.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The Predicted Breed: Bullmastiff



  Dog Detected
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/output_56_9.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The Predicted Breed: Mastiff



  Dog Detected
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/output_56_11.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The Predicted Breed: Mastiff
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">custom_img</span> <span class="o">=</span> <span class="nf">glob</span><span class="p">(</span><span class="s">"./images/*"</span><span class="p">)</span>
<span class="k">for</span> <span class="nb">file</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">custom_img</span><span class="p">)):</span>
    <span class="nf">run_app</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  Dog Detected
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/output_57_1.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The Predicted Breed: Irish red and white setter



 Human Detected
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/output_57_3.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Closest Dog Breed: Cocker spaniel



  Dog Detected
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/output_57_5.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The Predicted Breed: Labrador retriever



  Dog Detected
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/output_57_7.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The Predicted Breed: Curly-coated retriever
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/output_57_9.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> Error: Neither Dog nor Human Detected



  Dog Detected
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/output_57_11.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The Predicted Breed: Brittany



  Dog Detected
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/output_57_13.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The Predicted Breed: Labrador retriever



  Dog Detected
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/output_57_15.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The Predicted Breed: American water spaniel



  Dog Detected
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/output_57_17.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The Predicted Breed: Greyhound



  Dog Detected
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/shrikantnaidu/shrikantnaidu.github.io/main/_posts/assets/output_57_19.png" alt="png" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The Predicted Breed: Labrador retriever
</code></pre></div></div>


				</div>
				
			</div>
			<div class="col-lg-4">
  <div class="widget search-box">
    <form action="/search.html" method="get">
      <i class="ti-search"></i>
      <input type="search" id="search-box" class="form-control border-0 pl-5" name="query" placeholder="Search">
    </form>
  </div>
  <div class="widget">
    <h6 class="mb-4">LATEST POST</h6>
    
    <div class="media mb-4">
      <div class="post-thumb-sm mr-3">
        <img class="img-fluid" src="/assets/images/masonary-post/sentiment-analysis.jpg" alt="Sentiment Analysis on AWS SageMaker">
      </div>
      <div class="media-body">
        <ul class="list-inline d-flex justify-content-between mb-2">
          <li class="list-inline-item">Post By Shrikant</li>
          <li class="list-inline-item">June 25, 2020</li>
        </ul>
        <h6><a class="text-dark" href="/deep%20learning/2020/06/25/sentiment-analysis-on-aws-sagemaker/">Sentiment Analysis on AWS SageMaker</a></h6>
      </div>
    </div>
    
    <div class="media mb-4">
      <div class="post-thumb-sm mr-3">
        <img class="img-fluid" src="/assets/images/masonary-post/face-gen.jpg" alt="Face Generation with GAN">
      </div>
      <div class="media-body">
        <ul class="list-inline d-flex justify-content-between mb-2">
          <li class="list-inline-item">Post By Shrikant</li>
          <li class="list-inline-item">June 14, 2020</li>
        </ul>
        <h6><a class="text-dark" href="/deep%20learning/2020/06/14/face-generation/">Face Generation with GAN</a></h6>
      </div>
    </div>
    
    <div class="media mb-4">
      <div class="post-thumb-sm mr-3">
        <img class="img-fluid" src="/assets/images/masonary-post/tv-scripts.jpg" alt="Generate TV Scripts">
      </div>
      <div class="media-body">
        <ul class="list-inline d-flex justify-content-between mb-2">
          <li class="list-inline-item">Post By Shrikant</li>
          <li class="list-inline-item">June 8, 2020</li>
        </ul>
        <h6><a class="text-dark" href="/deep%20learning/2020/06/08/tv-script-generation/">Generate TV Scripts</a></h6>
      </div>
    </div>
    
  </div>
  <div class="widget">
    <h6 class="mb-4">CATEGORIES</h6>
    <ul class="list-inline tag-list">
      
      <li class="list-inline-item m-1"><a href="/category/nature">Nature</a></li>
      &nbsp;
      
      <li class="list-inline-item m-1"><a href="/category/article">Article</a></li>
      &nbsp;
      
      <li class="list-inline-item m-1"><a href="/category/fashion">Fashion</a></li>
      &nbsp;
      
      <li class="list-inline-item m-1"><a href="/category/philosophy">Philosophy</a></li>
      &nbsp;
      
      <li class="list-inline-item m-1"><a href="/category/lifestyle">Lifestyle</a></li>
      &nbsp;
      
      <li class="list-inline-item m-1"><a href="/category/food">Food</a></li>
      &nbsp;
      
      <li class="list-inline-item m-1"><a href="/category/deep-learning">Deep Learning</a></li>
      
      
    </ul>
  </div>
</div>
		</div>
	</div>
</section>
<!-- /blog single -->


  <footer class="bg-secondary">
  <div class="section">
    <div class="container">
      <div class="row">
        
        <div class="col-md-3 col-sm-6 mb-4 mb-md-0">
          <a href="/"><img src="/assets/images/s-logo.png" alt="Shrikant Naidu" class="img-fluid"></a>
        </div>
        
        
        <div class="col-md-3 col-sm-6 mb-4 mb-md-0">
          <h6>Address</h6>
          <ul class="list-unstyled">
            <li class="font-secondary text-dark">Mumbai</li>
            <li class="font-secondary text-dark"></li>
          </ul>
        </div>
        
        
        <div class="col-md-3 col-sm-6 mb-4 mb-md-0">
          <h6>Contact Info</h6>
          <ul class="list-unstyled">
            <li class="font-secondary text-dark">Tel: </li>
            <li class="font-secondary text-dark">Mail: shrikantnaidu777@gmail.com</li>
          </ul>
        </div>
        
        
        <div class="col-md-3 col-sm-6 mb-4 mb-md-0">
          <h6>Follow</h6>
          <ul class="list-inline d-inline-block">
            
            <li class="list-inline-item"><a href="https://www.linkedin.com/in/shrikant-naidu/" class="text-dark"><i class="ti-linkedin"></i></a></li>
            
            <li class="list-inline-item"><a href="https://github.com/shrikantnaidu" class="text-dark"><i class="ti-github"></i></a></li>
            
            <li class="list-inline-item"><a href="https://twitter.com/sk_dataholic" class="text-dark"><i class="ti-twitter-alt"></i></a></li>
            
            <li class="list-inline-item"><a href="https://www.instagram.com/sk.barcaholic/" class="text-dark"><i class="ti-instagram"></i></a></li>
            
          </ul>
        </div>
        
      </div>
    </div>
  </div>
  <div class="text-center pb-3"><p>Copyright 2023 Designed by <a href="https://themefisher.com">Themefisher</a> &amp; Developed by <a href="https://gethugothemes.com">Gethugothemes</a></p>
</div>
</footer>

  <!-- jQuery -->
  <script src="/assets/plugins/jQuery/jquery.min.js"></script>
  <!-- Bootstrap JS -->
  <script src="/assets/plugins/bootstrap/bootstrap.min.js"></script>
  <!-- slick slider -->
  <script src="/assets/plugins/slick/slick.min.js"></script>
  <!-- masonry -->
  <script src="/assets/plugins/masonry/masonry.js"></script>
  <!-- instafeed -->
  <script src="/assets/plugins/instafeed/instafeed.min.js"></script>
  <!-- smooth scroll -->
  <script src="/assets/plugins/smooth-scroll/smooth-scroll.js"></script>
  <!-- headroom -->
  <script src="/assets/plugins/headroom/headroom.js"></script>
  <!-- reading time -->
  <script src="/assets/plugins/reading-time/readingTime.min.js"></script>
  <!-- lunr.js -->
  <script src="/assets/plugins/search/lunr.min.js"></script>
  <!-- search -->
  <script src="/assets/plugins/search/search.js"></script>

  <!-- Main Script -->
  <script src="/assets/js/script.js"></script>
</body>

</html>