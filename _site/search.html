<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>Search Result</title>

  <!-- mobile responsive meta -->
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <!-- theme meta -->
  <meta name="theme-name" content="parsa-jekyll" />
  
  <!-- ** Plugins Needed for the Project ** -->
  <!-- Bootstrap -->
  <link rel="stylesheet" href="/assets/plugins/bootstrap/bootstrap.min.css">
  <!-- slick slider -->
  <link rel="stylesheet" href="/assets/plugins/slick/slick.css">
  <!-- themefy-icon -->
  <link rel="stylesheet" href="/assets/plugins/themify-icons/themify-icons.css">

  <!-- Main Stylesheet -->
  <link href="/assets/css/style.css" rel="stylesheet">
  
  <!--Favicon-->
  <link rel="shortcut icon" href="/assets/images/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/assets/images/favicon.ico" type="image/x-icon">

</head>


<body>
  

  
  <!-- preloader -->
  <div class="preloader">
    <div class="loader">
      <span class="dot"></span>
      <div class="dots">
        <span></span>
        <span></span>
        <span></span>
      </div>
    </div>
  </div>
  <!-- /preloader -->
  

  <header class="navigation">
  <nav class="navbar navbar-expand-lg navbar-light">
    
    <a class="navbar-brand" href="http://localhost:4000/"><img class="img-fluid" src="/assets/images/s-logo.png" alt="Shrikant Naidu"></a>
    
    <button class="navbar-toggler border-0" type="button" data-toggle="collapse" data-target="#navogation"
      aria-controls="navogation" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse text-center" id="navogation">
      <ul class="navbar-nav ml-auto">
        
        
        <li class="nav-item">
          <a class="nav-link text-uppercase text-dark" href="/">Home</a>
        </li>
        
        
        
        <li class="nav-item dropdown">
          <a class="nav-link text-uppercase text-dark dropdown-toggle" href="#" id="navbarDropdown"
            role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
            Projects
          </a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdown">
            
            <a class="dropdown-item" href="/DL/">Deep Learning</a>
            
          </div>
        </li>
        
        
        
        <li class="nav-item">
          <a class="nav-link text-uppercase text-dark" href="/about/">About</a>
        </li>
        
        
        
        <li class="nav-item">
          <a class="nav-link text-uppercase text-dark" href="/contact/">Contact</a>
        </li>
        
        
      </ul>
      <form class="form-inline position-relative ml-lg-4" action="/search.html" method="get">
        <input class="form-control px-0 w-100" type="search" id="search-box" name="query" placeholder="Search">
        <button class="search-icon" type="submit"><i class="ti-search text-dark"></i></button>
      </form>
    </div>
  </nav>
</header>

    <!-- page-title -->
<section class="section bg-secondary">
  <div class="container">
    <div class="row">
      <div class="col-lg-12">
        <h4>Search Result</h4>
      </div>
    </div>
  </div>
</section>
<!-- /page-title -->

<!-- search result -->
<section class="section">
  <div class="container">
    <div class="row">
      <div class="col-lg-10 mx-auto">
        <ul class="list-unstyled" id="search-results"></ul>
      </div>
    </div>
  </div>
</section>
<!-- /search result -->

<script>
  window.store = {
    
      "deep-20learning-2020-06-25-sentiment-analysis-on-aws-sagemaker": {
        "title": "Sentiment Analysis on AWS SageMaker",
        "author": "",
        "category": "",
        "content": "Creating a Sentiment Analysis Web AppUsing PyTorch and SageMakerNow that we have a basic understanding of how SageMaker works we will try to use it to construct a complete project from end to end. Our goal will be to have a simple web page which a user can use to enter a movie review. The web page will then send the review off to our deployed model which will predict the sentiment of the entered review.General OutlineRecall the general outline for SageMaker projects using a notebook instance.  Download or otherwise retrieve the data.  Process / Prepare the data.  Upload the processed data to S3.  Train a chosen model.  Test the trained model (typically using a batch transform job).  Deploy the trained model.  Use the deployed model.For this project, you will be following the steps in the general outline with some modifications.First, you will not be testing the model in its own step. You will still be testing the model, however, you will do it by deploying your model and then using the deployed model by sending the test data to it. One of the reasons for doing this is so that you can make sure that your deployed model is working correctly before moving forward.In addition, you will deploy and use your trained model a second time. In the second iteration you will customize the way that your trained model is deployed by including some of your own code. In addition, your newly deployed model will be used in the sentiment analysis web app.Step 1: Downloading the dataAs in the XGBoost in SageMaker notebook, we will be using the IMDb dataset  Maas, Andrew L., et al. Learning Word Vectors for Sentiment Analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, 2011.%mkdir ../data!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz!tar -zxf ../data/aclImdb_v1.tar.gz -C ../datamkdir: cannot create directory ‘../data’: File exists--2020-06-23 13:29:45--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gzResolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.HTTP request sent, awaiting response... 200 OKLength: 84125825 (80M) [application/x-gzip]Saving to: ‘../data/aclImdb_v1.tar.gz’../data/aclImdb_v1. 100%[===================&gt;]  80.23M  7.37MB/s    in 31s     2020-06-23 13:30:16 (2.58 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]Step 2: Preparing and Processing the dataAlso, as in the XGBoost notebook, we will be doing some initial data processing. The first few steps are the same as in the XGBoost example. To begin with, we will read in each of the reviews and combine them into a single input structure. Then, we will split the dataset into a training set and a testing set.import osimport globdef read_imdb_data(data_dir='../data/aclImdb'):    data = {}    labels = {}        for data_type in ['train', 'test']:        data[data_type] = {}        labels[data_type] = {}                for sentiment in ['pos', 'neg']:            data[data_type][sentiment] = []            labels[data_type][sentiment] = []                        path = os.path.join(data_dir, data_type, sentiment, '*.txt')            files = glob.glob(path)                        for f in files:                with open(f) as review:                    data[data_type][sentiment].append(review.read())                    # Here we represent a positive review by '1' and a negative review by '0'                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)                                assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)                    return data, labelsdata, labels = read_imdb_data()print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(            len(data['train']['pos']), len(data['train']['neg']),            len(data['test']['pos']), len(data['test']['neg'])))IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 negNow that we’ve read the raw training and testing data from the downloaded dataset, we will combine the positive and negative reviews and shuffle the resulting records.from sklearn.utils import shuffledef prepare_imdb_data(data, labels):    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"        #Combine positive and negative reviews and labels    data_train = data['train']['pos'] + data['train']['neg']    data_test = data['test']['pos'] + data['test']['neg']    labels_train = labels['train']['pos'] + labels['train']['neg']    labels_test = labels['test']['pos'] + labels['test']['neg']        #Shuffle reviews and corresponding labels within training and test sets    data_train, labels_train = shuffle(data_train, labels_train)    data_test, labels_test = shuffle(data_test, labels_test)        # Return a unified training data, test data, training labels, test labets    return data_train, data_test, labels_train, labels_testtrain_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))IMDb reviews (combined): train = 25000, test = 25000Now that we have our training and testing sets unified and prepared, we should do a quick check and see an example of the data our model will be trained on. This is generally a good idea as it allows you to see how each of the further processing steps affects the reviews and it also ensures that the data has been loaded correctly.print(train_X[100])print(train_y[100])I have never understood the appeal of this show. The acting is poor (Debra Jo Rupp being a notable exception), the plots of most episodes are trite and uninspiring, the dialogue is weak, the jokes unfunny and it is painful to try and sit through even half an episode. Furthermore the link between this show and the '70s' is extremely tenuous beyond the style of dress and the scenery and background used for the show -it seems to be nothing more than a modern sitcom with the same old unfunny, clichéd scripts that modern sitcoms have dressed up as depicting a show from twenty years ago in the hope that it will gain some nostalgic viewers or something like that. Both \"Happy Days\" and \"The Wonder Years\" employ the same technique much more effectively and are actually a pleasure to watch in contrast to this horrible, pathetic excuse for a show0The first step in processing the reviews is to make sure that any html tags that appear should be removed. In addition we wish to tokenize our input, that way words such as entertained and entertaining are considered the same with regard to sentiment analysis.import nltkfrom nltk.corpus import stopwordsfrom nltk.stem.porter import *import refrom bs4 import BeautifulSoupdef review_to_words(review):    nltk.download(\"stopwords\", quiet=True)    stemmer = PorterStemmer()        text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case    words = text.split() # Split string into words    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords    words = [PorterStemmer().stem(w) for w in words] # stem        return wordsThe review_to_words method defined above uses BeautifulSoup to remove any html tags that appear and uses the nltk package to tokenize the reviews. As a check to ensure we know how everything is working, try applying review_to_words to one of the reviews in the training set.# TODO: Apply review_to_words to a review (train_X[100] or any other review)review_to_words(train_X[100])['never', 'understood', 'appeal', 'show', 'act', 'poor', 'debra', 'jo', 'rupp', 'notabl', 'except', 'plot', 'episod', 'trite', 'uninspir', 'dialogu', 'weak', 'joke', 'unfunni', 'pain', 'tri', 'sit', 'even', 'half', 'episod', 'furthermor', 'link', 'show', '70', 'extrem', 'tenuou', 'beyond', 'style', 'dress', 'sceneri', 'background', 'use', 'show', 'seem', 'noth', 'modern', 'sitcom', 'old', 'unfunni', 'clich', 'script', 'modern', 'sitcom', 'dress', 'depict', 'show', 'twenti', 'year', 'ago', 'hope', 'gain', 'nostalg', 'viewer', 'someth', 'like', 'happi', 'day', 'wonder', 'year', 'employ', 'techniqu', 'much', 'effect', 'actual', 'pleasur', 'watch', 'contrast', 'horribl', 'pathet', 'excus', 'show']Question: Above we mentioned that review_to_words method removes html formatting and allows us to tokenize the words found in a review, for example, converting entertained and entertaining into entertain so that they are treated as though they are the same word. What else, if anything, does this method do to the input?Answer:  The method does the basic pre-processing required for natural language processing.  We apply Stemming to the input data to get a generalized form of each word.  All the words are lowercased to avoid duplicates with same meaning.  Symbols and irrelevant text is discarded to reduce the noise.  Stopwards which could appear very frequently and make no sense are removed.The method below applies the review_to_words method to each of the reviews in the training and testing datasets. In addition it caches the results. This is because performing this processing step can take a long time. This way if you are unable to complete the notebook in the current session, you can come back without needing to process the data a second time.import picklecache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")  # where to store cache filesos.makedirs(cache_dir, exist_ok=True)  # ensure cache directory existsdef preprocess_data(data_train, data_test, labels_train, labels_test,                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):    \"\"\"Convert each review to words; read from cache if available.\"\"\"    # If cache_file is not None, try to read from it first    cache_data = None    if cache_file is not None:        try:            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:                cache_data = pickle.load(f)            print(\"Read preprocessed data from cache file:\", cache_file)        except:            pass  # unable to read from cache, but that's okay        # If cache is missing, then do the heavy lifting    if cache_data is None:        # Preprocess training and test data to obtain words for each review        #words_train = list(map(review_to_words, data_train))        #words_test = list(map(review_to_words, data_test))        words_train = [review_to_words(review) for review in data_train]        words_test = [review_to_words(review) for review in data_test]                # Write to cache file for future runs        if cache_file is not None:            cache_data = dict(words_train=words_train, words_test=words_test,                              labels_train=labels_train, labels_test=labels_test)            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:                pickle.dump(cache_data, f)            print(\"Wrote preprocessed data to cache file:\", cache_file)    else:        # Unpack data loaded from cache file        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])        return words_train, words_test, labels_train, labels_test# Preprocess datatrain_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)Wrote preprocessed data to cache file: preprocessed_data.pklTransform the dataIn the XGBoost notebook we transformed the data from its word representation to a bag-of-words feature representation. For the model we are going to construct in this notebook we will construct a feature representation which is very similar. To start, we will represent each word as an integer. Of course, some of the words that appear in the reviews occur very infrequently and so likely don’t contain much information for the purposes of sentiment analysis. The way we will deal with this problem is that we will fix the size of our working vocabulary and we will only include the words that appear most frequently. We will then combine all of the infrequent words into a single category and, in our case, we will label it as 1.Since we will be using a recurrent neural network, it will be convenient if the length of each review is the same. To do this, we will fix a size for our reviews and then pad short reviews with the category ‘no word’ (which we will label 0) and truncate long reviews.(TODO) Create a word dictionaryTo begin with, we need to construct a way to map words that appear in the reviews to integers. Here we fix the size of our vocabulary (including the ‘no word’ and ‘infrequent’ categories) to be 5000 but you may wish to change this to see how it affects the model.  TODO: Complete the implementation for the build_dict() method below. Note that even though the vocab_size is set to 5000, we only want to construct a mapping for the most frequently appearing 4998 words. This is because we want to reserve the special labels 0 for ‘no word’ and 1 for ‘infrequent word’.import numpy as npdef build_dict(data, vocab_size = 5000):    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"        # TODO: Determine how often each word appears in `data`. Note that `data` is a list of sentences and that a    #       sentence is a list of words.        word_count = {} # A dict storing the words that appear in the reviews along with how often they occur        # TODO: Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and    #       sorted_words[-1] is the least frequently appearing word.    for each_review in data:        for each_word in each_review:            if each_word in word_count:                word_count[each_word] +=1            else:                word_count[each_word] = 1                    sorted_words = sorted(word_count, key=word_count.get, reverse=True)        word_dict = {} # This is what we are building, a dictionary that translates words into integers    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'        word_dict[word] = idx + 2                              # 'infrequent' labels            return word_dictword_dict = build_dict(train_X)Question: What are the five most frequently appearing (tokenized) words in the training set? Does it makes sense that these words appear frequently in the training set?Answer:  ‘movi’, ‘film’, ‘one’, ‘like’, ‘time’are the five most frequent words and it makes sense since it relates to reviews data.top_keywords = list(word_dict.keys())print(\"Top 5 Keywords:\",top_keywords[:5])Top 5 Keywords: ['movi', 'film', 'one', 'like', 'time']Save word_dictLater on when we construct an endpoint which processes a submitted review we will need to make use of the word_dict which we have created. As such, we will save it to a file now for future use.data_dir = '../data/pytorch' # The folder we will use for storing dataif not os.path.exists(data_dir): # Make sure that the folder exists    os.makedirs(data_dir)with open(os.path.join(data_dir, 'word_dict.pkl'), \"wb\") as f:    pickle.dump(word_dict, f)Transform the reviewsNow that we have our word dictionary which allows us to transform the words appearing in the reviews into integers, it is time to make use of it and convert our reviews to their integer sequence representation, making sure to pad or truncate to a fixed length, which in our case is 500.def convert_and_pad(word_dict, sentence, pad=500):    NOWORD = 0 # We will use 0 to represent the 'no word' category    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict        working_sentence = [NOWORD] * pad        for word_index, word in enumerate(sentence[:pad]):        if word in word_dict:            working_sentence[word_index] = word_dict[word]        else:            working_sentence[word_index] = INFREQ                return working_sentence, min(len(sentence), pad)def convert_and_pad_data(word_dict, data, pad=500):    result = []    lengths = []        for sentence in data:        converted, leng = convert_and_pad(word_dict, sentence, pad)        result.append(converted)        lengths.append(leng)            return np.array(result), np.array(lengths)train_X, train_X_len = convert_and_pad_data(word_dict, train_X)test_X, test_X_len = convert_and_pad_data(word_dict, test_X)As a quick check to make sure that things are working as intended, check to see what one of the reviews in the training set looks like after having been processeed. Does this look reasonable? What is the length of a review in the training set?# Use this cell to examine one of the processed reviews to make sure everything is working as intended.print(\"len of a review\",len(train_X[55]))print(train_X[55])len of a review 500[4675    1 4680  162  424 2174 2211   75  614    1  561 3446  940  610 1512    1 1444    1 1492   55    1 2649   68   29 1482 1512   48  983 1057  349 1019  219 4675 2636   33 1885    1  334    1    1    1 1482    1 3820 1491 4914 1512  530 1459  530 1627  228 2522 2355  263 4675  178 2650 1243 4675  617 1512   54  564    1  354   19    1    1  551 1876 4675  349 1754    1    1    1 1031  489    1 2484    1   11   79  162   27  596 1194 1047 2031  108  283  680  305   35   10 1715 1512  468    1  133 4675  610 4444  768 2108   13  343  227 2035    1  850  244  965  183   75   80  690 1042   10    1   23   91   91    1    1    9  850    1   45 1064  169  412   13 4675    9    1 1627  489   37  456  475    1 1264 1933  162 4781 1868 3874   29  128 1427  898    1    4 3381   29   55 1495  228  970 3465 3014 4680 2425   81    8  315 4915 2351    1  850  354  126    1  768  645 3820 1491    1    1  272  991 4736 2602    1    1    1 1257  290  804   29   42 3875  169 1398  290  212  312 1612 4675    1    1 1243  738   18  840   29    1  714 3183 1049 2620  169 4734 1754    1  746  431  307   50    1  368   11 3709    1    1    1    1  646  105  644    2  807   27  126 2251   29 1175  714 1641   23   22 2379  708  370  319 1288  118  462   25  651    1 1715    1 1089   25   64   91 3382    4 2495    2   79   62    7    2  241    1    1  111  552   53  251  352  379  352  598  848  651    6    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0]Question: In the cells above we use the preprocess_data and convert_and_pad_data methods to process both the training and testing set. Why or why not might this be a problem?Answer:  Since the machine learning algorithms/model cannot make sense from the text we do the necessary preprocessing steps in order to make the text compliant to the algorithms/model.  When the model receives input, for it make sense out of the input it needs to undergo the same preprocessing as in the test data otherwise the model’s predictions will be poor.  The only problem that can occur is that the test data coming from a different distribution which will result in poor predictions.  This can be dealt by updating the model vocabulary.Step 3: Upload the data to S3As in the XGBoost notebook, we will need to upload the training dataset to S3 in order for our training code to access it. For now we will save it locally and we will upload to S3 later on.Save the processed training dataset locallyIt is important to note the format of the data that we are saving as we will need to know it when we write the training code. In our case, each row of the dataset has the form label, length, review[500] where review[500] is a sequence of 500 integers representing the words in the review.import pandas as pd    pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis=1) \\        .to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)Uploading the training dataNext, we need to upload the training data to the SageMaker default S3 bucket so that we can provide access to it while training our model.import sagemakersagemaker_session = sagemaker.Session()bucket = sagemaker_session.default_bucket()prefix = 'sagemaker/sentiment_rnn'role = sagemaker.get_execution_role()input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)NOTE: The cell above uploads the entire contents of our data directory. This includes the word_dict.pkl file. This is fortunate as we will need this later on when we create an endpoint that accepts an arbitrary review. For now, we will just take note of the fact that it resides in the data directory (and so also in the S3 training bucket) and that we will need to make sure it gets saved in the model directory.Step 4: Build and Train the PyTorch ModelIn the XGBoost notebook we discussed what a model is in the SageMaker framework. In particular, a model comprises three objects  Model Artifacts,  Training Code, and  Inference Code,each of which interact with one another. In the XGBoost example we used training and inference code that was provided by Amazon. Here we will still be using containers provided by Amazon with the added benefit of being able to include our own custom code.We will start by implementing our own neural network in PyTorch along with a training script. For the purposes of this project we have provided the necessary model object in the model.py file, inside of the train folder. You can see the provided implementation by running the cell below.!pygmentize train/model.py\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mLSTMClassifier\u001b[39;49;00m(nn.Module):    \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33m    This is the simple RNN model we will be using to perform Sentiment Analysis.\u001b[39;49;00m\u001b[33m    \"\"\"\u001b[39;49;00m    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, embedding_dim, hidden_dim, vocab_size):        \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33m        Initialize the model by settingg up the various layers.\u001b[39;49;00m\u001b[33m        \"\"\"\u001b[39;49;00m        \u001b[36msuper\u001b[39;49;00m(LSTMClassifier, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()        \u001b[36mself\u001b[39;49;00m.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=\u001b[34m0\u001b[39;49;00m)        \u001b[36mself\u001b[39;49;00m.lstm = nn.LSTM(embedding_dim, hidden_dim)        \u001b[36mself\u001b[39;49;00m.dense = nn.Linear(in_features=hidden_dim, out_features=\u001b[34m1\u001b[39;49;00m)        \u001b[36mself\u001b[39;49;00m.sig = nn.Sigmoid()                \u001b[36mself\u001b[39;49;00m.word_dict = \u001b[34mNone\u001b[39;49;00m    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):        \u001b[33m\"\"\"\u001b[39;49;00m\u001b[33m        Perform a forward pass of our model on some input.\u001b[39;49;00m\u001b[33m        \"\"\"\u001b[39;49;00m        x = x.t()        lengths = x[\u001b[34m0\u001b[39;49;00m,:]        reviews = x[\u001b[34m1\u001b[39;49;00m:,:]        embeds = \u001b[36mself\u001b[39;49;00m.embedding(reviews)        lstm_out, _ = \u001b[36mself\u001b[39;49;00m.lstm(embeds)        out = \u001b[36mself\u001b[39;49;00m.dense(lstm_out)        out = out[lengths - \u001b[34m1\u001b[39;49;00m, \u001b[36mrange\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(lengths))]        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.sig(out.squeeze())The important takeaway from the implementation provided is that there are three parameters that we may wish to tweak to improve the performance of our model. These are the embedding dimension, the hidden dimension and the size of the vocabulary. We will likely want to make these parameters configurable in the training script so that if we wish to modify them we do not need to modify the script itself. We will see how to do this later on. To start we will write some of the training code in the notebook so that we can more easily diagnose any issues that arise.First we will load a small portion of the training data set to use as a sample. It would be very time consuming to try and train the model completely in the notebook as we do not have access to a gpu and the compute instance that we are using is not particularly powerful. However, we can work on a small bit of the data to get a feel for how our training script is behaving.import torchimport torch.utils.data# Read in only the first 250 rowstrain_sample = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None, names=None, nrows=250)# Turn the input pandas dataframe into tensorstrain_sample_y = torch.from_numpy(train_sample[[0]].values).float().squeeze()train_sample_X = torch.from_numpy(train_sample.drop([0], axis=1).values).long()# Build the datasettrain_sample_ds = torch.utils.data.TensorDataset(train_sample_X, train_sample_y)# Build the dataloadertrain_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=50)(TODO) Writing the training methodNext we need to write the training code itself. This should be very similar to training methods that you have written before to train PyTorch models. We will leave any difficult aspects such as model saving / loading and parameter loading until a little later.def train(model, train_loader, epochs, optimizer, loss_fn, device):    for epoch in range(1, epochs + 1):        model.train()        total_loss = 0        for batch in train_loader:                     batch_X, batch_y = batch                        batch_X = batch_X.to(device)            batch_y = batch_y.to(device)                        # TODO: Complete this train method to train the model provided.            optimizer.zero_grad()            # forward pass            output = model.forward(batch_X)            # calculate the batch loss            loss = loss_fn(output, batch_y)            # backpropagation            loss.backward()            #optimization            optimizer.step()                        total_loss += loss.data.item()        print(\"Epoch: {}, BCELoss: {}\".format(epoch, total_loss / len(train_loader)))Supposing we have the training method above, we will test that it is working by writing a bit of code in the notebook that executes our training method on the small sample training set that we loaded earlier. The reason for doing this in the notebook is so that we have an opportunity to fix any errors that arise early when they are easier to diagnose.import torch.optim as optimfrom train.model import LSTMClassifierdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")model = LSTMClassifier(32, 100, 5000).to(device)optimizer = optim.Adam(model.parameters())loss_fn = torch.nn.BCELoss()train(model, train_sample_dl, 5, optimizer, loss_fn, device)Epoch: 1, BCELoss: 0.6932154417037963Epoch: 2, BCELoss: 0.683722734451294Epoch: 3, BCELoss: 0.675475811958313Epoch: 4, BCELoss: 0.6661329388618469Epoch: 5, BCELoss: 0.6543593883514405In order to construct a PyTorch model using SageMaker we must provide SageMaker with a training script. We may optionally include a directory which will be copied to the container and from which our training code will be run. When the training container is executed it will check the uploaded directory (if there is one) for a requirements.txt file and install any required Python libraries, after which the training script will be run.(TODO) Training the modelWhen a PyTorch model is constructed in SageMaker, an entry point must be specified. This is the Python file which will be executed when the model is trained. Inside of the train directory is a file called train.py which has been provided and which contains most of the necessary code to train our model. The only thing that is missing is the implementation of the train() method which you wrote earlier in this notebook.TODO: Copy the train() method written above and paste it into the train/train.py file where required.The way that SageMaker passes hyperparameters to the training script is by way of arguments. These arguments can then be parsed and used in the training script. To see how this is done take a look at the provided train/train.py file.from sagemaker.pytorch import PyTorchestimator = PyTorch(entry_point=\"train.py\",                    source_dir=\"train\",                    role=role,                    framework_version='0.4.0',                    train_instance_count=1,                    train_instance_type='ml.p2.xlarge',                    hyperparameters={                        'epochs': 10,                        'hidden_dim': 200,                    })estimator.fit({'training': input_data})'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.2020-06-23 14:42:13 Starting - Starting the training job...2020-06-23 14:42:15 Starting - Launching requested ML instances.........2020-06-23 14:43:45 Starting - Preparing the instances for training......2020-06-23 14:45:06 Downloading - Downloading input data......2020-06-23 14:46:08 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\u001b[34mbash: no job control in this shell\u001b[0m\u001b[34m2020-06-23 14:46:09,614 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\u001b[34m2020-06-23 14:46:09,638 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\u001b[34m2020-06-23 14:46:10,252 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\u001b[34m2020-06-23 14:46:10,534 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\u001b[34mGenerating setup.py\u001b[0m\u001b[34m2020-06-23 14:46:10,534 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\u001b[34m2020-06-23 14:46:10,534 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\u001b[34m2020-06-23 14:46:10,534 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\u001b[34mProcessing /opt/ml/code\u001b[0m\u001b[34mCollecting pandas (from -r requirements.txt (line 1))  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\u001b[34mCollecting numpy (from -r requirements.txt (line 2))\u001b[0m\u001b[34m  Downloading https://files.pythonhosted.org/packages/b5/36/88723426b4ff576809fec7d73594fe17a35c27f8d01f93637637a29ae25b/numpy-1.18.5-cp35-cp35m-manylinux1_x86_64.whl (19.9MB)\u001b[0m\u001b[34mCollecting nltk (from -r requirements.txt (line 3))  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\u001b[0m\u001b[34mCollecting beautifulsoup4 (from -r requirements.txt (line 4))  Downloading https://files.pythonhosted.org/packages/66/25/ff030e2437265616a1e9b25ccc864e0371a0bc3adb7c5a404fd661c6f4f6/beautifulsoup4-4.9.1-py3-none-any.whl (115kB)\u001b[0m\u001b[34mCollecting html5lib (from -r requirements.txt (line 5))  Downloading https://files.pythonhosted.org/packages/6c/dd/a834df6482147d48e225a49515aabc28974ad5a4ca3215c18a882565b028/html5lib-1.1-py2.py3-none-any.whl (112kB)\u001b[0m\u001b[34mCollecting pytz&gt;=2011k (from pandas-&gt;-r requirements.txt (line 1))\u001b[0m\u001b[34m  Downloading https://files.pythonhosted.org/packages/4f/a4/879454d49688e2fad93e59d7d4efda580b783c745fd2ec2a3adf87b0808d/pytz-2020.1-py2.py3-none-any.whl (510kB)\u001b[0m\u001b[34mRequirement already satisfied, skipping upgrade: python-dateutil&gt;=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas-&gt;-r requirements.txt (line 1)) (2.7.5)\u001b[0m\u001b[34mRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.5/dist-packages (from nltk-&gt;-r requirements.txt (line 3)) (7.0)\u001b[0m\u001b[34mCollecting joblib (from nltk-&gt;-r requirements.txt (line 3))  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\u001b[0m\u001b[34mCollecting regex (from nltk-&gt;-r requirements.txt (line 3))\u001b[0m\u001b[34m  Downloading https://files.pythonhosted.org/packages/b8/7b/01510a6229c2176425bda54d15fba05a4b3df169b87265b008480261d2f9/regex-2020.6.8.tar.gz (690kB)\u001b[0m\u001b[34mCollecting tqdm (from nltk-&gt;-r requirements.txt (line 3))  Downloading https://files.pythonhosted.org/packages/f3/76/4697ce203a3d42b2ead61127b35e5fcc26bba9a35c03b32a2bd342a4c869/tqdm-4.46.1-py2.py3-none-any.whl (63kB)\u001b[0m\u001b[34mCollecting soupsieve&gt;1.2 (from beautifulsoup4-&gt;-r requirements.txt (line 4))  Downloading https://files.pythonhosted.org/packages/6f/8f/457f4a5390eeae1cc3aeab89deb7724c965be841ffca6cfca9197482e470/soupsieve-2.0.1-py3-none-any.whl\u001b[0m\u001b[34mRequirement already satisfied, skipping upgrade: six&gt;=1.9 in /usr/local/lib/python3.5/dist-packages (from html5lib-&gt;-r requirements.txt (line 5)) (1.11.0)\u001b[0m\u001b[34mCollecting webencodings (from html5lib-&gt;-r requirements.txt (line 5))  Downloading https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl\u001b[0m\u001b[34mBuilding wheels for collected packages: nltk, train, regex  Running setup.py bdist_wheel for nltk: started\u001b[0m\u001b[34m  Running setup.py bdist_wheel for nltk: finished with status 'done'  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306  Running setup.py bdist_wheel for train: started  Running setup.py bdist_wheel for train: finished with status 'done'  Stored in directory: /tmp/pip-ephem-wheel-cache-9m1iclsp/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3  Running setup.py bdist_wheel for regex: started\u001b[0m\u001b[34m  Running setup.py bdist_wheel for regex: finished with status 'done'  Stored in directory: /root/.cache/pip/wheels/9c/e2/cf/246ad8c87bcdf3cba1ec95fa89bc205c9037aa8f4d2e26fdad\u001b[0m\u001b[34mSuccessfully built nltk train regex\u001b[0m\u001b[34mInstalling collected packages: pytz, numpy, pandas, joblib, regex, tqdm, nltk, soupsieve, beautifulsoup4, webencodings, html5lib, train  Found existing installation: numpy 1.15.4    Uninstalling numpy-1.15.4:\u001b[0m\u001b[34m      Successfully uninstalled numpy-1.15.4\u001b[0m\u001b[34mSuccessfully installed beautifulsoup4-4.9.1 html5lib-1.1 joblib-0.14.1 nltk-3.5 numpy-1.18.5 pandas-0.24.2 pytz-2020.1 regex-2020.6.8 soupsieve-2.0.1 tqdm-4.46.1 train-1.0.0 webencodings-0.5.1\u001b[0m\u001b[34mYou are using pip version 18.1, however version 20.2b1 is available.\u001b[0m\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\u001b[34m2020-06-23 14:46:33,463 sagemaker-containers INFO     Invoking user script\u001b[0m\u001b[34mTraining Env:\u001b[0m\u001b[34m{    \"output_dir\": \"/opt/ml/output\",    \"model_dir\": \"/opt/ml/model\",    \"num_cpus\": 4,    \"output_data_dir\": \"/opt/ml/output/data\",    \"module_dir\": \"s3://sagemaker-us-east-2-152592716204/sagemaker-pytorch-2020-06-23-14-42-13-201/source/sourcedir.tar.gz\",    \"user_entry_point\": \"train.py\",    \"current_host\": \"algo-1\",    \"hosts\": [        \"algo-1\"    ],    \"input_dir\": \"/opt/ml/input\",    \"num_gpus\": 1,    \"resource_config\": {        \"current_host\": \"algo-1\",        \"hosts\": [            \"algo-1\"        ],        \"network_interface_name\": \"eth0\"    },    \"framework_module\": \"sagemaker_pytorch_container.training:main\",    \"log_level\": 20,    \"hyperparameters\": {        \"hidden_dim\": 200,        \"epochs\": 10    },    \"input_config_dir\": \"/opt/ml/input/config\",    \"module_name\": \"train\",    \"network_interface_name\": \"eth0\",    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",    \"channel_input_dirs\": {        \"training\": \"/opt/ml/input/data/training\"    },    \"job_name\": \"sagemaker-pytorch-2020-06-23-14-42-13-201\",    \"input_data_config\": {        \"training\": {            \"TrainingInputMode\": \"File\",            \"RecordWrapperType\": \"None\",            \"S3DistributionType\": \"FullyReplicated\"        }    },    \"additional_framework_parameters\": {}\u001b[0m\u001b[34m}\u001b[0m\u001b[34mEnvironment variables:\u001b[0m\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\u001b[34mSM_LOG_LEVEL=20\u001b[0m\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\u001b[34mSM_MODULE_NAME=train\u001b[0m\u001b[34mSM_HP_HIDDEN_DIM=200\u001b[0m\u001b[34mSM_HPS={\"epochs\":10,\"hidden_dim\":200}\u001b[0m\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\u001b[34mSM_NUM_GPUS=1\u001b[0m\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_dim\":200},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2020-06-23-14-42-13-201\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-152592716204/sagemaker-pytorch-2020-06-23-14-42-13-201/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\u001b[34mSM_NUM_CPUS=4\u001b[0m\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\u001b[34mSM_HP_EPOCHS=10\u001b[0m\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-152592716204/sagemaker-pytorch-2020-06-23-14-42-13-201/source/sourcedir.tar.gz\u001b[0m\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_dim\",\"200\"]\u001b[0m\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\u001b[34mInvoking script with the following command:\u001b[0m\u001b[34m/usr/bin/python -m train --epochs 10 --hidden_dim 200\u001b[0m\u001b[34mUsing device cuda.\u001b[0m\u001b[34mGet train data loader.\u001b[0m\u001b[34mModel loaded with embedding_dim 32, hidden_dim 200, vocab_size 5000.\u001b[0m\u001b[34mEpoch: 1, BCELoss: 0.6726251220216557\u001b[0m\u001b[34mEpoch: 2, BCELoss: 0.5979122312701478\u001b[0m\u001b[34mEpoch: 3, BCELoss: 0.5270530496324811\u001b[0m\u001b[34mEpoch: 4, BCELoss: 0.4490671170001127\u001b[0m\u001b[34mEpoch: 5, BCELoss: 0.4128714714731489\u001b[0m\u001b[34mEpoch: 6, BCELoss: 0.40253147908619474\u001b[0m\u001b[34mEpoch: 7, BCELoss: 0.3597642718529215\u001b[0m\u001b[34mEpoch: 8, BCELoss: 0.33334174630593283\u001b[0m\u001b[34mEpoch: 9, BCELoss: 0.3121759356284628\u001b[0m2020-06-23 14:49:42 Uploading - Uploading generated training model2020-06-23 14:49:42 Completed - Training job completed\u001b[34mEpoch: 10, BCELoss: 0.2970385502795784\u001b[0m\u001b[34m2020-06-23 14:49:35,346 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0mTraining seconds: 276Billable seconds: 276Step 5: Testing the modelAs mentioned at the top of this notebook, we will be testing this model by first deploying it and then sending the testing data to the deployed endpoint. We will do this so that we can make sure that the deployed model is working correctly.Step 6: Deploy the model for testingNow that we have trained our model, we would like to test it to see how it performs. Currently our model takes input of the form review_length, review[500] where review[500] is a sequence of 500 integers which describe the words present in the review, encoded using word_dict. Fortunately for us, SageMaker provides built-in inference code for models with simple inputs such as this.There is one thing that we need to provide, however, and that is a function which loads the saved model. This function must be called model_fn() and takes as its only parameter a path to the directory where the model artifacts are stored. This function must also be present in the python file which we specified as the entry point. In our case the model loading function has been provided and so no changes need to be made.NOTE: When the built-in inference code is run it must import the model_fn() method from the train.py file. This is why the training code is wrapped in a main guard ( ie, if __name__ == '__main__': )Since we don’t need to change anything in the code that was uploaded during training, we can simply deploy the current model as-is.NOTE: When deploying a model you are asking SageMaker to launch an compute instance that will wait for data to be sent to it. As a result, this compute instance will continue to run until you shut it down. This is important to know since the cost of a deployed endpoint depends on how long it has been running for.In other words If you are no longer using a deployed endpoint, shut it down!TODO: Deploy the trained model.# TODO: Deploy the trained modelpredictor = estimator.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')Parameter image will be renamed to image_uri in SageMaker Python SDK v2.'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.Using already existing model: sagemaker-pytorch-2020-06-23-14-42-13-201---------------!Step 7 - Use the model for testingOnce deployed, we can read in the test data and send it off to our deployed model to get some results. Once we collect all of the results we can determine how accurate our model is.test_X = pd.concat([pd.DataFrame(test_X_len), pd.DataFrame(test_X)], axis=1)# We split the data into chunks and send each chunk seperately, accumulating the results.def predict(data, rows=512):    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))    predictions = np.array([])    for array in split_array:        predictions = np.append(predictions, predictor.predict(array))        return predictionspredictions = predict(test_X.values)predictions = [round(num) for num in predictions]from sklearn.metrics import accuracy_scoreaccuracy_score(test_y, predictions)0.84772Question: How does this model compare to the XGBoost model you created earlier? Why might these two models perform differently on this dataset? Which do you think is better for sentiment analysis?Answer:  The XGBoost model perfomed slightly better than LSTM Network here.  The first reason is that we are using a simple LSTM maybe stacking LSTM cells might give better results whereas XGBoost is initialized with a good set a parameters.  There is not a single best model for sentiment analysis since it depends on data and data preprocessing.(TODO) More testingWe now have a trained model which has been deployed and which we can send processed reviews to and which returns the predicted sentiment. However, ultimately we would like to be able to send our model an unprocessed review. That is, we would like to send the review itself as a string. For example, suppose we wish to send the following review to our model.test_review = 'The simplest pleasures in life are the best, and this film is one of them. Combining a rather basic storyline of love and adventure this movie transcends the usual weekend fair with wit and unmitigated charm.'The question we now need to answer is, how do we send this review to our model?Recall in the first section of this notebook we did a bunch of data processing to the IMDb dataset. In particular, we did two specific things to the provided reviews.  Removed any html tags and stemmed the input  Encoded the review as a sequence of integers using word_dictIn order process the review we will need to repeat these two steps.TODO: Using the review_to_words and convert_and_pad methods from section one, convert test_review into a numpy array test_data suitable to send to our model. Remember that our model expects input of the form review_length, review[500].# TODO: Convert test_review into a form usable by the model and save the results in test_datatest_review_data, test_review_len = convert_and_pad(word_dict, review_to_words(test_review))# combine review length and data in one numpy arraytest_data = np.array(test_review_data)test_data = np.insert(test_data, 0, test_review_len)# add empty batch dimensiontest_data = test_data[None, :]Now that we have processed the review, we can send the resulting array to our model to predict the sentiment of the review.predictor.predict(test_data)array(0.871142, dtype=float32)Since the return value of our model is close to 1, we can be certain that the review we submitted is positive.Delete the endpointOf course, just like in the XGBoost notebook, once we’ve deployed an endpoint it continues to run until we tell it to shut down. Since we are done using our endpoint for now, we can delete it.estimator.delete_endpoint()Step 6 (again) - Deploy the model for the web appNow that we know that our model is working, it’s time to create some custom inference code so that we can send the model a review which has not been processed and have it determine the sentiment of the review.As we saw above, by default the estimator which we created, when deployed, will use the entry script and directory which we provided when creating the model. However, since we now wish to accept a string as input and our model expects a processed review, we need to write some custom inference code.We will store the code that we write in the serve directory. Provided in this directory is the model.py file that we used to construct our model, a utils.py file which contains the review_to_words and convert_and_pad pre-processing functions which we used during the initial data processing, and predict.py, the file which will contain our custom inference code. Note also that requirements.txt is present which will tell SageMaker what Python libraries are required by our custom inference code.When deploying a PyTorch model in SageMaker, you are expected to provide four functions which the SageMaker inference container will use.  model_fn: This function is the same function that we used in the training script and it tells SageMaker how to load our model.  input_fn: This function receives the raw serialized input that has been sent to the model’s endpoint and its job is to de-serialize and make the input available for the inference code.  output_fn: This function takes the output of the inference code and its job is to serialize this output and return it to the caller of the model’s endpoint.  predict_fn: The heart of the inference script, this is where the actual prediction is done and is the function which you will need to complete.For the simple website that we are constructing during this project, the input_fn and output_fn methods are relatively straightforward. We only require being able to accept a string as input and we expect to return a single value as output. You might imagine though that in a more complex application the input or output may be image data or some other binary data which would require some effort to serialize.(TODO) Writing inference codeBefore writing our custom inference code, we will begin by taking a look at the code which has been provided.!pygmentize serve/predict.py\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_containers\u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LSTMClassifier\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m review_to_words, convert_and_pad\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):    \u001b[33m\"\"\"Load the PyTorch model from the `model_dir` directory.\"\"\"\u001b[39;49;00m    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)    \u001b[37m# First, load the parameters used to create the model.\u001b[39;49;00m    model_info = {}    model_info_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:        model_info = torch.load(f)    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_info: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info))    \u001b[37m# Determine the device and construct the model.\u001b[39;49;00m    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)    model = LSTMClassifier(model_info[\u001b[33m'\u001b[39;49;00m\u001b[33membedding_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mvocab_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])    \u001b[37m# Load the store model parameters.\u001b[39;49;00m    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:        model.load_state_dict(torch.load(f))    \u001b[37m# Load the saved word_dict.\u001b[39;49;00m    word_dict_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mword_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(word_dict_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:        model.word_dict = pickle.load(f)    model.to(device).eval()    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDone loading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)    \u001b[34mreturn\u001b[39;49;00m model\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(serialized_input_data, content_type):    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mDeserializing the input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)    \u001b[34mif\u001b[39;49;00m content_type == \u001b[33m'\u001b[39;49;00m\u001b[33mtext/plain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:        data = serialized_input_data.decode(\u001b[33m'\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)        \u001b[34mreturn\u001b[39;49;00m data    \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in content_type: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + content_type)\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction_output, accept):    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSerializing the generated output.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(prediction_output)\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, model):    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mInferring sentiment of input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)        \u001b[34mif\u001b[39;49;00m model.word_dict \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:        \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mModel has not been loaded properly, no word_dict.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)        \u001b[37m# TODO: Process input_data so that it is ready to be sent to our model.\u001b[39;49;00m    \u001b[37m#       You should produce two variables:\u001b[39;49;00m    \u001b[37m#         data_X   - A sequence of length 500 which represents the converted review\u001b[39;49;00m    \u001b[37m#         data_len - The length of the review\u001b[39;49;00m    data_X = \u001b[34mNone\u001b[39;49;00m    data_len = \u001b[34mNone\u001b[39;49;00m    \u001b[37m# Using data_X and data_len we construct an appropriate input tensor. Remember\u001b[39;49;00m    \u001b[37m# that our model expects input data of the form 'len, review[500]'.\u001b[39;49;00m    data_pack = np.hstack((data_len, data_X))    data_pack = data_pack.reshape(\u001b[34m1\u001b[39;49;00m, -\u001b[34m1\u001b[39;49;00m)        data = torch.from_numpy(data_pack)    data = data.to(device)    \u001b[37m# Make sure to put the model into evaluation mode\u001b[39;49;00m    model.eval()    \u001b[37m# TODO: Compute the result of applying the model to the input data. The variable `result` should\u001b[39;49;00m    \u001b[37m#       be a numpy array which contains a single integer which is either 1 or 0\u001b[39;49;00m    result = \u001b[34mNone\u001b[39;49;00m    \u001b[34mreturn\u001b[39;49;00m resultAs mentioned earlier, the model_fn method is the same as the one provided in the training code and the input_fn and output_fn methods are very simple and your task will be to complete the predict_fn method. Make sure that you save the completed file as predict.py in the serve directory.TODO: Complete the predict_fn() method in the serve/predict.py file.Deploying the modelNow that the custom inference code has been written, we will create and deploy our model. To begin with, we need to construct a new PyTorchModel object which points to the model artifacts created during training and also points to the inference code that we wish to use. Then we can call the deploy method to launch the deployment container.NOTE: The default behaviour for a deployed PyTorch model is to assume that any input passed to the predictor is a numpy array. In our case we want to send a string so we need to construct a simple wrapper around the RealTimePredictor class to accomodate simple strings. In a more complicated situation you may want to provide a serialization object, for example if you wanted to sent image data.from sagemaker.predictor import RealTimePredictorfrom sagemaker.pytorch import PyTorchModelclass StringPredictor(RealTimePredictor):    def __init__(self, endpoint_name, sagemaker_session):        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')model = PyTorchModel(model_data=estimator.model_data,                     role = role,                     framework_version='0.4.0',                     entry_point='predict.py',                     source_dir='serve',                     predictor_cls=StringPredictor)predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')Parameter image will be renamed to image_uri in SageMaker Python SDK v2.'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.---------------!Testing the modelNow that we have deployed our model with the custom inference code, we should test to see if everything is working. Here we test our model by loading the first 250 positive and negative reviews and send them to the endpoint, then collect the results. The reason for only sending some of the data is that the amount of time it takes for our model to process the input and then perform inference is quite long and so testing the entire data set would be prohibitive.import globdef test_reviews(data_dir='../data/aclImdb', stop=250):        results = []    ground = []        # We make sure to test both positive and negative reviews        for sentiment in ['pos', 'neg']:                path = os.path.join(data_dir, 'test', sentiment, '*.txt')        files = glob.glob(path)                files_read = 0                print('Starting ', sentiment, ' files')                # Iterate through the files and send them to the predictor        for f in files:            with open(f) as review:                # First, we store the ground truth (was the review positive or negative)                if sentiment == 'pos':                    ground.append(1)                else:                    ground.append(0)                # Read in the review and convert to 'utf-8' for transmission via HTTP                review_input = review.read().encode('utf-8')                # Send the review to the predictor and store the results                results.append(int(predictor.predict(review_input)))                            # Sending reviews to our endpoint one at a time takes a while so we            # only send a small number of reviews            files_read += 1            if files_read == stop:                break                return ground, resultsground, results = test_reviews()Starting  pos  filesStarting  neg  filesfrom sklearn.metrics import accuracy_scoreaccuracy_score(ground, results)0.858As an additional test, we can try sending the test_review that we looked at earlier.predictor.predict(test_review)b'1'Now that we know our endpoint is working as expected, we can set up the web page that will interact with it. If you don’t have time to finish the project now, make sure to skip down to the end of this notebook and shut down your endpoint. You can deploy it again when you come back.Step 7 (again): Use the model for the web app  TODO: This entire section and the next contain tasks for you to complete, mostly using the AWS console.So far we have been accessing our model endpoint by constructing a predictor object which uses the endpoint and then just using the predictor object to perform inference. What if we wanted to create a web app which accessed our model? The way things are set up currently makes that not possible since in order to access a SageMaker endpoint the app would first have to authenticate with AWS using an IAM role which included access to SageMaker endpoints. However, there is an easier way! We just need to use some additional AWS services.The diagram above gives an overview of how the various services will work together. On the far right is the model which we trained above and which is deployed using SageMaker. On the far left is our web app that collects a user’s movie review, sends it off and expects a positive or negative sentiment in return.In the middle is where some of the magic happens. We will construct a Lambda function, which you can think of as a straightforward Python function that can be executed whenever a specified event occurs. We will give this function permission to send and recieve data from a SageMaker endpoint.Lastly, the method we will use to execute the Lambda function is a new endpoint that we will create using API Gateway. This endpoint will be a url that listens for data to be sent to it. Once it gets some data it will pass that data on to the Lambda function and then return whatever the Lambda function returns. Essentially it will act as an interface that lets our web app communicate with the Lambda function.Setting up a Lambda functionThe first thing we are going to do is set up a Lambda function. This Lambda function will be executed whenever our public API has data sent to it. When it is executed it will receive the data, perform any sort of processing that is required, send the data (the review) to the SageMaker endpoint we’ve created and then return the result.Part A: Create an IAM Role for the Lambda functionSince we want the Lambda function to call a SageMaker endpoint, we need to make sure that it has permission to do so. To do this, we will construct a role that we can later give the Lambda function.Using the AWS Console, navigate to the IAM page and click on Roles. Then, click on Create role. Make sure that the AWS service is the type of trusted entity selected and choose Lambda as the service that will use this role, then click Next: Permissions.In the search box type sagemaker and select the check box next to the AmazonSageMakerFullAccess policy. Then, click on Next: Review.Lastly, give this role a name. Make sure you use a name that you will remember later on, for example LambdaSageMakerRole. Then, click on Create role.Part B: Create a Lambda functionNow it is time to actually create the Lambda function.Using the AWS Console, navigate to the AWS Lambda page and click on Create a function. When you get to the next page, make sure that Author from scratch is selected. Now, name your Lambda function, using a name that you will remember later on, for example sentiment_analysis_func. Make sure that the Python 3.6 runtime is selected and then choose the role that you created in the previous part. Then, click on Create Function.On the next page you will see some information about the Lambda function you’ve just created. If you scroll down you should see an editor in which you can write the code that will be executed when your Lambda function is triggered. In our example, we will use the code below.# We need to use the low-level library to interact with SageMaker since the SageMaker API# is not available natively through Lambda.import boto3def lambda_handler(event, context):    # The SageMaker runtime is what allows us to invoke the endpoint that we've created.    runtime = boto3.Session().client('sagemaker-runtime')    # Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given    response = runtime.invoke_endpoint(EndpointName = '**ENDPOINT NAME HERE**',    # The name of the endpoint we created                                       ContentType = 'text/plain',                 # The data format that is expected                                       Body = event['body'])                       # The actual review    # The response is an HTTP response whose body contains the result of our inference    result = response['Body'].read().decode('utf-8')    return {        'statusCode' : 200,        'headers' : { 'Content-Type' : 'text/plain', 'Access-Control-Allow-Origin' : '*' },        'body' : result    }Once you have copy and pasted the code above into the Lambda code editor, replace the **ENDPOINT NAME HERE** portion with the name of the endpoint that we deployed earlier. You can determine the name of the endpoint using the code cell below.predictor.endpoint'sagemaker-pytorch-2020-06-23-15-40-30-967'Once you have added the endpoint name to the Lambda function, click on Save. Your Lambda function is now up and running. Next we need to create a way for our web app to execute the Lambda function.Setting up API GatewayNow that our Lambda function is set up, it is time to create a new API using API Gateway that will trigger the Lambda function we have just created.Using AWS Console, navigate to Amazon API Gateway and then click on Get started.On the next page, make sure that New API is selected and give the new api a name, for example, sentiment_analysis_api. Then, click on Create API.Now we have created an API, however it doesn’t currently do anything. What we want it to do is to trigger the Lambda function that we created earlier.Select the Actions dropdown menu and click Create Method. A new blank method will be created, select its dropdown menu and select POST, then click on the check mark beside it.For the integration point, make sure that Lambda Function is selected and click on the Use Lambda Proxy integration. This option makes sure that the data that is sent to the API is then sent directly to the Lambda function with no processing. It also means that the return value must be a proper response object as it will also not be processed by API Gateway.Type the name of the Lambda function you created earlier into the Lambda Function text entry box and then click on Save. Click on OK in the pop-up box that then appears, giving permission to API Gateway to invoke the Lambda function you created.The last step in creating the API Gateway is to select the Actions dropdown and click on Deploy API. You will need to create a new Deployment stage and name it anything you like, for example prod.You have now successfully set up a public API to access your SageMaker model. Make sure to copy or write down the URL provided to invoke your newly created public API as this will be needed in the next step. This URL can be found at the top of the page, highlighted in blue next to the text Invoke URL.Step 4: Deploying our web appNow that we have a publicly available API, we can start using it in a web app. For our purposes, we have provided a simple static html file which can make use of the public api you created earlier.In the website folder there should be a file called index.html. Download the file to your computer and open that file up in a text editor of your choice. There should be a line which contains **REPLACE WITH PUBLIC API URL**. Replace this string with the url that you wrote down in the last step and then save the file.Now, if you open index.html on your local computer, your browser will behave as a local web server and you can use the provided site to interact with your SageMaker model.If you’d like to go further, you can host this html file anywhere you’d like, for example using github or hosting a static site on Amazon’s S3. Once you have done this you can share the link with anyone you’d like and have them play with it too!  Important Note In order for the web app to communicate with the SageMaker endpoint, the endpoint has to actually be deployed and running. This means that you are paying for it. Make sure that the endpoint is running when you want to use the web app but that you shut it down when you don’t need it, otherwise you will end up with a surprisingly large AWS bill.TODO: Make sure that you include the edited index.html file in your project submission.Now that your web app is working, trying playing around with it and see how well it works.Question: Give an example of a review that you entered into your web app. What was the predicted sentiment of your example review?Answer:  Review : “Movie of the year! Deserves all the praise.”  Predicted sentiment: “Your review was POSITIVE!”Delete the endpointRemember to always shut down your endpoint if you are no longer using it. You are charged for the length of time that the endpoint is running so if you forget and leave it on you could end up with an unexpectedly large bill.predictor.delete_endpoint()",
        "url": "/deep%20learning/2020/06/25/sentiment-analysis-on-aws-sagemaker/",
        "date": "Jun 25, 2020"
      }
      ,
    
      "deep-20learning-2020-06-14-face-generation": {
        "title": "Face Generation with GAN",
        "author": "",
        "category": "",
        "content": "Face Generation with GANIn this project, you’ll define and train a DCGAN on a dataset of faces. Your goal is to get a generator network to generate new images of faces that look as realistic as possible!The project will be broken down into a series of tasks from loading in data to defining and training adversarial networks. At the end of the notebook, you’ll be able to visualize the results of your trained Generator to see how it performs; your generated samples should look like fairly realistic faces with small amounts of noise.Get the DataYou’ll be using the CelebFaces Attributes Dataset (CelebA) to train your adversarial networks.This dataset is more complex than the number datasets (like MNIST or SVHN) you’ve been working with, and so, you should prepare to define deeper networks and train them for a longer time to get good results. It is suggested that you utilize a GPU for training.Pre-processed DataSince the project’s main focus is on building the GANs, we’ve done some of the pre-processing for you. Each of the CelebA images has been cropped to remove parts of the image that don’t include a face, then resized down to 64x64x3 NumPy images. Some sample data is show below.  If you are working locally, you can download this data by clicking hereThis is a zip file that you’ll need to extract in the home directory of this notebook for further loading and processing. After extracting the data, you should be left with a directory of data processed_celeba_small/# can comment out after executing# !unzip processed_celeba_small.zipdata_dir = 'processed_celeba_small/'\"\"\"DON'T MODIFY ANYTHING IN THIS CELL\"\"\"import pickle as pklimport matplotlib.pyplot as pltimport numpy as npimport problem_unittests as tests#import helper%matplotlib inlineVisualize the CelebA DataThe CelebA dataset contains over 200,000 celebrity images with annotations. Since you’re going to be generating faces, you won’t need the annotations, you’ll only need the images. Note that these are color images with 3 color channels (RGB) each.Pre-process and Load the DataSince the project’s main focus is on building the GANs, we’ve done some of the pre-processing for you. Each of the CelebA images has been cropped to remove parts of the image that don’t include a face, then resized down to 64x64x3 NumPy images. This pre-processed dataset is a smaller subset of the very large CelebA data.  There are a few other steps that you’ll need to transform this data and create a DataLoader.Exercise: Complete the following get_dataloader function, such that it satisfies these requirements:  Your images should be square, Tensor images of size image_size x image_size in the x and y dimension.  Your function should return a DataLoader that shuffles and batches these Tensor images.ImageFolderTo create a dataset given a directory of images, it’s recommended that you use PyTorch’s ImageFolder wrapper, with a root directory processed_celeba_small/ and data transformation passed in.# necessary importsimport torchfrom torchvision import datasetsfrom torchvision import transformsdef get_dataloader(batch_size, image_size, data_dir='processed_celeba_small/'):    \"\"\"    Batch the neural network data using DataLoader    :param batch_size: The size of each batch; the number of images in a batch    :param img_size: The square size of the image data (x, y)    :param data_dir: Directory where image data is located    :return: DataLoader with batched data    \"\"\"        # TODO: Implement function and return a dataloader    # resize and normalize the images    transform = transforms.Compose([transforms.Resize(image_size),                                     transforms.ToTensor()])    # define datasets using ImageFolder    train_dataset = datasets.ImageFolder(data_dir, transform)    # create and return DataLoaders    data_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)        return data_loaderCreate a DataLoaderExercise: Create a DataLoader celeba_train_loader with appropriate hyperparameters.Call the above function and create a dataloader to view images.  You can decide on any reasonable batch_size parameter  Your image_size must be 32. Resizing the data to a smaller size will make for faster training, while still creating convincing images of faces!# Define function hyperparametersbatch_size = 32img_size = 32\"\"\"DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\"\"\"# Call your function and get a dataloaderceleba_train_loader = get_dataloader(batch_size, img_size)Next, you can view some images! You should seen square images of somewhat-centered faces.Note: You’ll need to convert the Tensor images into a NumPy type and transpose the dimensions to correctly display an image, suggested imshow code is below, but it may not be perfect.# helper display functiondef imshow(img):    npimg = img.numpy()    plt.imshow(np.transpose(npimg, (1, 2, 0)))\"\"\"DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\"\"\"# obtain one batch of training imagesdataiter = iter(celeba_train_loader)images, _ = dataiter.next() # _ for no labels# plot the images in the batch, along with the corresponding labelsfig = plt.figure(figsize=(20, 4))plot_size=20for idx in np.arange(plot_size):    ax = fig.add_subplot(2, plot_size/2, idx+1, xticks=[], yticks=[])    imshow(images[idx])Exercise: Pre-process your image data and scale it to a pixel range of -1 to 1You need to do a bit of pre-processing; you know that the output of a tanh activated generator will contain pixel values in a range from -1 to 1, and so, we need to rescale our training images to a range of -1 to 1. (Right now, they are in a range from 0-1.)# TODO: Complete the scale functiondef scale(x, feature_range=(-1, 1)):    ''' Scale takes in an image x and returns that image, scaled       with a feature_range of pixel values from -1 to 1.        This function assumes that the input x is already scaled from 0-1.'''    # assume x is scaled to (0, 1)    # scale to feature_range and return scaled x    min,max = feature_range    x = x*(max - min) + min    return x\"\"\"DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\"\"\"# check scaled range# should be close to -1 to 1img = images[0]scaled_img = scale(img)print('Min: ', scaled_img.min())print('Max: ', scaled_img.max())Min:  tensor(-0.9922)Max:  tensor(1.)Define the ModelA GAN is comprised of two adversarial networks, a discriminator and a generator.DiscriminatorYour first task will be to define the discriminator. This is a convolutional classifier like you’ve built before, only without any maxpooling layers. To deal with this complex data, it’s suggested you use a deep network with normalization. You are also allowed to create any helper functions that may be useful.Exercise: Complete the Discriminator class  The inputs to the discriminator are 32x32x3 tensor images  The output should be a single value that will indicate whether a given image is real or fakeimport torch.nn as nnimport torch.nn.functional as F# helper to build a convolution layerdef conv(in_channels,out_channels,kernel_size,stride = 2,padding = 1,batch_norm = True):    layers = []    conv_layer = nn.Conv2d(in_channels = in_channels,out_channels = out_channels,                      kernel_size = kernel_size,stride = stride,padding = padding,bias= False)        layers.append(conv_layer)    if batch_norm == True:        layers.append(nn.BatchNorm2d(out_channels))        return nn.Sequential(*layers)class Discriminator(nn.Module):    def __init__(self, conv_dim=32):        \"\"\"        Initialize the Discriminator Module        :param conv_dim: The depth of the first convolutional layer        \"\"\"        super(Discriminator, self).__init__()        self.conv_dim = conv_dim        # covolution layers                # input 32 x 32 x 3 -&gt; output 16 x 16 x 32        self.conv1 = conv(3,conv_dim,4,batch_norm = False)        # input 16 x 16 x 32 -&gt;  output 8 x 8 x 64        self.conv2 = conv(conv_dim,conv_dim*2,4)        # input 8 x 8 x 64 -&gt; output 4 x 4 x 128        self.conv3 = conv(conv_dim*2,conv_dim*4,4)        # input 4 x 4 x 128 -&gt; output 2 x 2 x 256        self.conv4 = conv(conv_dim*4,conv_dim*8,4)                # classification layers        self.fc = nn.Linear(conv_dim*8*2*2,1)            def forward(self, x):        \"\"\"        Forward propagation of the neural network        :param x: The input to the neural network             :return: Discriminator logits; the output of the neural network        \"\"\"        # define feedforward behavior        x = F.leaky_relu(self.conv1(x),0.2)        x = F.leaky_relu(self.conv2(x),0.2)        x = F.leaky_relu(self.conv3(x),0.2)        x = F.leaky_relu(self.conv4(x),0.2)                # output        x = x.view(-1,self.conv_dim*8*2*2)        x = self.fc(x)            return x\"\"\"DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\"\"\"tests.test_discriminator(Discriminator)Tests PassedGeneratorThe generator should upsample an input and generate a new image of the same size as our training data 32x32x3. This should be mostly transpose convolutional layers with normalization applied to the outputs.Exercise: Complete the Generator class  The inputs to the generator are vectors of some length z_size  The output should be a image of shape 32x32x3def deconv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):    # create a sequence of transpose + optional batch norm layers    layers = []    transpose_conv_layer = nn.ConvTranspose2d(in_channels, out_channels,                                               kernel_size, stride, padding, bias=False)    # append transpose convolutional layer    layers.append(transpose_conv_layer)        if batch_norm:        # append batchnorm layer        layers.append(nn.BatchNorm2d(out_channels))            return nn.Sequential(*layers)class Generator(nn.Module):        def __init__(self, z_size, conv_dim = 32):        \"\"\"        Initialize the Generator Module        :param z_size: The length of the input latent vector, z        :param conv_dim: The depth of the inputs to the *last* transpose convolutional layer        \"\"\"        super(Generator, self).__init__()        self.conv_dim = conv_dim                self.fc = nn.Linear(z_size,conv_dim*8*2*2)                self.t_conv1 = deconv(conv_dim*8,conv_dim*4,4)        self.t_conv2 = deconv(conv_dim*4,conv_dim*2,4)        self.t_conv3 = deconv(conv_dim*2,conv_dim,4)        self.t_conv4 = deconv(conv_dim,3,4,batch_norm = False)    def forward(self, x):        \"\"\"        Forward propagation of the neural network        :param x: The input to the neural network             :return: A 32x32x3 Tensor image as output        \"\"\"        # define feedforward behavior        x = self.fc(x)        x = x.view(-1,self.conv_dim*8,2,2)                x = F.relu(self.t_conv1(x))        x = F.relu(self.t_conv2(x))        x = F.relu(self.t_conv3(x))        x = F.tanh(self.t_conv4(x))                return x\"\"\"DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\"\"\"tests.test_generator(Generator)Tests PassedInitialize the weights of your networksTo help your models converge, you should initialize the weights of the convolutional and linear layers in your model. From reading the original DCGAN paper, they say:  All weights were initialized from a zero-centered Normal distribution with standard deviation 0.02.So, your next task will be to define a weight initialization function that does just this!You can refer back to the lesson on weight initialization or even consult existing model code, such as that from the networks.py file in CycleGAN Github repository to help you complete this function.Exercise: Complete the weight initialization function  This should initialize only convolutional and linear layers  Initialize the weights to a normal distribution, centered around 0, with a standard deviation of 0.02.  The bias terms, if they exist, may be left alone or set to 0.def weights_init_normal(m):    \"\"\"    Applies initial weights to certain layers in a model .    The weights are taken from a normal distribution     with mean = 0, std dev = 0.02.    :param m: A module or layer in a network        \"\"\"    # classname will be something like:    # `Conv`, `BatchNorm2d`, `Linear`, etc.    classname = m.__class__.__name__        # TODO: Apply initial weights to convolutional and linear layers    if classname.find('Conv') != -1 or classname.find('Linear') != -1:        nn.init.normal_(m.weight.data, 0, 0.02)    if hasattr(m, 'bias') and m.bias is not None:        m.bias.data.fill_(0)Build complete networkDefine your models’ hyperparameters and instantiate the discriminator and generator from the classes defined above. Make sure you’ve passed in the correct input arguments.\"\"\"DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\"\"\"def build_network(d_conv_dim, g_conv_dim, z_size):    # define discriminator and generator    D = Discriminator(d_conv_dim)    G = Generator(z_size=z_size, conv_dim=g_conv_dim)    # initialize model weights    D.apply(weights_init_normal)    G.apply(weights_init_normal)    print(D)    print()    print(G)        return D, GExercise: Define model hyperparameters# Define model hyperparamsd_conv_dim = 32g_conv_dim = 32z_size = 100\"\"\"DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\"\"\"D, G = build_network(d_conv_dim, g_conv_dim, z_size)Discriminator(  (conv1): Sequential(    (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)  )  (conv2): Sequential(    (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  )  (conv3): Sequential(    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  )  (conv4): Sequential(    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  )  (fc): Linear(in_features=1024, out_features=1, bias=True))Generator(  (fc): Linear(in_features=100, out_features=1024, bias=True)  (t_conv1): Sequential(    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  )  (t_conv2): Sequential(    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  )  (t_conv3): Sequential(    (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  )  (t_conv4): Sequential(    (0): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)  ))Training on GPUCheck if you can train on GPU. Here, we’ll set this as a boolean variable train_on_gpu. Later, you’ll be responsible for making sure that      Models,    Model inputs, and    Loss function arguments  Are moved to GPU, where appropriate.\"\"\"DON'T MODIFY ANYTHING IN THIS CELL\"\"\"import torch# Check for a GPUtrain_on_gpu = torch.cuda.is_available()if not train_on_gpu:    print('No GPU found. Please use a GPU to train your neural network.')else:    print('Training on GPU!')Training on GPU!Discriminator and Generator LossesNow we need to calculate the losses for both types of adversarial networks.Discriminator Losses      For the discriminator, the total loss is the sum of the losses for real and fake images, d_loss = d_real_loss + d_fake_loss.    Remember that we want the discriminator to output 1 for real images and 0 for fake images, so we need to set up the losses to reflect that.  Generator LossThe generator loss will look similar only with flipped labels. The generator’s goal is to get the discriminator to think its generated images are real.Exercise: Complete real and fake loss functionsYou may choose to use either cross entropy or a least squares error loss to complete the following real_loss and fake_loss functions.def real_loss(D_out,smooth=False):    batch_size = D_out.size(0)    if smooth:        # smooth, real labels = 0.9        labels = torch.ones(batch_size)*0.9    else:        labels = torch.ones(batch_size) # real labels = 1    # move labels to GPU if available         if train_on_gpu:        labels = labels.cuda()    # binary cross entropy with logits loss    criterion = nn.BCEWithLogitsLoss()    # calculate loss    loss = criterion(D_out.squeeze(), labels)    return lossdef fake_loss(D_out):    '''Calculates how close discriminator outputs are to being fake.       param, D_out: discriminator logits       return: fake loss'''    batch_size = D_out.size(0)    labels = torch.zeros(batch_size) # fake labels = 0    if train_on_gpu:        labels = labels.cuda()    criterion = nn.BCEWithLogitsLoss()    # calculate loss    loss = criterion(D_out.squeeze(), labels)    return lossOptimizersExercise: Define optimizers for your Discriminator (D) and Generator (G)Define optimizers for your models with appropriate hyperparameters.import torch.optim as optim# Create optimizers for the discriminator D and generator Glr = 0.0002beta1=0.5beta2=0.999 # default value# Create optimizers for the discriminator and generatord_optimizer = optim.Adam(D.parameters(), lr, [beta1, beta2])g_optimizer = optim.Adam(G.parameters(), lr, [beta1, beta2])TrainingTraining will involve alternating between training the discriminator and the generator. You’ll use your functions real_loss and fake_loss to help you calculate the discriminator losses.  You should train the discriminator by alternating on real and fake images  Then the generator, which tries to trick the discriminator and should have an opposing loss functionSaving SamplesYou’ve been given some code to print out some loss statistics and save some generated “fake” samples.Exercise: Complete the training functionKeep in mind that, if you’ve moved your models to GPU, you’ll also have to move any model inputs to GPU.def train(D, G, n_epochs, print_every=50):    '''Trains adversarial networks for some number of epochs       param, D: the discriminator network       param, G: the generator network       param, n_epochs: number of epochs to train for       param, print_every: when to print and record the models' losses       return: D and G losses'''        # move models to GPU    if train_on_gpu:        D.cuda()        G.cuda()    # keep track of loss and generated, \"fake\" samples    samples = []    losses = []    # Get some fixed data for sampling. These are images that are held    # constant throughout training, and allow us to inspect the model's performance    sample_size=16    fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))    fixed_z = torch.from_numpy(fixed_z).float()    # move z to GPU if available    if train_on_gpu:        fixed_z = fixed_z.cuda()    # epoch training loop    for epoch in range(n_epochs):        # batch training loop        for batch_i, (real_images, _) in enumerate(celeba_train_loader):            batch_size = real_images.size(0)            real_images = scale(real_images)            # ===============================================            #         YOUR CODE HERE: TRAIN THE NETWORKS            # ===============================================                        # ============================================            #            TRAIN THE DISCRIMINATOR            # ============================================                    d_optimizer.zero_grad()            # 1. Train the discriminator on real and fake images                        # Train with real images            if train_on_gpu:                real_images = real_images.cuda()                        D_real = D(real_images)            d_real_loss = real_loss(D_real)                        # 2. Train with fake images                    # Generate fake images            z = np.random.uniform(-1, 1, size=(batch_size, z_size))            z = torch.from_numpy(z).float()            # move x to GPU, if available            if train_on_gpu:                z = z.cuda()            fake_images = G(z)                        # Compute the discriminator losses on fake images                        D_fake = D(fake_images)            d_fake_loss = fake_loss(D_fake)            # add up loss and perform backprop            d_loss = d_real_loss + d_fake_loss            d_loss.backward()            d_optimizer.step()                         # =========================================            #            TRAIN THE GENERATOR            # =========================================            g_optimizer.zero_grad()            # 1. Train with fake images and flipped labels            # Generate fake images            z = np.random.uniform(-1, 1, size=(batch_size, z_size))            z = torch.from_numpy(z).float()            if train_on_gpu:                z = z.cuda()            fake_images = G(z)            # Compute the discriminator losses on fake images             # using flipped labels!            D_fake = D(fake_images)            g_loss = real_loss(D_fake) # use real loss to flip labels            # perform backprop            g_loss.backward()            g_optimizer.step()                                                        # ===============================================            #              END OF YOUR CODE            # ===============================================            # Print some loss stats            if batch_i % print_every == 0:                # append discriminator loss and generator loss                losses.append((d_loss.item(), g_loss.item()))                # print discriminator and generator loss                print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(                        epoch+1, n_epochs, d_loss.item(), g_loss.item()))        ## AFTER EACH EPOCH##            # this code assumes your generator is named G, feel free to change the name        # generate and save sample, fake images        G.eval() # for generating samples        samples_z = G(fixed_z)        samples.append(samples_z)        G.train() # back to training mode    # Save training generator samples    with open('train_samples.pkl', 'wb') as f:        pkl.dump(samples, f)        # finally return losses    return lossesSet your number of training epochs and train your GAN!# set number of epochs n_epochs = 10\"\"\"DON'T MODIFY ANYTHING IN THIS CELL\"\"\"# call training functionlosses = train(D, G, n_epochs=n_epochs)Epoch [    1/   10] | d_loss: 1.4375 | g_loss: 0.8283Epoch [    1/   10] | d_loss: 0.1367 | g_loss: 3.4099Epoch [    1/   10] | d_loss: 0.0330 | g_loss: 4.5209Epoch [    1/   10] | d_loss: 0.0989 | g_loss: 4.5074Epoch [    1/   10] | d_loss: 0.2173 | g_loss: 3.7985Epoch [    1/   10] | d_loss: 0.2060 | g_loss: 3.5710Epoch [    1/   10] | d_loss: 0.4686 | g_loss: 4.2082Epoch [    1/   10] | d_loss: 0.9417 | g_loss: 4.9502Epoch [    1/   10] | d_loss: 0.1812 | g_loss: 2.7069Epoch [    1/   10] | d_loss: 0.2640 | g_loss: 3.7460Epoch [    1/   10] | d_loss: 0.3547 | g_loss: 3.1814Epoch [    1/   10] | d_loss: 0.9886 | g_loss: 1.3821Epoch [    1/   10] | d_loss: 0.6250 | g_loss: 2.3265Epoch [    1/   10] | d_loss: 0.4463 | g_loss: 2.8969Epoch [    1/   10] | d_loss: 0.8534 | g_loss: 2.5419Epoch [    1/   10] | d_loss: 0.6156 | g_loss: 3.4758Epoch [    1/   10] | d_loss: 0.6725 | g_loss: 2.4061Epoch [    1/   10] | d_loss: 0.5060 | g_loss: 3.2003Epoch [    1/   10] | d_loss: 1.2175 | g_loss: 1.5792Epoch [    1/   10] | d_loss: 1.0904 | g_loss: 1.3760Epoch [    1/   10] | d_loss: 1.0209 | g_loss: 1.4324Epoch [    1/   10] | d_loss: 0.7405 | g_loss: 2.5089Epoch [    1/   10] | d_loss: 0.7077 | g_loss: 2.0425Epoch [    1/   10] | d_loss: 0.9027 | g_loss: 1.8268Epoch [    1/   10] | d_loss: 0.7777 | g_loss: 1.2721Epoch [    1/   10] | d_loss: 0.8589 | g_loss: 1.4374Epoch [    1/   10] | d_loss: 0.5048 | g_loss: 2.3149Epoch [    1/   10] | d_loss: 0.9330 | g_loss: 2.0192Epoch [    1/   10] | d_loss: 0.7803 | g_loss: 1.7245Epoch [    1/   10] | d_loss: 0.8362 | g_loss: 2.3229Epoch [    1/   10] | d_loss: 0.5941 | g_loss: 1.9812Epoch [    1/   10] | d_loss: 0.6449 | g_loss: 1.8468Epoch [    1/   10] | d_loss: 0.8615 | g_loss: 3.1905Epoch [    1/   10] | d_loss: 1.0127 | g_loss: 2.1786Epoch [    1/   10] | d_loss: 0.7481 | g_loss: 1.3780Epoch [    1/   10] | d_loss: 0.6624 | g_loss: 2.0557Epoch [    1/   10] | d_loss: 1.2710 | g_loss: 0.8236Epoch [    1/   10] | d_loss: 0.7089 | g_loss: 1.8478Epoch [    1/   10] | d_loss: 1.1152 | g_loss: 2.5029Epoch [    1/   10] | d_loss: 0.5635 | g_loss: 1.5920Epoch [    1/   10] | d_loss: 0.8962 | g_loss: 2.8575Epoch [    1/   10] | d_loss: 1.1699 | g_loss: 3.3993Epoch [    1/   10] | d_loss: 0.6756 | g_loss: 1.6613Epoch [    1/   10] | d_loss: 0.8714 | g_loss: 1.6527Epoch [    1/   10] | d_loss: 0.6948 | g_loss: 2.4757Epoch [    1/   10] | d_loss: 0.9373 | g_loss: 1.5293Epoch [    1/   10] | d_loss: 0.7582 | g_loss: 1.7391Epoch [    1/   10] | d_loss: 0.8760 | g_loss: 1.5461Epoch [    1/   10] | d_loss: 1.0955 | g_loss: 2.5232Epoch [    1/   10] | d_loss: 0.8702 | g_loss: 1.9970Epoch [    1/   10] | d_loss: 0.6099 | g_loss: 1.8245Epoch [    1/   10] | d_loss: 0.5941 | g_loss: 3.0635Epoch [    1/   10] | d_loss: 0.7388 | g_loss: 1.6954Epoch [    1/   10] | d_loss: 0.8215 | g_loss: 1.4895Epoch [    1/   10] | d_loss: 0.4918 | g_loss: 2.6645Epoch [    1/   10] | d_loss: 0.7824 | g_loss: 3.0270Epoch [    1/   10] | d_loss: 0.7255 | g_loss: 2.5206Epoch [    2/   10] | d_loss: 0.7361 | g_loss: 1.4261Epoch [    2/   10] | d_loss: 0.8221 | g_loss: 1.3983Epoch [    2/   10] | d_loss: 0.6529 | g_loss: 1.5358Epoch [    2/   10] | d_loss: 0.7648 | g_loss: 1.1755Epoch [    2/   10] | d_loss: 0.7850 | g_loss: 2.1602Epoch [    2/   10] | d_loss: 0.5967 | g_loss: 1.7183Epoch [    2/   10] | d_loss: 0.9293 | g_loss: 0.4311Epoch [    2/   10] | d_loss: 0.8048 | g_loss: 2.4040Epoch [    2/   10] | d_loss: 0.9546 | g_loss: 2.9014Epoch [    2/   10] | d_loss: 1.0130 | g_loss: 1.4458Epoch [    2/   10] | d_loss: 0.7787 | g_loss: 1.9187Epoch [    2/   10] | d_loss: 0.8206 | g_loss: 1.3583Epoch [    2/   10] | d_loss: 0.7602 | g_loss: 1.4859Epoch [    2/   10] | d_loss: 1.6796 | g_loss: 2.4626Epoch [    2/   10] | d_loss: 0.6030 | g_loss: 1.8397Epoch [    2/   10] | d_loss: 0.8352 | g_loss: 2.1232Epoch [    2/   10] | d_loss: 0.8560 | g_loss: 1.8031Epoch [    2/   10] | d_loss: 0.5210 | g_loss: 2.2766Epoch [    2/   10] | d_loss: 1.0136 | g_loss: 0.9756Epoch [    2/   10] | d_loss: 0.8236 | g_loss: 1.8405Epoch [    2/   10] | d_loss: 0.6088 | g_loss: 2.2920Epoch [    2/   10] | d_loss: 0.8901 | g_loss: 2.2165Epoch [    2/   10] | d_loss: 0.7736 | g_loss: 1.4659Epoch [    2/   10] | d_loss: 0.6071 | g_loss: 2.0560Epoch [    2/   10] | d_loss: 1.0470 | g_loss: 1.2345Epoch [    2/   10] | d_loss: 0.8429 | g_loss: 1.0077Epoch [    2/   10] | d_loss: 0.5577 | g_loss: 2.2328Epoch [    2/   10] | d_loss: 0.6432 | g_loss: 1.2822Epoch [    2/   10] | d_loss: 0.4028 | g_loss: 2.3859Epoch [    2/   10] | d_loss: 0.8212 | g_loss: 2.2920Epoch [    2/   10] | d_loss: 1.0978 | g_loss: 1.2951Epoch [    2/   10] | d_loss: 0.8459 | g_loss: 3.2531Epoch [    2/   10] | d_loss: 1.3283 | g_loss: 1.5369Epoch [    2/   10] | d_loss: 0.6483 | g_loss: 2.5114Epoch [    2/   10] | d_loss: 0.6488 | g_loss: 2.3417Epoch [    2/   10] | d_loss: 0.8230 | g_loss: 1.4507Epoch [    2/   10] | d_loss: 0.6827 | g_loss: 2.0984Epoch [    2/   10] | d_loss: 0.5202 | g_loss: 3.4786Epoch [    2/   10] | d_loss: 0.5568 | g_loss: 2.5354Epoch [    2/   10] | d_loss: 0.9407 | g_loss: 2.3356Epoch [    2/   10] | d_loss: 0.5611 | g_loss: 2.2820Epoch [    2/   10] | d_loss: 1.2914 | g_loss: 2.4045Epoch [    2/   10] | d_loss: 0.4088 | g_loss: 2.1361Epoch [    2/   10] | d_loss: 0.4458 | g_loss: 1.8634Epoch [    2/   10] | d_loss: 0.6059 | g_loss: 1.1045Epoch [    2/   10] | d_loss: 0.6917 | g_loss: 1.9551Epoch [    2/   10] | d_loss: 0.8114 | g_loss: 2.0670Epoch [    2/   10] | d_loss: 1.0485 | g_loss: 1.6118Epoch [    2/   10] | d_loss: 0.6789 | g_loss: 1.2525Epoch [    2/   10] | d_loss: 0.3771 | g_loss: 1.4703Epoch [    2/   10] | d_loss: 0.6499 | g_loss: 3.0064Epoch [    2/   10] | d_loss: 0.8662 | g_loss: 2.8859Epoch [    2/   10] | d_loss: 0.6973 | g_loss: 2.0897Epoch [    2/   10] | d_loss: 1.0380 | g_loss: 0.4704Epoch [    2/   10] | d_loss: 0.7165 | g_loss: 1.2300Epoch [    2/   10] | d_loss: 1.0388 | g_loss: 1.9755Epoch [    2/   10] | d_loss: 1.1867 | g_loss: 2.1088Epoch [    3/   10] | d_loss: 0.8097 | g_loss: 3.0393Epoch [    3/   10] | d_loss: 0.5019 | g_loss: 2.5709Epoch [    3/   10] | d_loss: 0.7984 | g_loss: 1.5304Epoch [    3/   10] | d_loss: 0.7146 | g_loss: 2.8520Epoch [    3/   10] | d_loss: 0.6904 | g_loss: 2.6993Epoch [    3/   10] | d_loss: 0.5089 | g_loss: 2.0663Epoch [    3/   10] | d_loss: 0.9721 | g_loss: 1.3061Epoch [    3/   10] | d_loss: 0.7447 | g_loss: 3.1119Epoch [    3/   10] | d_loss: 0.6252 | g_loss: 2.6550Epoch [    3/   10] | d_loss: 0.3589 | g_loss: 2.8221Epoch [    3/   10] | d_loss: 0.4794 | g_loss: 1.8572Epoch [    3/   10] | d_loss: 0.4721 | g_loss: 2.7487Epoch [    3/   10] | d_loss: 1.2881 | g_loss: 0.6078Epoch [    3/   10] | d_loss: 0.8306 | g_loss: 1.6300Epoch [    3/   10] | d_loss: 0.4302 | g_loss: 2.5741Epoch [    3/   10] | d_loss: 0.4856 | g_loss: 2.8508Epoch [    3/   10] | d_loss: 1.0638 | g_loss: 1.2981Epoch [    3/   10] | d_loss: 1.2009 | g_loss: 2.4391Epoch [    3/   10] | d_loss: 0.5120 | g_loss: 0.8365Epoch [    3/   10] | d_loss: 1.0198 | g_loss: 1.5580Epoch [    3/   10] | d_loss: 0.8820 | g_loss: 2.1524Epoch [    3/   10] | d_loss: 0.8794 | g_loss: 1.9807Epoch [    3/   10] | d_loss: 1.2404 | g_loss: 1.5583Epoch [    3/   10] | d_loss: 0.5065 | g_loss: 1.5696Epoch [    3/   10] | d_loss: 0.7311 | g_loss: 2.3812Epoch [    3/   10] | d_loss: 0.7586 | g_loss: 1.6321Epoch [    3/   10] | d_loss: 1.1888 | g_loss: 3.0969Epoch [    3/   10] | d_loss: 0.6019 | g_loss: 1.3327Epoch [    3/   10] | d_loss: 0.9333 | g_loss: 1.7940Epoch [    3/   10] | d_loss: 0.9408 | g_loss: 3.4469Epoch [    3/   10] | d_loss: 0.4765 | g_loss: 1.4752Epoch [    3/   10] | d_loss: 0.5436 | g_loss: 2.0790Epoch [    3/   10] | d_loss: 0.8359 | g_loss: 1.3764Epoch [    3/   10] | d_loss: 0.8532 | g_loss: 3.2155Epoch [    3/   10] | d_loss: 0.8930 | g_loss: 1.4274Epoch [    3/   10] | d_loss: 0.7218 | g_loss: 2.5226Epoch [    3/   10] | d_loss: 0.5686 | g_loss: 1.5422Epoch [    3/   10] | d_loss: 0.9219 | g_loss: 1.8003Epoch [    3/   10] | d_loss: 1.2073 | g_loss: 0.9530Epoch [    3/   10] | d_loss: 0.8309 | g_loss: 1.0924Epoch [    3/   10] | d_loss: 0.8689 | g_loss: 1.7498Epoch [    3/   10] | d_loss: 1.1529 | g_loss: 0.9297Epoch [    3/   10] | d_loss: 0.8227 | g_loss: 1.6970Epoch [    3/   10] | d_loss: 0.5959 | g_loss: 1.0709Epoch [    3/   10] | d_loss: 0.8132 | g_loss: 2.8963Epoch [    3/   10] | d_loss: 1.2516 | g_loss: 1.1911Epoch [    3/   10] | d_loss: 0.9279 | g_loss: 1.3907Epoch [    3/   10] | d_loss: 1.0880 | g_loss: 2.5361Epoch [    3/   10] | d_loss: 0.8747 | g_loss: 2.5947Epoch [    3/   10] | d_loss: 1.0380 | g_loss: 1.3927Epoch [    3/   10] | d_loss: 0.8980 | g_loss: 1.9233Epoch [    3/   10] | d_loss: 0.9106 | g_loss: 2.5329Epoch [    3/   10] | d_loss: 0.5308 | g_loss: 2.5881Epoch [    3/   10] | d_loss: 1.0285 | g_loss: 0.8125Epoch [    3/   10] | d_loss: 0.7343 | g_loss: 2.8655Epoch [    3/   10] | d_loss: 0.7057 | g_loss: 1.7360Epoch [    3/   10] | d_loss: 0.5431 | g_loss: 1.2880Epoch [    4/   10] | d_loss: 0.8401 | g_loss: 1.4240Epoch [    4/   10] | d_loss: 0.8616 | g_loss: 2.2292Epoch [    4/   10] | d_loss: 0.8513 | g_loss: 0.8143Epoch [    4/   10] | d_loss: 0.9198 | g_loss: 2.0126Epoch [    4/   10] | d_loss: 0.7864 | g_loss: 0.7880Epoch [    4/   10] | d_loss: 0.6304 | g_loss: 2.2339Epoch [    4/   10] | d_loss: 0.9616 | g_loss: 1.9981Epoch [    4/   10] | d_loss: 0.9861 | g_loss: 3.1358Epoch [    4/   10] | d_loss: 0.6280 | g_loss: 2.0134Epoch [    4/   10] | d_loss: 0.4167 | g_loss: 1.8948Epoch [    4/   10] | d_loss: 0.9732 | g_loss: 1.7885Epoch [    4/   10] | d_loss: 1.3507 | g_loss: 3.3076Epoch [    4/   10] | d_loss: 0.8582 | g_loss: 2.4259Epoch [    4/   10] | d_loss: 0.3222 | g_loss: 1.3931Epoch [    4/   10] | d_loss: 0.3784 | g_loss: 2.0218Epoch [    4/   10] | d_loss: 0.5218 | g_loss: 3.1135Epoch [    4/   10] | d_loss: 1.2346 | g_loss: 1.9277Epoch [    4/   10] | d_loss: 0.8060 | g_loss: 2.9532Epoch [    4/   10] | d_loss: 0.5906 | g_loss: 3.2836Epoch [    4/   10] | d_loss: 0.3541 | g_loss: 2.9068Epoch [    4/   10] | d_loss: 0.9875 | g_loss: 0.9372Epoch [    4/   10] | d_loss: 0.6712 | g_loss: 2.4969Epoch [    4/   10] | d_loss: 0.6518 | g_loss: 1.6130Epoch [    4/   10] | d_loss: 0.5608 | g_loss: 2.5040Epoch [    4/   10] | d_loss: 0.8226 | g_loss: 1.6709Epoch [    4/   10] | d_loss: 0.4274 | g_loss: 1.9891Epoch [    4/   10] | d_loss: 0.9803 | g_loss: 1.1438Epoch [    4/   10] | d_loss: 0.6038 | g_loss: 2.0693Epoch [    4/   10] | d_loss: 0.7240 | g_loss: 1.6234Epoch [    4/   10] | d_loss: 1.1355 | g_loss: 2.6563Epoch [    4/   10] | d_loss: 0.6979 | g_loss: 1.5845Epoch [    4/   10] | d_loss: 0.6271 | g_loss: 2.4673Epoch [    4/   10] | d_loss: 0.8741 | g_loss: 3.4031Epoch [    4/   10] | d_loss: 0.8035 | g_loss: 1.0717Epoch [    4/   10] | d_loss: 0.5466 | g_loss: 3.6925Epoch [    4/   10] | d_loss: 0.7398 | g_loss: 1.3586Epoch [    4/   10] | d_loss: 0.9093 | g_loss: 1.7668Epoch [    4/   10] | d_loss: 0.9914 | g_loss: 2.2887Epoch [    4/   10] | d_loss: 0.6766 | g_loss: 3.2470Epoch [    4/   10] | d_loss: 0.9100 | g_loss: 2.9327Epoch [    4/   10] | d_loss: 0.4537 | g_loss: 1.7861Epoch [    4/   10] | d_loss: 0.7286 | g_loss: 2.2573Epoch [    4/   10] | d_loss: 0.7621 | g_loss: 1.6164Epoch [    4/   10] | d_loss: 0.6815 | g_loss: 2.1096Epoch [    4/   10] | d_loss: 0.5489 | g_loss: 1.9791Epoch [    4/   10] | d_loss: 0.4194 | g_loss: 2.2744Epoch [    4/   10] | d_loss: 0.5542 | g_loss: 1.8974Epoch [    4/   10] | d_loss: 0.5394 | g_loss: 2.2294Epoch [    4/   10] | d_loss: 0.5985 | g_loss: 2.3418Epoch [    4/   10] | d_loss: 0.7873 | g_loss: 2.4874Epoch [    4/   10] | d_loss: 0.5031 | g_loss: 2.3360Epoch [    4/   10] | d_loss: 0.6192 | g_loss: 1.5156Epoch [    4/   10] | d_loss: 1.2282 | g_loss: 0.9613Epoch [    4/   10] | d_loss: 0.7149 | g_loss: 3.4017Epoch [    4/   10] | d_loss: 0.6998 | g_loss: 1.7441Epoch [    4/   10] | d_loss: 0.2832 | g_loss: 2.7605Epoch [    4/   10] | d_loss: 0.4165 | g_loss: 2.6288Epoch [    5/   10] | d_loss: 1.1319 | g_loss: 2.9711Epoch [    5/   10] | d_loss: 0.8193 | g_loss: 1.6927Epoch [    5/   10] | d_loss: 0.6316 | g_loss: 2.9231Epoch [    5/   10] | d_loss: 0.8828 | g_loss: 2.3579Epoch [    5/   10] | d_loss: 0.7815 | g_loss: 0.9519Epoch [    5/   10] | d_loss: 0.6774 | g_loss: 1.9494Epoch [    5/   10] | d_loss: 0.5645 | g_loss: 2.0426Epoch [    5/   10] | d_loss: 0.4826 | g_loss: 1.4202Epoch [    5/   10] | d_loss: 0.9609 | g_loss: 1.2853Epoch [    5/   10] | d_loss: 0.6564 | g_loss: 0.6472Epoch [    5/   10] | d_loss: 0.7649 | g_loss: 2.5583Epoch [    5/   10] | d_loss: 0.8553 | g_loss: 2.9331Epoch [    5/   10] | d_loss: 0.9975 | g_loss: 3.8976Epoch [    5/   10] | d_loss: 0.9121 | g_loss: 1.9868Epoch [    5/   10] | d_loss: 1.0603 | g_loss: 0.9347Epoch [    5/   10] | d_loss: 0.5517 | g_loss: 2.0222Epoch [    5/   10] | d_loss: 0.8386 | g_loss: 1.4041Epoch [    5/   10] | d_loss: 0.8956 | g_loss: 2.2966Epoch [    5/   10] | d_loss: 0.5073 | g_loss: 2.2514Epoch [    5/   10] | d_loss: 0.6612 | g_loss: 1.8508Epoch [    5/   10] | d_loss: 0.4714 | g_loss: 1.4664Epoch [    5/   10] | d_loss: 0.4838 | g_loss: 1.4921Epoch [    5/   10] | d_loss: 0.5782 | g_loss: 2.1109Epoch [    5/   10] | d_loss: 0.7387 | g_loss: 2.9436Epoch [    5/   10] | d_loss: 0.6819 | g_loss: 0.8043Epoch [    5/   10] | d_loss: 0.4835 | g_loss: 3.2234Epoch [    5/   10] | d_loss: 0.3402 | g_loss: 2.0674Epoch [    5/   10] | d_loss: 0.4534 | g_loss: 3.4563Epoch [    5/   10] | d_loss: 0.5540 | g_loss: 1.1556Epoch [    5/   10] | d_loss: 0.7765 | g_loss: 1.2208Epoch [    5/   10] | d_loss: 1.0301 | g_loss: 1.1580Epoch [    5/   10] | d_loss: 0.6595 | g_loss: 1.4798Epoch [    5/   10] | d_loss: 0.7811 | g_loss: 2.1072Epoch [    5/   10] | d_loss: 0.8951 | g_loss: 2.2031Epoch [    5/   10] | d_loss: 0.6046 | g_loss: 3.0415Epoch [    5/   10] | d_loss: 0.3505 | g_loss: 3.3982Epoch [    5/   10] | d_loss: 0.8029 | g_loss: 2.2610Epoch [    5/   10] | d_loss: 0.6194 | g_loss: 2.2170Epoch [    5/   10] | d_loss: 0.8113 | g_loss: 2.6177Epoch [    5/   10] | d_loss: 0.3110 | g_loss: 1.6645Epoch [    5/   10] | d_loss: 0.6087 | g_loss: 1.7842Epoch [    5/   10] | d_loss: 0.5965 | g_loss: 1.0506Epoch [    5/   10] | d_loss: 0.6210 | g_loss: 1.7927Epoch [    5/   10] | d_loss: 0.7243 | g_loss: 3.9066Epoch [    5/   10] | d_loss: 0.6715 | g_loss: 2.7507Epoch [    5/   10] | d_loss: 0.5632 | g_loss: 2.4817Epoch [    5/   10] | d_loss: 0.6050 | g_loss: 1.5578Epoch [    5/   10] | d_loss: 0.3460 | g_loss: 2.9607Epoch [    5/   10] | d_loss: 0.6075 | g_loss: 2.7551Epoch [    5/   10] | d_loss: 0.8077 | g_loss: 1.5106Epoch [    5/   10] | d_loss: 0.4638 | g_loss: 1.6624Epoch [    5/   10] | d_loss: 1.2398 | g_loss: 0.6678Epoch [    5/   10] | d_loss: 1.2159 | g_loss: 3.4430Epoch [    5/   10] | d_loss: 0.6071 | g_loss: 2.7914Epoch [    5/   10] | d_loss: 0.4359 | g_loss: 2.0077Epoch [    5/   10] | d_loss: 0.7977 | g_loss: 2.0437Epoch [    5/   10] | d_loss: 0.4534 | g_loss: 3.0467Epoch [    6/   10] | d_loss: 0.6131 | g_loss: 1.6626Epoch [    6/   10] | d_loss: 0.4299 | g_loss: 2.2227Epoch [    6/   10] | d_loss: 0.3006 | g_loss: 0.7026Epoch [    6/   10] | d_loss: 0.1854 | g_loss: 2.6362Epoch [    6/   10] | d_loss: 0.5243 | g_loss: 2.2669Epoch [    6/   10] | d_loss: 0.6989 | g_loss: 1.7068Epoch [    6/   10] | d_loss: 0.4597 | g_loss: 1.4009Epoch [    6/   10] | d_loss: 0.9632 | g_loss: 1.0400Epoch [    6/   10] | d_loss: 1.3017 | g_loss: 0.9841Epoch [    6/   10] | d_loss: 0.4176 | g_loss: 2.6286Epoch [    6/   10] | d_loss: 0.4484 | g_loss: 1.8289Epoch [    6/   10] | d_loss: 0.4174 | g_loss: 1.4095Epoch [    6/   10] | d_loss: 0.7201 | g_loss: 1.3313Epoch [    6/   10] | d_loss: 0.2836 | g_loss: 0.5963Epoch [    6/   10] | d_loss: 0.3262 | g_loss: 2.7093Epoch [    6/   10] | d_loss: 0.5079 | g_loss: 1.6562Epoch [    6/   10] | d_loss: 0.5641 | g_loss: 2.6865Epoch [    6/   10] | d_loss: 0.4018 | g_loss: 3.5414Epoch [    6/   10] | d_loss: 0.7538 | g_loss: 1.3195Epoch [    6/   10] | d_loss: 0.6896 | g_loss: 2.3884Epoch [    6/   10] | d_loss: 0.5189 | g_loss: 2.3032Epoch [    6/   10] | d_loss: 0.9189 | g_loss: 1.8686Epoch [    6/   10] | d_loss: 0.4986 | g_loss: 3.0515Epoch [    6/   10] | d_loss: 0.3765 | g_loss: 2.6408Epoch [    6/   10] | d_loss: 1.4173 | g_loss: 0.8775Epoch [    6/   10] | d_loss: 0.5607 | g_loss: 2.1340Epoch [    6/   10] | d_loss: 0.8014 | g_loss: 2.4330Epoch [    6/   10] | d_loss: 0.5984 | g_loss: 0.9653Epoch [    6/   10] | d_loss: 0.6888 | g_loss: 1.7870Epoch [    6/   10] | d_loss: 0.2652 | g_loss: 3.0309Epoch [    6/   10] | d_loss: 0.4509 | g_loss: 1.8343Epoch [    6/   10] | d_loss: 0.8102 | g_loss: 2.9208Epoch [    6/   10] | d_loss: 0.5327 | g_loss: 2.4754Epoch [    6/   10] | d_loss: 0.4832 | g_loss: 2.7773Epoch [    6/   10] | d_loss: 0.5500 | g_loss: 3.3042Epoch [    6/   10] | d_loss: 0.5702 | g_loss: 3.2377Epoch [    6/   10] | d_loss: 0.7310 | g_loss: 1.1722Epoch [    6/   10] | d_loss: 0.5023 | g_loss: 2.2215Epoch [    6/   10] | d_loss: 0.8471 | g_loss: 3.6402Epoch [    6/   10] | d_loss: 0.5849 | g_loss: 2.6509Epoch [    6/   10] | d_loss: 0.3653 | g_loss: 2.6616Epoch [    6/   10] | d_loss: 0.3248 | g_loss: 1.9398Epoch [    6/   10] | d_loss: 0.9223 | g_loss: 4.0499Epoch [    6/   10] | d_loss: 0.6721 | g_loss: 3.2913Epoch [    6/   10] | d_loss: 0.7161 | g_loss: 1.5024Epoch [    6/   10] | d_loss: 0.4479 | g_loss: 2.5791Epoch [    6/   10] | d_loss: 0.5212 | g_loss: 1.7683Epoch [    6/   10] | d_loss: 0.5045 | g_loss: 1.3497Epoch [    6/   10] | d_loss: 0.6152 | g_loss: 1.1267Epoch [    6/   10] | d_loss: 0.5551 | g_loss: 1.5158Epoch [    6/   10] | d_loss: 0.4587 | g_loss: 1.8742Epoch [    6/   10] | d_loss: 0.6807 | g_loss: 2.9760Epoch [    6/   10] | d_loss: 0.5110 | g_loss: 2.5312Epoch [    6/   10] | d_loss: 0.8837 | g_loss: 3.1058Epoch [    6/   10] | d_loss: 0.3380 | g_loss: 4.4900Epoch [    6/   10] | d_loss: 0.6072 | g_loss: 3.4840Epoch [    6/   10] | d_loss: 0.4818 | g_loss: 0.8018Epoch [    7/   10] | d_loss: 0.3787 | g_loss: 3.2713Epoch [    7/   10] | d_loss: 0.4111 | g_loss: 2.1725Epoch [    7/   10] | d_loss: 0.6174 | g_loss: 2.1986Epoch [    7/   10] | d_loss: 0.4450 | g_loss: 2.6382Epoch [    7/   10] | d_loss: 0.3990 | g_loss: 3.3629Epoch [    7/   10] | d_loss: 0.3088 | g_loss: 2.7460Epoch [    7/   10] | d_loss: 0.4958 | g_loss: 1.6353Epoch [    7/   10] | d_loss: 0.6304 | g_loss: 2.3534Epoch [    7/   10] | d_loss: 0.2532 | g_loss: 4.5817Epoch [    7/   10] | d_loss: 0.6479 | g_loss: 3.0984Epoch [    7/   10] | d_loss: 0.5513 | g_loss: 3.4089Epoch [    7/   10] | d_loss: 0.3134 | g_loss: 2.6342Epoch [    7/   10] | d_loss: 0.3460 | g_loss: 1.7633Epoch [    7/   10] | d_loss: 0.6270 | g_loss: 1.3131Epoch [    7/   10] | d_loss: 0.4562 | g_loss: 1.2855Epoch [    7/   10] | d_loss: 0.6967 | g_loss: 3.7443Epoch [    7/   10] | d_loss: 0.3883 | g_loss: 1.1247Epoch [    7/   10] | d_loss: 1.1162 | g_loss: 1.4496Epoch [    7/   10] | d_loss: 0.6862 | g_loss: 1.9642Epoch [    7/   10] | d_loss: 0.7530 | g_loss: 2.4123Epoch [    7/   10] | d_loss: 0.7394 | g_loss: 0.2484Epoch [    7/   10] | d_loss: 0.6291 | g_loss: 1.7968Epoch [    7/   10] | d_loss: 1.1447 | g_loss: 0.4640Epoch [    7/   10] | d_loss: 0.4903 | g_loss: 1.3460Epoch [    7/   10] | d_loss: 0.5742 | g_loss: 2.7956Epoch [    7/   10] | d_loss: 0.6709 | g_loss: 2.9093Epoch [    7/   10] | d_loss: 0.1588 | g_loss: 3.7518Epoch [    7/   10] | d_loss: 0.3172 | g_loss: 2.7774Epoch [    7/   10] | d_loss: 0.5264 | g_loss: 2.1130Epoch [    7/   10] | d_loss: 0.7936 | g_loss: 2.6904Epoch [    7/   10] | d_loss: 0.2563 | g_loss: 1.9693Epoch [    7/   10] | d_loss: 0.6558 | g_loss: 1.7162Epoch [    7/   10] | d_loss: 0.3564 | g_loss: 1.8198Epoch [    7/   10] | d_loss: 0.4630 | g_loss: 2.8980Epoch [    7/   10] | d_loss: 0.8806 | g_loss: 0.8306Epoch [    7/   10] | d_loss: 0.3239 | g_loss: 1.6085Epoch [    7/   10] | d_loss: 0.4486 | g_loss: 2.5150Epoch [    7/   10] | d_loss: 0.8813 | g_loss: 1.4289Epoch [    7/   10] | d_loss: 0.3901 | g_loss: 3.3365Epoch [    7/   10] | d_loss: 0.1809 | g_loss: 2.6817Epoch [    7/   10] | d_loss: 0.3283 | g_loss: 1.8781Epoch [    7/   10] | d_loss: 0.7115 | g_loss: 2.0561Epoch [    7/   10] | d_loss: 0.4457 | g_loss: 2.0092Epoch [    7/   10] | d_loss: 0.6913 | g_loss: 2.0837Epoch [    7/   10] | d_loss: 0.3181 | g_loss: 1.7014Epoch [    7/   10] | d_loss: 0.4084 | g_loss: 1.0058Epoch [    7/   10] | d_loss: 0.2859 | g_loss: 2.8504Epoch [    7/   10] | d_loss: 0.2003 | g_loss: 2.5487Epoch [    7/   10] | d_loss: 1.1387 | g_loss: 1.0666Epoch [    7/   10] | d_loss: 0.6104 | g_loss: 2.0029Epoch [    7/   10] | d_loss: 0.4825 | g_loss: 2.1227Epoch [    7/   10] | d_loss: 0.4514 | g_loss: 2.6752Epoch [    7/   10] | d_loss: 1.2751 | g_loss: 2.3475Epoch [    7/   10] | d_loss: 0.8994 | g_loss: 3.8799Epoch [    7/   10] | d_loss: 0.3868 | g_loss: 2.2264Epoch [    7/   10] | d_loss: 0.3284 | g_loss: 1.9207Epoch [    7/   10] | d_loss: 0.3513 | g_loss: 2.3701Epoch [    8/   10] | d_loss: 0.3545 | g_loss: 1.5507Epoch [    8/   10] | d_loss: 0.2104 | g_loss: 3.5212Epoch [    8/   10] | d_loss: 0.5248 | g_loss: 2.2367Epoch [    8/   10] | d_loss: 0.4900 | g_loss: 4.0762Epoch [    8/   10] | d_loss: 0.4916 | g_loss: 3.2304Epoch [    8/   10] | d_loss: 0.2471 | g_loss: 1.7792Epoch [    8/   10] | d_loss: 0.6362 | g_loss: 3.2936Epoch [    8/   10] | d_loss: 0.5648 | g_loss: 2.2958Epoch [    8/   10] | d_loss: 0.3065 | g_loss: 3.9343Epoch [    8/   10] | d_loss: 0.5667 | g_loss: 2.6154Epoch [    8/   10] | d_loss: 0.2314 | g_loss: 1.7458Epoch [    8/   10] | d_loss: 0.2667 | g_loss: 2.6632Epoch [    8/   10] | d_loss: 1.0024 | g_loss: 3.9373Epoch [    8/   10] | d_loss: 0.2404 | g_loss: 1.8878Epoch [    8/   10] | d_loss: 0.6905 | g_loss: 2.8458Epoch [    8/   10] | d_loss: 0.2784 | g_loss: 2.5856Epoch [    8/   10] | d_loss: 0.5212 | g_loss: 2.4392Epoch [    8/   10] | d_loss: 0.4670 | g_loss: 2.4646Epoch [    8/   10] | d_loss: 1.0045 | g_loss: 4.2842Epoch [    8/   10] | d_loss: 0.2298 | g_loss: 4.3155Epoch [    8/   10] | d_loss: 0.2212 | g_loss: 2.6108Epoch [    8/   10] | d_loss: 0.3668 | g_loss: 2.5877Epoch [    8/   10] | d_loss: 0.7712 | g_loss: 3.1950Epoch [    8/   10] | d_loss: 0.3119 | g_loss: 1.8850Epoch [    8/   10] | d_loss: 0.2629 | g_loss: 2.6502Epoch [    8/   10] | d_loss: 0.2770 | g_loss: 1.8937Epoch [    8/   10] | d_loss: 0.4351 | g_loss: 2.0899Epoch [    8/   10] | d_loss: 0.2559 | g_loss: 2.8211Epoch [    8/   10] | d_loss: 0.9524 | g_loss: 1.7890Epoch [    8/   10] | d_loss: 0.3591 | g_loss: 3.1456Epoch [    8/   10] | d_loss: 0.5351 | g_loss: 1.7695Epoch [    8/   10] | d_loss: 0.9661 | g_loss: 4.8195Epoch [    8/   10] | d_loss: 0.4399 | g_loss: 3.0966Epoch [    8/   10] | d_loss: 0.7260 | g_loss: 2.5558Epoch [    8/   10] | d_loss: 0.3313 | g_loss: 2.9619Epoch [    8/   10] | d_loss: 0.4049 | g_loss: 2.5061Epoch [    8/   10] | d_loss: 0.7257 | g_loss: 3.5069Epoch [    8/   10] | d_loss: 0.5488 | g_loss: 2.5658Epoch [    8/   10] | d_loss: 0.5171 | g_loss: 2.5113Epoch [    8/   10] | d_loss: 0.2932 | g_loss: 2.5040Epoch [    8/   10] | d_loss: 0.4522 | g_loss: 2.6739Epoch [    8/   10] | d_loss: 0.5376 | g_loss: 2.9858Epoch [    8/   10] | d_loss: 0.5467 | g_loss: 1.6005Epoch [    8/   10] | d_loss: 0.3488 | g_loss: 2.6331Epoch [    8/   10] | d_loss: 0.5938 | g_loss: 0.8402Epoch [    8/   10] | d_loss: 0.1440 | g_loss: 1.7671Epoch [    8/   10] | d_loss: 0.4936 | g_loss: 2.6844Epoch [    8/   10] | d_loss: 0.7796 | g_loss: 1.9666Epoch [    8/   10] | d_loss: 0.3242 | g_loss: 3.2707Epoch [    8/   10] | d_loss: 0.3156 | g_loss: 2.2630Epoch [    8/   10] | d_loss: 1.4755 | g_loss: 1.0196Epoch [    8/   10] | d_loss: 0.3777 | g_loss: 2.7318Epoch [    8/   10] | d_loss: 0.3318 | g_loss: 2.6214Epoch [    8/   10] | d_loss: 0.6964 | g_loss: 1.4799Epoch [    8/   10] | d_loss: 0.6144 | g_loss: 3.8209Epoch [    8/   10] | d_loss: 0.5881 | g_loss: 2.3210Epoch [    8/   10] | d_loss: 0.3095 | g_loss: 3.3764Epoch [    9/   10] | d_loss: 0.7958 | g_loss: 2.0137Epoch [    9/   10] | d_loss: 1.1180 | g_loss: 4.9843Epoch [    9/   10] | d_loss: 0.3233 | g_loss: 2.2818Epoch [    9/   10] | d_loss: 0.4081 | g_loss: 1.6872Epoch [    9/   10] | d_loss: 0.1328 | g_loss: 1.7398Epoch [    9/   10] | d_loss: 0.3839 | g_loss: 3.1949Epoch [    9/   10] | d_loss: 0.3956 | g_loss: 3.4710Epoch [    9/   10] | d_loss: 0.4662 | g_loss: 2.0692Epoch [    9/   10] | d_loss: 0.1396 | g_loss: 2.1416Epoch [    9/   10] | d_loss: 0.7033 | g_loss: 4.5267Epoch [    9/   10] | d_loss: 0.3810 | g_loss: 2.4842Epoch [    9/   10] | d_loss: 0.8668 | g_loss: 3.4257Epoch [    9/   10] | d_loss: 0.8092 | g_loss: 2.0416Epoch [    9/   10] | d_loss: 0.1970 | g_loss: 2.4944Epoch [    9/   10] | d_loss: 0.3658 | g_loss: 1.3878Epoch [    9/   10] | d_loss: 0.5938 | g_loss: 4.3212Epoch [    9/   10] | d_loss: 0.6824 | g_loss: 3.0047Epoch [    9/   10] | d_loss: 0.4710 | g_loss: 2.3355Epoch [    9/   10] | d_loss: 0.4070 | g_loss: 2.6194Epoch [    9/   10] | d_loss: 0.6451 | g_loss: 3.7926Epoch [    9/   10] | d_loss: 0.4403 | g_loss: 2.0283Epoch [    9/   10] | d_loss: 0.3301 | g_loss: 3.2711Epoch [    9/   10] | d_loss: 0.5004 | g_loss: 3.1164Epoch [    9/   10] | d_loss: 0.2670 | g_loss: 4.5862Epoch [    9/   10] | d_loss: 0.3445 | g_loss: 2.5394Epoch [    9/   10] | d_loss: 0.4404 | g_loss: 2.4993Epoch [    9/   10] | d_loss: 0.2074 | g_loss: 3.8929Epoch [    9/   10] | d_loss: 0.3747 | g_loss: 2.8142Epoch [    9/   10] | d_loss: 0.3066 | g_loss: 2.4967Epoch [    9/   10] | d_loss: 0.3258 | g_loss: 2.8470Epoch [    9/   10] | d_loss: 0.2147 | g_loss: 3.2469Epoch [    9/   10] | d_loss: 0.1356 | g_loss: 4.4160Epoch [    9/   10] | d_loss: 0.1353 | g_loss: 3.1965Epoch [    9/   10] | d_loss: 0.6778 | g_loss: 4.4086Epoch [    9/   10] | d_loss: 0.3097 | g_loss: 1.4800Epoch [    9/   10] | d_loss: 0.3600 | g_loss: 1.2397Epoch [    9/   10] | d_loss: 0.5439 | g_loss: 4.4019Epoch [    9/   10] | d_loss: 0.2376 | g_loss: 2.5645Epoch [    9/   10] | d_loss: 1.0602 | g_loss: 4.8398Epoch [    9/   10] | d_loss: 0.1502 | g_loss: 3.6295Epoch [    9/   10] | d_loss: 0.3918 | g_loss: 4.1729Epoch [    9/   10] | d_loss: 0.3563 | g_loss: 2.6311Epoch [    9/   10] | d_loss: 0.3923 | g_loss: 2.1126Epoch [    9/   10] | d_loss: 0.2049 | g_loss: 3.7298Epoch [    9/   10] | d_loss: 1.0741 | g_loss: 2.4022Epoch [    9/   10] | d_loss: 0.1391 | g_loss: 2.6011Epoch [    9/   10] | d_loss: 1.1179 | g_loss: 1.7936Epoch [    9/   10] | d_loss: 0.3378 | g_loss: 2.3964Epoch [    9/   10] | d_loss: 0.2668 | g_loss: 3.0656Epoch [    9/   10] | d_loss: 0.0952 | g_loss: 3.2094Epoch [    9/   10] | d_loss: 0.2538 | g_loss: 3.7583Epoch [    9/   10] | d_loss: 0.3544 | g_loss: 1.5199Epoch [    9/   10] | d_loss: 0.5133 | g_loss: 3.7610Epoch [    9/   10] | d_loss: 0.5193 | g_loss: 4.1392Epoch [    9/   10] | d_loss: 0.5395 | g_loss: 5.6215Epoch [    9/   10] | d_loss: 0.3722 | g_loss: 2.2540Epoch [    9/   10] | d_loss: 0.3496 | g_loss: 3.0168Epoch [   10/   10] | d_loss: 0.1506 | g_loss: 3.1284Epoch [   10/   10] | d_loss: 0.5005 | g_loss: 2.7923Epoch [   10/   10] | d_loss: 0.2329 | g_loss: 4.3172Epoch [   10/   10] | d_loss: 0.2920 | g_loss: 2.8481Epoch [   10/   10] | d_loss: 0.4869 | g_loss: 2.7364Epoch [   10/   10] | d_loss: 0.3645 | g_loss: 1.3832Epoch [   10/   10] | d_loss: 0.2958 | g_loss: 4.2701Epoch [   10/   10] | d_loss: 0.2752 | g_loss: 4.2040Epoch [   10/   10] | d_loss: 0.3000 | g_loss: 2.0245Epoch [   10/   10] | d_loss: 0.1832 | g_loss: 3.6009Epoch [   10/   10] | d_loss: 0.6361 | g_loss: 3.0735Epoch [   10/   10] | d_loss: 0.4378 | g_loss: 3.5753Epoch [   10/   10] | d_loss: 0.7674 | g_loss: 1.6762Epoch [   10/   10] | d_loss: 0.0906 | g_loss: 2.8359Epoch [   10/   10] | d_loss: 0.2643 | g_loss: 3.0888Epoch [   10/   10] | d_loss: 0.2865 | g_loss: 5.3736Epoch [   10/   10] | d_loss: 0.1423 | g_loss: 2.6674Epoch [   10/   10] | d_loss: 0.3027 | g_loss: 1.9178Epoch [   10/   10] | d_loss: 0.0606 | g_loss: 2.7456Epoch [   10/   10] | d_loss: 0.2992 | g_loss: 2.0044Epoch [   10/   10] | d_loss: 0.0929 | g_loss: 2.7819Epoch [   10/   10] | d_loss: 0.5342 | g_loss: 3.5143Epoch [   10/   10] | d_loss: 0.5569 | g_loss: 2.7988Epoch [   10/   10] | d_loss: 0.5476 | g_loss: 4.1120Epoch [   10/   10] | d_loss: 0.4387 | g_loss: 3.9358Epoch [   10/   10] | d_loss: 0.2141 | g_loss: 3.2421Epoch [   10/   10] | d_loss: 0.3969 | g_loss: 2.3368Epoch [   10/   10] | d_loss: 0.6126 | g_loss: 2.3273Epoch [   10/   10] | d_loss: 0.3728 | g_loss: 4.0061Epoch [   10/   10] | d_loss: 0.1637 | g_loss: 2.8562Epoch [   10/   10] | d_loss: 0.3026 | g_loss: 4.2510Epoch [   10/   10] | d_loss: 0.9926 | g_loss: 1.6901Epoch [   10/   10] | d_loss: 0.4880 | g_loss: 3.0351Epoch [   10/   10] | d_loss: 0.5228 | g_loss: 1.2235Epoch [   10/   10] | d_loss: 0.8631 | g_loss: 5.2191Epoch [   10/   10] | d_loss: 0.2152 | g_loss: 5.0519Epoch [   10/   10] | d_loss: 0.3977 | g_loss: 3.8114Epoch [   10/   10] | d_loss: 2.2134 | g_loss: 4.7074Epoch [   10/   10] | d_loss: 0.3417 | g_loss: 2.9938Epoch [   10/   10] | d_loss: 0.9200 | g_loss: 2.3087Epoch [   10/   10] | d_loss: 0.4359 | g_loss: 1.9508Epoch [   10/   10] | d_loss: 0.2884 | g_loss: 3.0795Epoch [   10/   10] | d_loss: 0.1838 | g_loss: 3.2346Epoch [   10/   10] | d_loss: 0.1815 | g_loss: 2.5961Epoch [   10/   10] | d_loss: 0.3024 | g_loss: 3.3063Epoch [   10/   10] | d_loss: 0.6810 | g_loss: 2.7535Epoch [   10/   10] | d_loss: 0.6004 | g_loss: 3.7914Epoch [   10/   10] | d_loss: 0.3055 | g_loss: 4.7553Epoch [   10/   10] | d_loss: 0.3839 | g_loss: 2.9566Epoch [   10/   10] | d_loss: 0.5230 | g_loss: 2.2097Epoch [   10/   10] | d_loss: 0.1586 | g_loss: 1.2258Epoch [   10/   10] | d_loss: 0.5196 | g_loss: 3.2109Epoch [   10/   10] | d_loss: 0.1643 | g_loss: 3.9196Epoch [   10/   10] | d_loss: 0.1626 | g_loss: 2.8338Epoch [   10/   10] | d_loss: 0.1378 | g_loss: 2.6072Epoch [   10/   10] | d_loss: 0.3793 | g_loss: 4.5483Epoch [   10/   10] | d_loss: 0.2544 | g_loss: 3.4693Training lossPlot the training losses for the generator and discriminator, recorded after each epoch.fig, ax = plt.subplots()losses = np.array(losses)plt.plot(losses.T[0], label='Discriminator', alpha=0.5)plt.plot(losses.T[1], label='Generator', alpha=0.5)plt.title(\"Training Losses\")plt.legend()&lt;matplotlib.legend.Legend at 0x7f13f004a128&gt;Generator samples from trainingView samples of images from the generator, and answer a question about the strengths and weaknesses of your trained models.# helper function for viewing a list of passed in sample imagesdef view_samples(epoch, samples):    fig, axes = plt.subplots(figsize=(16,4), nrows=2, ncols=8, sharey=True, sharex=True)    for ax, img in zip(axes.flatten(), samples[epoch]):        img = img.detach().cpu().numpy()        img = np.transpose(img, (1, 2, 0))        img = ((img + 1)*255 / (2)).astype(np.uint8)        ax.xaxis.set_visible(False)        ax.yaxis.set_visible(False)        im = ax.imshow(img.reshape((32,32,3)))# Load samples from generator, taken while trainingwith open('train_samples.pkl', 'rb') as f:    samples = pkl.load(f)_ = view_samples(-1, samples)Question: What do you notice about your generated samples and how might you improve this model?When you answer this question, consider the following factors:  The dataset is biased; it is made of “celebrity” faces that are mostly white  Model size; larger models have the opportunity to learn more features in a data feature space  Optimization strategy; optimizers and number of epochs affect your final resultAnswer:  At the end of 10 iteration,it’s clear that the discriminator is performing better than the generator.The facial features are a bit complex even for a model with 4 convolutions.  Increasing the convolution layers and the no. of epochs for the current setting can give better results.  I’m a bit hesistant to tweak the hyperparameters since the gans are sensitive to hyperparameters so I went with the default parameters mentioned in the paper.  Using a different optimizer could also produce better results.",
        "url": "/deep%20learning/2020/06/14/face-generation/",
        "date": "Jun 14, 2020"
      }
      ,
    
      "deep-20learning-2020-06-08-tv-script-generation": {
        "title": "Generate TV Scripts",
        "author": "",
        "category": "",
        "content": "TV Script GenerationIn this project, you’ll generate your own Seinfeld TV scripts using RNNs.  You’ll be using part of the Seinfeld dataset of scripts from 9 seasons.  The Neural Network you’ll build will generate a new ,”fake” TV script, based on patterns it recognizes in this training data.Get the DataThe data is already provided for you in ./data/Seinfeld_Scripts.txt and you’re encouraged to open that file and look at the text.      As a first step, we’ll load in this data and look at some samples.    Then, you’ll be tasked with defining and training an RNN to generate a new script!  \"\"\"DON'T MODIFY ANYTHING IN THIS CELL\"\"\"# load in dataimport helperdata_dir = './data/Seinfeld_Scripts.txt'text = helper.load_data(data_dir)Explore the DataPlay around with view_line_range to view different parts of the data. This will give you a sense of the data you’ll be working with. You can see, for example, that it is all lowercase text, and each new line of dialogue is separated by a newline character \\n.view_line_range = (0, 10)\"\"\"DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\"\"\"import numpy as npprint('Dataset Stats')print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))lines = text.split('\\n')print('Number of lines: {}'.format(len(lines)))word_count_line = [len(line.split()) for line in lines]print('Average number of words in each line: {}'.format(np.average(word_count_line)))print()print('The lines {} to {}:'.format(*view_line_range))print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))Dataset StatsRoughly the number of unique words: 46367Number of lines: 109233Average number of words in each line: 5.544240293684143The lines 0 to 10:jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people trying to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, what do you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go. jerry: (pointing at georges shirt) see, to me, that button is in the worst possible spot. the second button literally makes or breaks the shirt, look at it. its too high! its in no-mans-land. you look like you live with your mother. george: are you through? jerry: you do of course try on, when you buy? george: yes, it was purple, i liked it, i dont actually recall considering the buttons. Implement Pre-processing FunctionsThe first thing to do to any dataset is pre-processing.  Implement the following pre-processing functions below:  Lookup Table  Tokenize PunctuationLookup TableTo create a word embedding, you first need to transform the words to ids.  In this function, create two dictionaries:  Dictionary to go from the words to an id, we’ll call vocab_to_int  Dictionary to go from the id to word, we’ll call int_to_vocabReturn these dictionaries in the following tuple (vocab_to_int, int_to_vocab)import problem_unittests as testsfrom collections import Counterdef create_lookup_tables(text):    \"\"\"    Create lookup tables for vocabulary    :param text: The text of tv scripts split into words    :return: A tuple of dicts (vocab_to_int, int_to_vocab)    \"\"\"    # TODO: Implement Function    word_counts = Counter(text)        sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)        int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}        return (vocab_to_int,int_to_vocab)\"\"\"DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\"\"\"tests.test_create_lookup_tables(create_lookup_tables)Tests PassedTokenize PunctuationWe’ll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks can create multiple ids for the same word. For example, “bye” and “bye!” would generate two different word ids.Implement the function token_lookup to return a dict that will be used to tokenize symbols like “!” into “||Exclamation_Mark||”.  Create a dictionary for the following symbols where the symbol is the key and value is the token:  Period ( . )  Comma ( , )  Quotation Mark ( “ )  Semicolon ( ; )  Exclamation mark ( ! )  Question mark ( ? )  Left Parentheses ( ( )  Right Parentheses ( ) )  Dash ( - )  Return ( \\n )This dictionary will be used to tokenize the symbols and add the delimiter (space) around it.  This separates each symbols as its own word, making it easier for the neural network to predict the next word. Make sure you don’t use a value that could be confused as a word.def token_lookup():    \"\"\"    Generate a dict to turn punctuation into a token.    :return: Tokenized dictionary where the key is the punctuation and the value is the token    \"\"\"    # TODO: Implement Function    tokens = {                '.': '||period||',                ',': '||comma||',                '\"': '||quotation_mark||',                ';': '||semicolon||',                '!': '||exclamation_mark||',                '?': '||question_mark||',                '(': '||left_parentheses||',                ')': '||right_Parentheses||',                '-': '||dash||',                '\\n': '||return||'            }        return tokens\"\"\"DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\"\"\"tests.test_tokenize(token_lookup)Tests PassedPre-process all the data and save itRunning the code cell below will pre-process all the data and save it to file. You’re encouraged to lok at the code for preprocess_and_save_data in the helpers.py file to see what it’s doing in detail, but you do not need to change this code.\"\"\"DON'T MODIFY ANYTHING IN THIS CELL\"\"\"# pre-process training datahelper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)Check PointThis is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk.\"\"\"DON'T MODIFY ANYTHING IN THIS CELL\"\"\"import helperimport problem_unittests as testsint_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()Build the Neural NetworkIn this section, you’ll build the components necessary to build an RNN by implementing the RNN Module and forward and backpropagation functions.Check Access to GPU\"\"\"DON'T MODIFY ANYTHING IN THIS CELL\"\"\"import torch# Check for a GPUtrain_on_gpu = torch.cuda.is_available()if not train_on_gpu:    print('No GPU found. Please use a GPU to train your neural network.')InputLet’s start with the preprocessed input data. We’ll use TensorDataset to provide a known format to our dataset; in combination with DataLoader, it will handle batching, shuffling, and other dataset iteration functions.You can create data with TensorDataset by passing in feature and target tensors. Then create a DataLoader as usual.data = TensorDataset(feature_tensors, target_tensors)data_loader = torch.utils.data.DataLoader(data,                                           batch_size=batch_size)BatchingImplement the batch_data function to batch words data into chunks of size batch_size using the TensorDataset and DataLoader classes.  You can batch words using the DataLoader, but it will be up to you to create feature_tensors and target_tensors of the correct size and content for a given sequence_length.For example, say we have these as input:words = [1, 2, 3, 4, 5, 6, 7]sequence_length = 4Your first feature_tensor should contain the values:[1, 2, 3, 4]And the corresponding target_tensor should just be the next “word”/tokenized word value:5This should continue with the second feature_tensor, target_tensor being:[2, 3, 4, 5]  # features6             # targetfrom torch.utils.data import TensorDataset, DataLoaderimport numpy as npdef batch_data(words, sequence_length, batch_size):    \"\"\"    Batch the neural network data using DataLoader    :param words: The word ids of the TV scripts    :param sequence_length: The sequence length of each batch    :param batch_size: The size of each batch; the number of sequences in a batch    :return: DataLoader with batched data    \"\"\"    # TODO: Implement function     #number of batches    #'//' integer division    number_batches = len(words)//batch_size             # only take full batches    words = words[:number_batches*batch_size]            # x -&gt; feature , y -&gt; target    x, y = [], []        for ii in range(0, len(words)- sequence_length):        x.append(words[ii:ii+sequence_length])        y.append(words[ii + sequence_length])               #convert numpy arrays to tensors    x_tensors = torch.from_numpy(np.array(x))    y_tensors = torch.from_numpy(np.array(y))            #Dataset wrapping tensors    data = TensorDataset(x_tensors, y_tensors)        #multi-process iterators over the dataset (our data loader)    data_loader = torch.utils.data.DataLoader(data, shuffle=True,                                          batch_size=batch_size)        # return a dataloader    return data_loader       # there is no test for this function, but you are encouraged to create# print statements and tests of your ownwords = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]sequence_length = 3data_loader = batch_data(words,sequence_length, 3)for feature, target in data_loader:    print(\"Feature: \"+ str(feature))    print(\"Target: \"+str(target))    print(\"Feature shape: \"+str(feature.shape))    print(\"Target shape: \"+str(target.shape))Feature: tensor([[  3,   4,   5],        [  2,   3,   4],        [  9,  10,  11]])Target: tensor([  6,   5,  12])Feature shape: torch.Size([3, 3])Target shape: torch.Size([3])Feature: tensor([[  5,   6,   7],        [  8,   9,  10],        [  1,   2,   3]])Target: tensor([  8,  11,   4])Feature shape: torch.Size([3, 3])Target shape: torch.Size([3])Feature: tensor([[ 4,  5,  6],        [ 7,  8,  9],        [ 6,  7,  8]])Target: tensor([  7,  10,   9])Feature shape: torch.Size([3, 3])Target shape: torch.Size([3])Test your dataloaderYou’ll have to modify this code to test a batching function, but it should look fairly similar.Below, we’re generating some test text data and defining a dataloader using the function you defined, above. Then, we are getting some sample batch of inputs sample_x and targets sample_y from our dataloader.Your code should return something like the following (likely in a different order, if you shuffled your data):torch.Size([10, 5])tensor([[ 28,  29,  30,  31,  32],        [ 21,  22,  23,  24,  25],        [ 17,  18,  19,  20,  21],        [ 34,  35,  36,  37,  38],        [ 11,  12,  13,  14,  15],        [ 23,  24,  25,  26,  27],        [  6,   7,   8,   9,  10],        [ 38,  39,  40,  41,  42],        [ 25,  26,  27,  28,  29],        [  7,   8,   9,  10,  11]])torch.Size([10])tensor([ 33,  26,  22,  39,  16,  28,  11,  43,  30,  12])SizesYour sample_x should be of size (batch_size, sequence_length) or (10, 5) in this case and sample_y should just have one dimension: batch_size (10).ValuesYou should also notice that the targets, sample_y, are the next value in the ordered test_text data. So, for an input sequence [ 28,  29,  30,  31,  32] that ends with the value 32, the corresponding output should be 33.# test dataloadertest_text = range(50)t_loader = batch_data(test_text, sequence_length=5, batch_size=10)data_iter = iter(t_loader)sample_x, sample_y = data_iter.next()print(sample_x.shape)print(sample_x)print()print(sample_y.shape)print(sample_y)torch.Size([10, 5])tensor([[ 26,  27,  28,  29,  30],        [  3,   4,   5,   6,   7],        [ 29,  30,  31,  32,  33],        [ 28,  29,  30,  31,  32],        [ 22,  23,  24,  25,  26],        [ 44,  45,  46,  47,  48],        [ 10,  11,  12,  13,  14],        [ 25,  26,  27,  28,  29],        [  4,   5,   6,   7,   8],        [ 36,  37,  38,  39,  40]])torch.Size([10])tensor([ 31,   8,  34,  33,  27,  49,  15,  30,   9,  41])Build the Neural NetworkImplement an RNN using PyTorch’s Module class. You may choose to use a GRU or an LSTM. To complete the RNN, you’ll have to implement the following functions for the class:  __init__ - The initialize function.  init_hidden - The initialization function for an LSTM/GRU hidden state  forward - Forward propagation function.The initialize function should create the layers of the neural network and save them to the class. The forward propagation function will use these layers to run forward propagation and generate an output and a hidden state.The output of this model should be the last batch of word scores after a complete sequence has been processed. That is, for each input sequence of words, we only want to output the word scores for a single, most likely, next word.Hints  Make sure to stack the outputs of the lstm to pass to your fully-connected layer, you can do this with lstm_output = lstm_output.contiguous().view(-1, self.hidden_dim)  You can get the last batch of word scores by shaping the output of the final, fully-connected layer like so:# reshape into (batch_size, seq_length, output_size)output = output.view(batch_size, -1, self.output_size)# get last batchout = output[:, -1]import torch.nn as nnclass RNN(nn.Module):        def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):        \"\"\"        Initialize the PyTorch RNN Module        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)        :param output_size: The number of output dimensions of the neural network        :param embedding_dim: The size of embeddings, should you choose to use them                :param hidden_dim: The size of the hidden layer outputs        :param dropout: dropout to add in between LSTM/GRU layers        \"\"\"        super(RNN, self).__init__()            # TODO: Implement function                # set class variables        self.output_size = output_size        self.n_layers = n_layers        self.hidden_dim = hidden_dim        self.vocab_size = vocab_size        self.embedding_dim = embedding_dim                self.dropout = nn.Dropout(0.20)         # define model layers        # embedding and LSTM layers        self.embedding = nn.Embedding(vocab_size, embedding_dim)        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,                             dropout = dropout, batch_first=True)                        #linear fully connnected layer        self.fc = nn.Linear(hidden_dim, output_size)                    def forward(self, nn_input, hidden):        \"\"\"        Forward propagation of the neural network        :param nn_input: The input to the neural network        :param hidden: The hidden state                :return: Two Tensors, the output of the neural network and the latest hidden state        \"\"\"        # TODO: Implement function                   #first dimension is batch size        batch_size = nn_input.size(0)                # embeddings and lstm_out        nn_input = nn_input.long()        embeds = self.embedding(nn_input)        lstm_out , hidden = self.lstm(embeds,hidden)                        # stack up lstm outputs        lstm_out = lstm_out.contiguous().view(-1,self.hidden_dim)                # dropout and fully-connected layer        output = self.dropout(lstm_out)        output = self.fc(output)                # reshape to be batch_size first        output = output.view(batch_size, -1, self.output_size)        out = output[:, -1] # get last batch of labels                # return one batch of output word scores and the hidden state        return out, hidden                def init_hidden(self, batch_size):        '''        Initialize the hidden state of an LSTM/GRU        :param batch_size: The batch_size of the hidden state        :return: hidden state of dims (n_layers, batch_size, hidden_dim)        '''        # Implement function                # initialize hidden state with zero weights, and move to GPU if available        weight = next(self.parameters()).data                if (train_on_gpu):            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())        else:            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())                return hidden\"\"\"DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\"\"\"tests.test_rnn(RNN, train_on_gpu)Tests PassedDefine forward and backpropagationUse the RNN class you implemented to apply forward and back propagation. This function will be called, iteratively, in the training loop as follows:loss = forward_back_prop(decoder, decoder_optimizer, criterion, inp, target)And it should return the average loss over a batch and the hidden state returned by a call to RNN(inp, hidden). Recall that you can get this loss by computing it, as usual, and calling loss.item().If a GPU is available, you should move your data to that GPU device, here.def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):    \"\"\"    Forward and backward propagation on the neural network    :param decoder: The PyTorch Module that holds the neural network    :param decoder_optimizer: The PyTorch optimizer for the neural network    :param criterion: The PyTorch loss function    :param inp: A batch of input to the neural network    :param target: The target output for the batch of input    :return: The loss and the latest hidden state Tensor    \"\"\"        # TODO: Implement Function        # move data to GPU, if available    if (train_on_gpu):        inp, target = inp.cuda(), target.cuda()     # perform backpropagation and optimization    hidden = tuple([each.data for each in hidden])    # zero accumulated gradients    rnn.zero_grad()    # get the output from the model    output, hidden = rnn(inp, hidden)    # calculate the loss and perform backprop    loss = criterion(output, target)    loss.backward()    # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.    clip = 5    nn.utils.clip_grad_norm_(rnn.parameters(), clip)    optimizer.step()        # return the loss over a batch and the hidden state produced by our model    return loss.item(), hidden# Note that these tests aren't completely extensive.# they are here to act as general checks on the expected outputs of your functions\"\"\"DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\"\"\"tests.test_forward_back_prop(RNN, forward_back_prop, train_on_gpu)Tests PassedNeural Network TrainingWith the structure of the network complete and data ready to be fed in the neural network, it’s time to train it.Train LoopThe training loop is implemented for you in the train_decoder function. This function will train the network over all the batches for the number of epochs given. The model progress will be shown every number of batches. This number is set with the show_every_n_batches parameter. You’ll set this parameter along with other parameters in the next section.\"\"\"DON'T MODIFY ANYTHING IN THIS CELL\"\"\"def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):    batch_losses = []        rnn.train()    print(\"Training for %d epoch(s)...\" % n_epochs)    for epoch_i in range(1, n_epochs + 1):                # initialize hidden state        hidden = rnn.init_hidden(batch_size)                for batch_i, (inputs, labels) in enumerate(train_loader, 1):                        # make sure you iterate over completely full batches, only            n_batches = len(train_loader.dataset)//batch_size            if(batch_i &gt; n_batches):                break                        # forward, back prop            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)                      # record loss            batch_losses.append(loss)            # printing loss stats            if batch_i % show_every_n_batches == 0:                print('Epoch: {:&gt;4}/{:&lt;4}  Loss: {}\\n'.format(                    epoch_i, n_epochs, np.average(batch_losses)))                batch_losses = []    # returns a trained rnn    return rnnHyperparametersSet and train the neural network with the following parameters:  Set sequence_length to the length of a sequence.  Set batch_size to the batch size.  Set num_epochs to the number of epochs to train for.  Set learning_rate to the learning rate for an Adam optimizer.  Set vocab_size to the number of uniqe tokens in our vocabulary.  Set output_size to the desired size of the output.  Set embedding_dim to the embedding dimension; smaller than the vocab_size.  Set hidden_dim to the hidden dimension of your RNN.  Set n_layers to the number of layers/cells in your RNN.  Set show_every_n_batches to the number of batches at which the neural network should print progress.If the network isn’t getting the desired results, tweak these parameters and/or the layers in the RNN class.# Data params# Sequence Lengthsequence_length = 10  # of words in a sequence# Batch Sizebatch_size = 256# data loader - do not changetrain_loader = batch_data(int_text, sequence_length, batch_size)# Training parameters# Number of Epochsnum_epochs = 15# Learning Ratelearning_rate = 0.001# Model parameters# Vocab sizevocab_size = len(vocab_to_int)# Output sizeoutput_size = vocab_size# Embedding Dimensionembedding_dim = 200# Hidden Dimensionhidden_dim = 300# Number of RNN Layersn_layers = 2# Show stats for every n number of batchesshow_every_n_batches = 500TrainIn the next cell, you’ll train the neural network on the pre-processed data.  If you have a hard time getting a good loss, you may consider changing your hyperparameters. In general, you may get better results with larger hidden and n_layer dimensions, but larger models take a longer time to train.  You should aim for a loss less than 3.5.You should also experiment with different sequence lengths, which determine the size of the long range dependencies that a model can learn.\"\"\"DON'T MODIFY ANYTHING IN THIS CELL\"\"\"# create model and move to gpu if availablernn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)if train_on_gpu:    rnn.cuda()# defining loss and optimization functions for trainingoptimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)criterion = nn.CrossEntropyLoss()# training the modeltrained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)# saving the trained modelhelper.save_model('./save/trained_rnn', trained_rnn)print('Model Trained and Saved')Training for 15 epoch(s)...Epoch:    1/15    Loss: 5.400850810050964Epoch:    1/15    Loss: 4.755062057495117Epoch:    1/15    Loss: 4.561995505332947Epoch:    1/15    Loss: 4.446177285194397Epoch:    1/15    Loss: 4.357284804344177Epoch:    1/15    Loss: 4.301316299915314Epoch:    2/15    Loss: 4.1930661497378034Epoch:    2/15    Loss: 4.1000709667205815Epoch:    2/15    Loss: 4.075804531574249Epoch:    2/15    Loss: 4.063328097820282Epoch:    2/15    Loss: 4.043516805171967Epoch:    2/15    Loss: 4.033186579227448Epoch:    3/15    Loss: 3.954846878381615Epoch:    3/15    Loss: 3.894479990005493Epoch:    3/15    Loss: 3.8999813141822814Epoch:    3/15    Loss: 3.88797496509552Epoch:    3/15    Loss: 3.8634607906341554Epoch:    3/15    Loss: 3.889809859752655Epoch:    4/15    Loss: 3.8078963885108426Epoch:    4/15    Loss: 3.7565750269889833Epoch:    4/15    Loss: 3.771946996688843Epoch:    4/15    Loss: 3.7733569803237916Epoch:    4/15    Loss: 3.752300311088562Epoch:    4/15    Loss: 3.7664224491119387Epoch:    5/15    Loss: 3.710319888676656Epoch:    5/15    Loss: 3.6493622069358826Epoch:    5/15    Loss: 3.6655515828132628Epoch:    5/15    Loss: 3.6671839327812195Epoch:    5/15    Loss: 3.682479444026947Epoch:    5/15    Loss: 3.6798709683418274Epoch:    6/15    Loss: 3.635241383817407Epoch:    6/15    Loss: 3.5837333970069887Epoch:    6/15    Loss: 3.584761240005493Epoch:    6/15    Loss: 3.60858926486969Epoch:    6/15    Loss: 3.602405499458313Epoch:    6/15    Loss: 3.6211153764724733Epoch:    7/15    Loss: 3.571412881296261Epoch:    7/15    Loss: 3.5239075717926025Epoch:    7/15    Loss: 3.5279815411567688Epoch:    7/15    Loss: 3.540113302707672Epoch:    7/15    Loss: 3.546058063983917Epoch:    7/15    Loss: 3.549504738330841Epoch:    8/15    Loss: 3.505566673385527Epoch:    8/15    Loss: 3.477623617172241Epoch:    8/15    Loss: 3.4766470856666567Epoch:    8/15    Loss: 3.488004832267761Epoch:    8/15    Loss: 3.494438717842102Epoch:    8/15    Loss: 3.5211046676635744Epoch:    9/15    Loss: 3.466254509113812Epoch:    9/15    Loss: 3.42097306394577Epoch:    9/15    Loss: 3.4331134581565856Epoch:    9/15    Loss: 3.4487393898963927Epoch:    9/15    Loss: 3.466208869457245Epoch:    9/15    Loss: 3.4740055375099184Epoch:   10/15    Loss: 3.4155538324664594Epoch:   10/15    Loss: 3.3784340810775757Epoch:   10/15    Loss: 3.414306341171265Epoch:   10/15    Loss: 3.4264081296920774Epoch:   10/15    Loss: 3.4214731831550598Epoch:   10/15    Loss: 3.405343810081482Epoch:   11/15    Loss: 3.3835746024414095Epoch:   11/15    Loss: 3.3425019755363463Epoch:   11/15    Loss: 3.363470724582672Epoch:   11/15    Loss: 3.3808001279830933Epoch:   11/15    Loss: 3.398699206829071Epoch:   11/15    Loss: 3.3857973647117614Epoch:   12/15    Loss: 3.354548788507254Epoch:   12/15    Loss: 3.3126464138031007Epoch:   12/15    Loss: 3.325894530296326Epoch:   12/15    Loss: 3.3447298274040222Epoch:   12/15    Loss: 3.3667372989654543Epoch:   12/15    Loss: 3.3693391184806822Epoch:   13/15    Loss: 3.3256343301169626Epoch:   13/15    Loss: 3.2843553018569946Epoch:   13/15    Loss: 3.3178746104240417Epoch:   13/15    Loss: 3.3183268308639526Epoch:   13/15    Loss: 3.3216225266456605Epoch:   13/15    Loss: 3.336115911483765Epoch:   14/15    Loss: 3.3017076548579265Epoch:   14/15    Loss: 3.2707825059890747Epoch:   14/15    Loss: 3.2764017219543455Epoch:   14/15    Loss: 3.2912625250816343Epoch:   14/15    Loss: 3.3043338356018066Epoch:   14/15    Loss: 3.3351227378845216Epoch:   15/15    Loss: 3.2753917993873447Epoch:   15/15    Loss: 3.231657418727875Epoch:   15/15    Loss: 3.2617148365974424Epoch:   15/15    Loss: 3.2862412815093993Epoch:   15/15    Loss: 3.288367618560791Epoch:   15/15    Loss: 3.294407793521881/opt/conda/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.  \"type \" + obj.__name__ + \". It won't be checked \"Model Trained and SavedQuestion: How did you decide on your model hyperparameters?For example, did you try different sequence_lengths and find that one size made the model converge faster? What about your hidden_dim and n_layers; how did you decide on those?Answer:  Number of layers: Selecting a single layer for the model resulted in the model not performing upto expectations so tried a 3 layerer model but it took a lot of training time and did not converge.So I decide to stick with a two layer network.  Embedding Dimensions : An embedding dimension of 200-300 is generally used and I decided to go with 300 since the loss was lesser than 200.  Batch Size: Started with a batch of 32,training was fast and loss was converging rapidly at first but then it randomly shooted upwards so I tested the values in a sequence on 32,64,..256 where 256 had the least loss at epoch 1.  Hidden Dimension: Hidden Dimension is generally selected in the range between 200 and 500.The training time with more dimensions was always high so I decided to stick with 200.  Sequence Length: Almost all the words in english that make sense is less than 10 so I initially had set the range between 5 to 10 and later found that sequence of 10 was ideal.CheckpointAfter running the above training cell, your model will be saved by name, trained_rnn, and if you save your notebook progress, you can pause here and come back to this code at another time. You can resume your progress by running the next cell, which will load in our word:id dictionaries and load in your saved model by name!\"\"\"DON'T MODIFY ANYTHING IN THIS CELL\"\"\"import torchimport helperimport problem_unittests as tests_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()trained_rnn = helper.load_model('./save/trained_rnn')Generate TV ScriptWith the network trained and saved, you’ll use it to generate a new, “fake” Seinfeld TV script in this section.Generate TextTo generate the text, the network needs to start with a single word and repeat its predictions until it reaches a set length. You’ll be using the generate function to do this. It takes a word id to start with, prime_id, and generates a set length of text, predict_len. Also note that it uses topk sampling to introduce some randomness in choosing the most likely next word, given an output set of word scores!\"\"\"DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\"\"\"import torch.nn.functional as Fdef generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):    \"\"\"    Generate text using the neural network    :param decoder: The PyTorch Module that holds the trained neural network    :param prime_id: The word id to start the first prediction    :param int_to_vocab: Dict of word id keys to word values    :param token_dict: Dict of puncuation tokens keys to puncuation values    :param pad_value: The value used to pad a sequence    :param predict_len: The length of text to generate    :return: The generated text    \"\"\"    rnn.eval()        # create a sequence (batch_size=1) with the prime_id    current_seq = np.full((1, sequence_length), pad_value)    current_seq[-1][-1] = prime_id    predicted = [int_to_vocab[prime_id]]        for _ in range(predict_len):        if train_on_gpu:            current_seq = torch.LongTensor(current_seq).cuda()        else:            current_seq = torch.LongTensor(current_seq)                # initialize the hidden state        hidden = rnn.init_hidden(current_seq.size(0))                # get the output of the rnn        output, _ = rnn(current_seq, hidden)                # get the next word probabilities        p = F.softmax(output, dim=1).data        if(train_on_gpu):            p = p.cpu() # move to cpu                 # use top_k sampling to get the index of the next word        top_k = 5        p, top_i = p.topk(top_k)        top_i = top_i.numpy().squeeze()                # select the likely next word index with some element of randomness        p = p.numpy().squeeze()        word_i = np.random.choice(top_i, p=p/p.sum())                # retrieve that word from the dictionary        word = int_to_vocab[word_i]        predicted.append(word)                     # the generated word becomes the next \"current sequence\" and the cycle can continue        current_seq = np.roll(current_seq, -1, 1)        current_seq[-1][-1] = word_i        gen_sentences = ' '.join(predicted)        # Replace punctuation tokens    for key, token in token_dict.items():        ending = ' ' if key in ['\\n', '(', '\"'] else ''        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)    gen_sentences = gen_sentences.replace('\\n ', '\\n')    gen_sentences = gen_sentences.replace('( ', '(')        # return all the sentences    return gen_sentencesGenerate a New ScriptIt’s time to generate the text. Set gen_length to the length of TV script you want to generate and set prime_word to one of the following to start the prediction:  “jerry”  “elaine”  “george”  “kramer”You can set the prime word to any word in our dictionary, but it’s best to start with a name for generating a TV script. (You can also start with any other names you find in the original text file!)# run the cell multiple times to get different results!gen_length = 400 # modify the length to your preferenceprime_word = 'jerry' # name for starting the script\"\"\"DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\"\"\"pad_word = helper.SPECIAL_WORDS['PADDING']generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)print(generated_script)/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:53: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().jerry:.\"\"jerry:(to jerry) oh, yeah, yeah.jerry:(to himself) you see?elaine: oh, yeah...jerry: hey.kramer: well, i got to be a very interesting driver.(george nods)kramer:(pointing at the table) you know, i just had to go to the bathroom. i was wondering if you want to go with him, i don't know what you want, i can't.george:(laughs) oh, no... i just got a little more than the best way to be in the bathroom?jerry: no, it's all the way.george: oh, well. you don't have to go.jerry:(confused) what?elaine: well, you know, you know..jerry:(pointing at the counter) hey.jerry: hey!elaine:(quietly) yeah. i got it. i don't want to have any money.elaine:(pointing out) what is this?george: i can't. i don't know...george: oh, hi. hi jerry.jerry: hi, hi.elaine: hi, jerry.elaine:(shouting) what are you talking about?jerry: yeah, i don't know what i do.george: i don't know. i don't know...jerry: i don't have it. i mean, i don't want to be able to tell you this. you know what i want to say,\"george:\" what happened?kramer: well, i don't think you know, i just had a little.kramer: yeah, i think i got the tape.jerry: i don't know. but i don't know how to have a good time.elaine: oh my god. you want to know?jerry: well, i just don't want to have aSave your favorite scriptsOnce you have a script that you like (or find interesting), save it to a text file!# save script to a text filef =  open(\"generated_script_1.txt\",\"w\")f.write(generated_script)f.close()The TV Script is Not PerfectIt’s ok if the TV script doesn’t make perfect sense. It should look like alternating lines of dialogue, here is one such example of a few generated lines.Example generated script  jerry: what about me?  jerry: i don’t have to wait.  kramer:(to the sales table)  elaine:(to jerry) hey, look at this, i’m a good doctor.  newman:(to elaine) you think i have no idea of this…  elaine: oh, you better take the phone, and he was a little nervous.  kramer:(to the phone) hey, hey, jerry, i don’t want to be a little bit.(to kramer and jerry) you can’t.  jerry: oh, yeah. i don’t even know, i know.  jerry:(to the phone) oh, i know.  kramer:(laughing) you know…(to jerry) you don’t know.You can see that there are multiple characters that say (somewhat) complete sentences, but it doesn’t have to be perfect! It takes quite a while to get good results, and often, you’ll have to use a smaller vocabulary (and discard uncommon words), or get more data.  The Seinfeld dataset is about 3.4 MB, which is big enough for our purposes; for script generation you’ll want more than 1 MB of text, generally.",
        "url": "/deep%20learning/2020/06/08/tv-script-generation/",
        "date": "Jun 08, 2020"
      }
      ,
    
      "deep-20learning-2020-06-04-dog-breed-classifier-app": {
        "title": "Dog Breed Classifier with CNN",
        "author": "",
        "category": "",
        "content": "Convolutional Neural NetworksWhy We’re HereIn this notebook, you will make the first steps towards developing an algorithm that could be used as part of a mobile or web app.  At the end of this project, your code will accept any user-supplied image as input.  If a dog is detected in the image, it will provide an estimate of the dog’s breed.  If a human is detected, it will provide an estimate of the dog breed that is most resembling.  The image below displays potential sample output of your finished project (… but we expect that each student’s algorithm will behave differently!).In this real-world setting, you will need to piece together a series of models to perform different tasks; for instance, the algorithm that detects humans in an image will be different from the CNN that infers dog breed.  There are many points of possible failure, and no perfect algorithm exists.  Your imperfect solution will nonetheless create a fun user experience!The Road AheadWe break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.  Step 0: Import Datasets  Step 1: Detect Humans  Step 2: Detect Dogs  Step 3: Create a CNN to Classify Dog Breeds (from Scratch)  Step 4: Create a CNN to Classify Dog Breeds (using Transfer Learning)  Step 5: Write your Algorithm  Step 6: Test Your AlgorithmStep 0: Import DatasetsMake sure that you’ve downloaded the required human and dog datasets:Note: if you are using the Udacity workspace, you DO NOT need to re-download these - they can be found in the /data folder as noted in the cell below.      Download the dog dataset.  Unzip the folder and place it in this project’s home directory, at the location /dog_images.        Download the human dataset.  Unzip the folder and place it in the home directory, at location /lfw.  Note: If you are using a Windows machine, you are encouraged to use 7zip to extract the folder.In the code cell below, we save the file paths for both the human (LFW) dataset and dog dataset in the numpy arrays human_files and dog_files.import numpy as npfrom glob import glob# load filenames for human and dog imageshuman_files = np.array(glob(\"/data/lfw/*/*\"))dog_files = np.array(glob(\"/data/dog_images/*/*/*\"))# print number of images in each datasetprint('There are %d total human images.' % len(human_files))print('There are %d total dog images.' % len(dog_files))There are 13233 total human images.There are 8351 total dog images.Step 1: Detect HumansIn this section, we use OpenCV’s implementation of Haar feature-based cascade classifiers to detect human faces in images.OpenCV provides many pre-trained face detectors, stored as XML files on github.  We have downloaded one of these detectors and stored it in the haarcascades directory.  In the next code cell, we demonstrate how to use this detector to find human faces in a sample image.import cv2                import matplotlib.pyplot as plt                        %matplotlib inline                               # extract pre-trained face detectorface_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')# load color (BGR) imageimg = cv2.imread(human_files[0])# convert BGR image to grayscalegray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)# find faces in imagefaces = face_cascade.detectMultiScale(gray)# print number of faces detected in the imageprint('Number of faces detected:', len(faces))# get bounding box for each detected facefor (x,y,w,h) in faces:    # add bounding box to color image    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)    # convert BGR image to RGB for plottingcv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)# display the image, along with bounding boxplt.imshow(cv_rgb)plt.show()Number of faces detected: 1Before using any of the face detectors, it is standard procedure to convert the images to grayscale.  The detectMultiScale function executes the classifier stored in face_cascade and takes the grayscale image as a parameter.In the above code, faces is a numpy array of detected faces, where each row corresponds to a detected face.  Each detected face is a 1D array with four entries that specifies the bounding box of the detected face.  The first two entries in the array (extracted in the above code as x and y) specify the horizontal and vertical positions of the top left corner of the bounding box.  The last two entries in the array (extracted here as w and h) specify the width and height of the box.Write a Human Face DetectorWe can use this procedure to write a function that returns True if a human face is detected in an image and False otherwise.  This function, aptly named face_detector, takes a string-valued file path to an image as input and appears in the code block below.# returns \"True\" if face is detected in image stored at img_pathdef face_detector(img_path):    img = cv2.imread(img_path)    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)    faces = face_cascade.detectMultiScale(gray)    return len(faces) &gt; 0(IMPLEMENTATION) Assess the Human Face DetectorQuestion 1: Use the code cell below to test the performance of the face_detector function.  What percentage of the first 100 images in human_files have a detected human face?  What percentage of the first 100 images in dog_files have a detected human face?Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face.  You will see that our algorithm falls short of this goal, but still gives acceptable performance.  We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays human_files_short and dog_files_short.Answer: Percentage of Human Faces detected in human_files is 98% whereas Percentage of Human Faces detected in dog_files is 17%from tqdm import tqdmhuman_files_short = human_files[:100]dog_files_short = dog_files[:100]#-#-# Do NOT modify the code above this line. #-#-### TODO: Test the performance of the face_detector algorithm ## on the images in human_files_short and dog_files_short.face_detected_in_human_files = 0face_detected_in_dog_files = 0for images in human_files_short:    if face_detector(images):        face_detected_in_human_files += 1for images in dog_files_short:    if face_detector(images):        face_detected_in_dog_files += 1        print(f\"Percentage of Human Faces detected in human_files : {face_detected_in_human_files}%\")print(f\"Percentage of Human Faces detected in dog_files : {face_detected_in_dog_files}%\")Percentage of Human Faces detected in human_files : 98%Percentage of Human Faces detected in dog_files : 17%We suggest the face detector from OpenCV as a potential way to detect human images in your algorithm, but you are free to explore other approaches, especially approaches that make use of deep learning :).  Please use the code cell below to design and test your own face detection algorithm.  If you decide to pursue this optional task, report performance on human_files_short and dog_files_short.### (Optional) ### TODO: Test performance of anotherface detection algorithm.### Feel free to use as many code cells as needed.Step 2: Detect DogsIn this section, we use a pre-trained model to detect dogs in images.Obtain Pre-trained VGG-16 ModelThe code cell below downloads the VGG-16 model, along with weights that have been trained on ImageNet, a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of 1000 categories.import torchimport torchvision.models as models# define VGG16 modelVGG16 = models.vgg16(pretrained=True)# check if CUDA is availableuse_cuda = torch.cuda.is_available()# move model to GPU if CUDA is availableif use_cuda:    VGG16 = VGG16.cuda()Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.torch/models/vgg16-397923af.pth100%|██████████| 553433881/553433881 [00:05&lt;00:00, 99176335.73it/s] Given an image, this pre-trained VGG-16 model returns a prediction (derived from the 1000 possible categories in ImageNet) for the object that is contained in the image.(IMPLEMENTATION) Making Predictions with a Pre-trained ModelIn the next code cell, you will write a function that accepts a path to an image (such as 'dogImages/train/001.Affenpinscher/Affenpinscher_00001.jpg') as input and returns the index corresponding to the ImageNet class that is predicted by the pre-trained VGG-16 model.  The output should always be an integer between 0 and 999, inclusive.Before writing the function, make sure that you take the time to learn  how to appropriately pre-process tensors for pre-trained models in the PyTorch documentation.from PIL import Imageimport torchvision.transforms as transformsdef VGG16_predict(img_path):    '''    Use pre-trained VGG-16 model to obtain index corresponding to     predicted ImageNet class for image at specified path        Args:        img_path: path to an image            Returns:        Index corresponding to VGG-16 model's prediction    '''        ## TODO: Complete the function.    ## Load and pre-process an image from the given img_path    ## Return the *index* of the predicted class for that image    image = Image.open(img_path).convert('RGB')        data_transform = transforms.Compose([transforms.Resize(224),                                         transforms.CenterCrop(224),                                         transforms.ToTensor(),                                         transforms.Normalize([0.485, 0.456, 0.406],                                                              [0.229, 0.224, 0.225])])        image = data_transform(image).unsqueeze(0)    if use_cuda:        image = image.cuda()    predict = VGG16(image)    predicted_class = predict.data.cpu().argmax()            return int(predicted_class) # predicted class index(IMPLEMENTATION) Write a Dog DetectorWhile looking at the dictionary, you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from 'Chihuahua' to 'Mexican hairless'.  Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained VGG-16 model, we need only check if the pre-trained model predicts an index between 151 and 268 (inclusive).Use these ideas to complete the dog_detector function below, which returns True if a dog is detected in an image (and False if not).### returns \"True\" if a dog is detected in the image stored at img_pathdef dog_detector(img_path):    ## TODO: Complete the function.    class_label = VGG16_predict(img_path)        return ( (class_label &gt;= 151) &amp; (class_label &lt;=  268))  # true/false(IMPLEMENTATION) Assess the Dog DetectorQuestion 2: Use the code cell below to test the performance of your dog_detector function.  What percentage of the images in human_files_short have a detected dog?  What percentage of the images in dog_files_short have a detected dog?Answer: Dog Images in Human dataset is 1% whereas Dog Images in Dog Dataset is 100%### TODO: Test the performance of the dog_detector function### on the images in human_files_short and dog_files_short.from tqdm import tqdm# Initiallizing:dog_images_in_human_files = 0dog_images_in_dog_files = 0for i in tqdm(range(len(human_files_short))):    if dog_detector(human_files_short[i]):        dog_images_in_human_files += 1        for i in tqdm(range(len(dog_files_short))):    if dog_detector(dog_files_short[i]):        dog_images_in_dog_files += 1    print(f'Dog Images in Human dataset: {dog_images_in_human_files}%')print(f'Dog Images in Dog Dataset: {dog_images_in_dog_files}%')100%|██████████| 100/100 [00:03&lt;00:00, 28.38it/s]100%|██████████| 100/100 [00:04&lt;00:00, 25.46it/s]Dog Images in Human dataset: 1%Dog Images in Dog Dataset: 100%We suggest VGG-16 as a potential network to detect dog images in your algorithm, but you are free to explore other pre-trained networks (such as Inception-v3, ResNet-50, etc).  Please use the code cell below to test other pre-trained PyTorch models.  If you decide to pursue this optional task, report performance on human_files_short and dog_files_short.### (Optional) ### TODO: Report the performance of another pre-trained network.### Feel free to use as many code cells as needed.Step 3: Create a CNN to Classify Dog Breeds (from Scratch)Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images.  In this step, you will create a CNN that classifies dog breeds.  You must create your CNN from scratch (so, you can’t use transfer learning yet!), and you must attain a test accuracy of at least 10%.  In Step 4 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.We mention that the task of assigning breed to dogs from images is considered exceptionally challenging.  To see why, consider that even a human would have trouble distinguishing between a Brittany and a Welsh Springer Spaniel.            Brittany      Welsh Springer Spaniel                              It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).            Curly-Coated Retriever      American Water Spaniel                              Likewise, recall that labradors come in yellow, chocolate, and black.  Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.            Yellow Labrador      Chocolate Labrador      Black Labrador                                    We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.Remember that the practice is far ahead of the theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun!(IMPLEMENTATION) Specify Data Loaders for the Dog DatasetUse the code cell below to write three separate data loaders for the training, validation, and test datasets of dog images (located at dog_images/train, dog_images/valid, and dog_images/test, respectively).  You may find this documentation on custom datasets to be a useful resource.  If you are interested in augmenting your training and/or validation data, check out the wide variety of transforms!import osfrom torchvision import datasetsfrom PIL import ImageFileImageFile.LOAD_TRUNCATED_IMAGES = True### TODO: Write data loaders for training, validation, and test sets## Specify appropriate transforms, and batch_sizesnum_workers = 0batch_size = 24data_dir = '/data/dog_images/'train_dir = os.path.join(data_dir, 'train/')test_dir = os.path.join(data_dir, 'test/')valid_dir = os.path.join(data_dir, 'valid/')data_transforms = {                    'train' : transforms.Compose([ transforms.RandomRotation(30),                                                   transforms.RandomResizedCrop(224),                                                   transforms.RandomHorizontalFlip(),                                                   transforms.ToTensor(),                                                   transforms.Normalize([0.485, 0.456, 0.406],                                                                        [0.229, 0.224, 0.225])]),                                       'test' : transforms.Compose([ transforms.Resize(255),                                                  transforms.CenterCrop(224),                                                  transforms.ToTensor(),                                                  transforms.Normalize([0.485, 0.456, 0.406],                                                                       [0.229, 0.224, 0.225])]),                   'valid' : transforms.Compose([ transforms.Resize(255),                                                  transforms.CenterCrop(224),                                                  transforms.ToTensor(),                                                  transforms.Normalize([0.485, 0.456, 0.406],                                                                       [0.229, 0.224, 0.225])])                  }dataset = {               'train_data' : datasets.ImageFolder(train_dir, transform=data_transforms['train']),            'test_data' : datasets.ImageFolder(test_dir,transform=data_transforms['test']),            'val_data' : datasets.ImageFolder(valid_dir,transform=data_transforms['valid'])            }loaders_scratch  = {                         'train' : torch.utils.data.DataLoader(dataset['train_data'], batch_size=batch_size,                                                                num_workers=num_workers, shuffle=True),                        'test' : torch.utils.data.DataLoader(dataset['test_data'], batch_size=batch_size,                                                               num_workers=num_workers, shuffle=True),                        'valid' : torch.utils.data.DataLoader(dataset['val_data'], batch_size=batch_size,                                                               num_workers=num_workers, shuffle=True)                    }Question 3: Describe your chosen procedure for preprocessing the data.  How does your code resize the images (by cropping, stretching, etc)?  What size did you pick for the input tensor, and why?  Did you decide to augment the dataset?  If so, how (through translations, flips, rotations, etc)?  If not, why not?Answer:  Since I have decided to keep the same data loader throughout the project,transforming the images to the input size that is accepted by a pre-trained model is essential so the size of the image is resized to 224 x 224.  Data Augmentation improves the performance of models so I have used a Random Rotation and Horizontal Flip along with RandomResizedCrop which will randomly crop the original image and it’s aspect ratio and then resize it to the desired size mentioned.  Since Normalization of the Input tensors speed the training as well as improve the performance I normalized the data with standard values used in transfer learning.(IMPLEMENTATION) Model ArchitectureCreate a CNN to classify dog breed.  Use the template in the code cell below.import torch.nn as nnimport torch.nn.functional as F# define the CNN architectureclass Net(nn.Module):    ### TODO: choose an architecture, and complete the class    def __init__(self):        super(Net, self).__init__()        ## Define layers of a CNN        # convolutional layer (sees 32x32x3 image tensor)        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)        # convolutional layer (sees 16x16x16 tensor)        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)        # convolutional layer (sees 8x8x32 tensor)        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)        # max pooling layer        self.pool = nn.MaxPool2d(2, 2)        # linear layer (64 * 4 * 4 -&gt; 500)        self.fc1 = nn.Linear(64 * 28 * 28, 500)        # linear layer (500 -&gt; 10)        self.fc2 = nn.Linear(500, 133)        # dropout layer (p=0.25)        self.dropout = nn.Dropout(0.25)            def forward(self, x):        ## Define forward behavior        # Convolution Layers                        x = self.pool(F.relu(self.conv1(x)))        x = self.pool(F.relu(self.conv2(x)))        x = self.pool(F.relu(self.conv3(x)))        # flatten image input        x = x.view(-1, 64 * 28 * 28)        # add dropout layer        x = self.dropout(x)        # add 1st hidden layer, with relu activation function        x = F.relu(self.fc1(x))        # add dropout layer        x = self.dropout(x)        # add 2nd hidden layer, with relu activation function        x = self.fc2(x)              return x#-#-# You so NOT have to modify the code below this line. #-#-## instantiate the CNNmodel_scratch = Net()# move tensors to GPU if CUDA is availableif use_cuda:    model_scratch.cuda()Question 4: Outline the steps you took to get to your final CNN architecture and your reasoning at each step.Answer:  The class labels have a length of 133 which means the output of last dense layers will be 133.  The input is a color image which is resized into a 224x224x3 pixels i.e a depth of 3 and x-y dimensions of 224x224, so input channel of the first convolution layer is 3(depth).  As we computation in high dimensionality will slow the training process, Max Pooling layer that reduces the x-y dimensions is used along with a convolution kernel of size 3 i.e 3x3.This setting will half the x-y dimension in the forward pass.  Each convolution layer had the depth more than the previous layer to capture complex patterns.  Every convolution layer had a relu activation and used a max pooling layer after every activation.  The first Dense layer was attached with a droput of 25% to avoid overfitting followed by an relu activation.  The last dense layer did not follow an activation function since Cross Entropy loss function is used as the loss function.(IMPLEMENTATION) Specify Loss Function and OptimizerUse the next code cell to specify a loss function and optimizer.  Save the chosen loss function as criterion_scratch, and the optimizer as optimizer_scratch below.import torch.optim as optim### TODO: select loss functioncriterion_scratch = nn.CrossEntropyLoss()### TODO: select optimizeroptimizer_scratch = optim.SGD(model_scratch.parameters(), lr=0.03)(IMPLEMENTATION) Train and Validate the ModelTrain and validate your model in the code cell below.  Save the final model parameters at filepath 'model_scratch.pt'.def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):    \"\"\"returns trained model\"\"\"    # initialize tracker for minimum validation loss    valid_loss_min = np.Inf         for epoch in range(1, n_epochs+1):        # initialize variables to monitor training and validation loss        train_loss = 0.0        valid_loss = 0.0                ###################        # train the model #        ###################        model.train()        for batch_idx, (data, target) in enumerate(loaders['train']):            # move to GPU            if use_cuda:                data, target = data.cuda(), target.cuda()            ## find the loss and update the model parameters accordingly            ## record the average training loss, using something like            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))            optimizer.zero_grad()            # forward pass: compute predicted outputs by passing inputs to the model            output = model(data)            # calculate the batch loss            loss = criterion(output, target)            # backward pass: compute gradient of the loss with respect to model parameters            loss.backward()            # perform a single optimization step (parameter update)            optimizer.step()            # update training loss            train_loss += loss.item()*data.size(0)                                ######################            # validate the model #        ######################        model.eval()        for batch_idx, (data, target) in enumerate(loaders['valid']):            # move to GPU            if use_cuda:                data, target = data.cuda(), target.cuda()            ## update the average validation loss            output = model(data)            loss = criterion(output, target)            # update average validation loss             valid_loss += loss.item()*data.size(0)                # calculate average losses        train_loss = train_loss/len(loaders['train'].dataset)        valid_loss = valid_loss/len(loaders['valid'].dataset)                # print training/validation statistics         print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(            epoch,             train_loss,            valid_loss            ))                ## TODO: save the model if validation loss has decreased        if valid_loss &lt;= valid_loss_min:            print('Validation loss decreased ({:.6f} --&gt; {:.6f}).  Saving model ...'.format(            valid_loss_min,            valid_loss))            torch.save(model.state_dict(), save_path)            valid_loss_min = valid_loss       # return trained model    return model# train the modelmodel_scratch = train(25, loaders_scratch, model_scratch, optimizer_scratch,                       criterion_scratch, use_cuda, 'model_scratch.pt')# load the model that got the best validation accuracymodel_scratch.load_state_dict(torch.load('model_scratch.pt'))Epoch: 1 \tTraining Loss: 4.876332 \tValidation Loss: 4.816575Validation loss decreased (inf --&gt; 4.816575).  Saving model ...Epoch: 2 \tTraining Loss: 4.748259 \tValidation Loss: 4.572024Validation loss decreased (4.816575 --&gt; 4.572024).  Saving model ...Epoch: 3 \tTraining Loss: 4.627809 \tValidation Loss: 4.499690Validation loss decreased (4.572024 --&gt; 4.499690).  Saving model ...Epoch: 4 \tTraining Loss: 4.586300 \tValidation Loss: 4.449453Validation loss decreased (4.499690 --&gt; 4.449453).  Saving model ...Epoch: 5 \tTraining Loss: 4.526802 \tValidation Loss: 4.497794Epoch: 6 \tTraining Loss: 4.492073 \tValidation Loss: 4.417369Validation loss decreased (4.449453 --&gt; 4.417369).  Saving model ...Epoch: 7 \tTraining Loss: 4.443948 \tValidation Loss: 4.287956Validation loss decreased (4.417369 --&gt; 4.287956).  Saving model ...Epoch: 8 \tTraining Loss: 4.408243 \tValidation Loss: 4.230388Validation loss decreased (4.287956 --&gt; 4.230388).  Saving model ...Epoch: 9 \tTraining Loss: 4.376940 \tValidation Loss: 4.178228Validation loss decreased (4.230388 --&gt; 4.178228).  Saving model ...Epoch: 10 \tTraining Loss: 4.344263 \tValidation Loss: 4.196465Epoch: 11 \tTraining Loss: 4.300478 \tValidation Loss: 4.197212Epoch: 12 \tTraining Loss: 4.262984 \tValidation Loss: 4.088739Validation loss decreased (4.178228 --&gt; 4.088739).  Saving model ...Epoch: 13 \tTraining Loss: 4.233462 \tValidation Loss: 4.032015Validation loss decreased (4.088739 --&gt; 4.032015).  Saving model ...Epoch: 14 \tTraining Loss: 4.188986 \tValidation Loss: 4.074294Epoch: 15 \tTraining Loss: 4.154463 \tValidation Loss: 3.936569Validation loss decreased (4.032015 --&gt; 3.936569).  Saving model ...Epoch: 16 \tTraining Loss: 4.113894 \tValidation Loss: 3.955761Epoch: 17 \tTraining Loss: 4.070495 \tValidation Loss: 3.964011Epoch: 18 \tTraining Loss: 4.016574 \tValidation Loss: 3.931126Validation loss decreased (3.936569 --&gt; 3.931126).  Saving model ...Epoch: 19 \tTraining Loss: 4.018847 \tValidation Loss: 3.961809Epoch: 20 \tTraining Loss: 3.985453 \tValidation Loss: 3.931244Epoch: 21 \tTraining Loss: 3.957856 \tValidation Loss: 3.969822Epoch: 22 \tTraining Loss: 3.932438 \tValidation Loss: 4.001125Epoch: 23 \tTraining Loss: 3.899744 \tValidation Loss: 3.956952Epoch: 24 \tTraining Loss: 3.873465 \tValidation Loss: 3.933152Epoch: 25 \tTraining Loss: 3.850253 \tValidation Loss: 3.939559(IMPLEMENTATION) Test the ModelTry out your model on the test dataset of dog images.  Use the code cell below to calculate and print the test loss and accuracy.  Ensure that your test accuracy is greater than 10%.def test(loaders, model, criterion, use_cuda):    # monitor test loss and accuracy    test_loss = 0.    correct = 0.    total = 0.    model.eval()    for batch_idx, (data, target) in enumerate(loaders['test']):        # move to GPU        if use_cuda:            data, target = data.cuda(), target.cuda()        # forward pass: compute predicted outputs by passing inputs to the model        output = model(data)        # calculate the loss        loss = criterion(output, target)        # update average test loss         test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))        # convert output probabilities to predicted class        pred = output.data.max(1, keepdim=True)[1]        # compare predictions to true label        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())        total += data.size(0)                print('Test Loss: {:.6f}\\n'.format(test_loss))    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (        100. * correct / total, correct, total))# call test function    test(loaders_scratch, model_scratch, criterion_scratch, use_cuda)Test Loss: 3.923931Test Accuracy: 11% (99/836)Step 4: Create a CNN to Classify Dog Breeds (using Transfer Learning)You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.(IMPLEMENTATION) Specify Data Loaders for the Dog DatasetUse the code cell below to write three separate data loaders for the training, validation, and test datasets of dog images (located at dogImages/train, dogImages/valid, and dogImages/test, respectively).If you like, you are welcome to use the same data loaders from the previous step, when you created a CNN from scratch.## TODO: Specify data loadersloaders_transfer = loaders_scratchprint(VGG16)print(\"Input Features:\",VGG16.classifier[6].in_features) print(\"Output Features: \",VGG16.classifier[6].out_features)VGG(  (features): Sequential(    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (1): ReLU(inplace)    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (3): ReLU(inplace)    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (6): ReLU(inplace)    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (8): ReLU(inplace)    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (11): ReLU(inplace)    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (13): ReLU(inplace)    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (15): ReLU(inplace)    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (18): ReLU(inplace)    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (20): ReLU(inplace)    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (22): ReLU(inplace)    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (25): ReLU(inplace)    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (27): ReLU(inplace)    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (29): ReLU(inplace)    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (classifier): Sequential(    (0): Linear(in_features=25088, out_features=4096, bias=True)    (1): ReLU(inplace)    (2): Dropout(p=0.5)    (3): Linear(in_features=4096, out_features=4096, bias=True)    (4): ReLU(inplace)    (5): Dropout(p=0.5)    (6): Linear(in_features=4096, out_features=1000, bias=True)  ))40961000(IMPLEMENTATION) Model ArchitectureUse transfer learning to create a CNN to classify dog breed.  Use the code cell below, and save your initialized model as the variable model_transfer.import torchvision.models as modelsimport torch.nn as nn## TODO: Specify model architecture model_transfer = models.vgg16(pretrained=True)# Freeze training for all \"features\" layersfor param in model_transfer.features.parameters():    param.requires_grad = Falsen_inputs = model_transfer.classifier[6].in_featureslast_layer = nn.Linear(n_inputs, 133)model_transfer.classifier[6] = last_layerif use_cuda:    model_transfer = model_transfer.cuda()Question 5: Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem.Answer:  VGG16 is trained on the ImageNet dataset and it can easily capture the features of our dog images.  First step was to freeze all the layers except classifier.  Now changing the output of the last dense layer to 133 makes the model ready for testing.(IMPLEMENTATION) Specify Loss Function and OptimizerUse the next code cell to specify a loss function and optimizer.  Save the chosen loss function as criterion_transfer, and the optimizer as optimizer_transfer below.criterion_transfer = nn.CrossEntropyLoss()optimizer_transfer = optim.SGD(model_transfer.classifier.parameters(), lr=0.001)(IMPLEMENTATION) Train and Validate the ModelTrain and validate your model in the code cell below.  Save the final model parameters at filepath 'model_transfer.pt'.n_epochs = 15# train the modelmodel_transfer = train(n_epochs, loaders_transfer, model_transfer, optimizer_transfer, criterion_transfer, use_cuda, 'model_transfer.pt')# load the model that got the best validation accuracy (uncomment the line below)model_transfer.load_state_dict(torch.load('model_transfer.pt'))Epoch: 1 \tTraining Loss: 4.395283 \tValidation Loss: 3.219059Validation loss decreased (inf --&gt; 3.219059).  Saving model ...Epoch: 2 \tTraining Loss: 3.086744 \tValidation Loss: 1.650656Validation loss decreased (3.219059 --&gt; 1.650656).  Saving model ...Epoch: 3 \tTraining Loss: 2.150576 \tValidation Loss: 0.968228Validation loss decreased (1.650656 --&gt; 0.968228).  Saving model ...Epoch: 4 \tTraining Loss: 1.715927 \tValidation Loss: 0.711452Validation loss decreased (0.968228 --&gt; 0.711452).  Saving model ...Epoch: 5 \tTraining Loss: 1.492721 \tValidation Loss: 0.612683Validation loss decreased (0.711452 --&gt; 0.612683).  Saving model ...Epoch: 6 \tTraining Loss: 1.391120 \tValidation Loss: 0.539134Validation loss decreased (0.612683 --&gt; 0.539134).  Saving model ...Epoch: 7 \tTraining Loss: 1.288818 \tValidation Loss: 0.503064Validation loss decreased (0.539134 --&gt; 0.503064).  Saving model ...Epoch: 8 \tTraining Loss: 1.234156 \tValidation Loss: 0.470367Validation loss decreased (0.503064 --&gt; 0.470367).  Saving model ...Epoch: 9 \tTraining Loss: 1.194840 \tValidation Loss: 0.455888Validation loss decreased (0.470367 --&gt; 0.455888).  Saving model ...Epoch: 10 \tTraining Loss: 1.144792 \tValidation Loss: 0.439842Validation loss decreased (0.455888 --&gt; 0.439842).  Saving model ...Epoch: 11 \tTraining Loss: 1.132506 \tValidation Loss: 0.430309Validation loss decreased (0.439842 --&gt; 0.430309).  Saving model ...Epoch: 12 \tTraining Loss: 1.087484 \tValidation Loss: 0.421268Validation loss decreased (0.430309 --&gt; 0.421268).  Saving model ...Epoch: 13 \tTraining Loss: 1.072485 \tValidation Loss: 0.416770Validation loss decreased (0.421268 --&gt; 0.416770).  Saving model ...Epoch: 14 \tTraining Loss: 1.027749 \tValidation Loss: 0.408649Validation loss decreased (0.416770 --&gt; 0.408649).  Saving model ...Epoch: 15 \tTraining Loss: 1.005084 \tValidation Loss: 0.391689Validation loss decreased (0.408649 --&gt; 0.391689).  Saving model ...(IMPLEMENTATION) Test the ModelTry out your model on the test dataset of dog images. Use the code cell below to calculate and print the test loss and accuracy.  Ensure that your test accuracy is greater than 60%.test(loaders_transfer, model_transfer, criterion_transfer, use_cuda)Test Loss: 0.439616Test Accuracy: 86% (725/836)(IMPLEMENTATION) Predict Dog Breed with the ModelWrite a function that takes an image path as input and returns the dog breed (Affenpinscher, Afghan hound, etc) that is predicted by your model.### TODO: Write a function that takes a path to an image as input### and returns the dog breed that is predicted by the model.# list of class names by index, i.e. a name can be accessed like class_names[0]class_names = [item[4:].replace(\"_\", \" \") for item in dataset['train_data'].classes]def predict_breed_transfer(img_path):    # load the image and return the predicted breed    image = Image.open(img_path).convert('RGB')        data_transform = transforms.Compose([transforms.Resize(224),                                         transforms.CenterCrop(224),                                         transforms.ToTensor(),                                         transforms.Normalize([0.485, 0.456, 0.406],                                                              [0.229, 0.224, 0.225])])        image = data_transform(image).unsqueeze(0)    if use_cuda:        image = image.cuda()    predict = model_transfer(image)    predict = predict.data.cpu().argmax()        return class_names[predict]Step 5: Write your AlgorithmWrite an algorithm that accepts a file path to an image and first determines whether the image contains a human, dog, or neither.  Then,  if a dog is detected in the image, return the predicted breed.  if a human is detected in the image, return the resembling dog breed.  if neither is detected in the image, provide output that indicates an error.You are welcome to write your own functions for detecting humans and dogs in images, but feel free to use the face_detector and human_detector functions developed above.  You are required to use your CNN from Step 4 to predict dog breed.Some sample output for our algorithm is provided below, but feel free to design your own user experience!(IMPLEMENTATION) Write your Algorithm### TODO: Write your algorithm.### Feel free to use as many code cells as needed.def display_image(img_path):    img = Image.open(img_path)    plt.imshow(img)    plt.show()def run_app(img_path):            # load and transform image    image = Image.open(img_path).convert('RGB')        data_transform = transforms.Compose([transforms.Resize(224),                                         transforms.CenterCrop(224),                                         transforms.ToTensor(),                                         transforms.Normalize([0.485, 0.456, 0.406],                                                              [0.229, 0.224, 0.225])])        image = data_transform(image).unsqueeze(0)        ## handle cases for a human face, dog, and neither    breed_pred = predict_breed_transfer(img_path)    if dog_detector(img_path):        print ('\\n\\n\\n  Dog Detected')        display_image(img_path)        return print ('The Predicted Breed:', breed_pred)            elif face_detector(img_path):        print ('\\n\\n\\n Human Detected')        display_image(img_path)        return print ('Closest Dog Breed:', breed_pred)        else:        display_image(img_path)        print ('\\n\\n\\n Error: Neither Dog nor Human Detected')Step 6: Test Your AlgorithmIn this section, you will take your new algorithm for a spin!  What kind of dog does the algorithm think that you look like?  If you have a dog, does it predict your dog’s breed accurately?  If you have a cat, does it mistakenly think that your cat is a dog?(IMPLEMENTATION) Test Your Algorithm on Sample Images!Test your algorithm at least six images on your computer.  Feel free to use any images you like.  Use at least two human and two dog images.Question 6: Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm.Answer:  The ouput is just as expected which correctly classifies dog and somewhat classifies the closest remsemblance to a dog for a human.  One thing that surprised me while testing on custom images was that the model worked even with random noise in the test images and gave almost correct results.  Training on more data and augmenting this data can improve the accuracy of our model.  Increasing the number of epochs can also improve the performance.## TODO: Execute your algorithm from Step 6 on## at least 6 images on your computer.## Feel free to use as many code cells as needed.## suggested code, belowfor file in np.hstack((human_files[:3], dog_files[:3])):    run_app(file) Human DetectedClosest Dog Breed: American staffordshire terrier Human DetectedClosest Dog Breed: Dachshund Human DetectedClosest Dog Breed: Cocker spaniel  Dog DetectedThe Predicted Breed: Bullmastiff  Dog DetectedThe Predicted Breed: Mastiff  Dog DetectedThe Predicted Breed: Mastiffcustom_img = glob(\"./images/*\")for file in np.hstack((custom_img)):    run_app(file)  Dog DetectedThe Predicted Breed: Irish red and white setter Human DetectedClosest Dog Breed: Cocker spaniel  Dog DetectedThe Predicted Breed: Labrador retriever  Dog DetectedThe Predicted Breed: Curly-coated retriever Error: Neither Dog nor Human Detected  Dog DetectedThe Predicted Breed: Brittany  Dog DetectedThe Predicted Breed: Labrador retriever  Dog DetectedThe Predicted Breed: American water spaniel  Dog DetectedThe Predicted Breed: Greyhound  Dog DetectedThe Predicted Breed: Labrador retriever",
        "url": "/deep%20learning/2020/06/04/dog-breed-classifier-app/",
        "date": "Jun 04, 2020"
      }
      ,
    
      "deep-20learning-2020-05-30-predicting-bike-sharing-patterns": {
        "title": "Predicting Bike Sharing Patterns",
        "author": "",
        "category": "",
        "content": "Your first neural networkWhy We’re HereIn this project, you’ll build your first neural network and use it to predict daily bike rental ridership. We’ve provided some of the code, but left the implementation of the neural network up to you (for the most part). After you’ve submitted this project, feel free to explore the data and the model more.%matplotlib inline%load_ext autoreload%autoreload 2%config InlineBackend.figure_format = 'retina'import numpy as npimport pandas as pdimport matplotlib.pyplot as pltLoad and prepare the data  A critical step in working with neural networks is preparing the data correctly. Variables on different scales make it difficult for the network to efficiently learn the correct weights. Below, we’ve written the code to load and prepare the data. You’ll learn more about this soon!data_path = 'Bike-Sharing-Dataset/hour.csv'rides = pd.read_csv(data_path)rides.head()                  instant      dteday      season      yr      mnth      hr      holiday      weekday      workingday      weathersit      temp      atemp      hum      windspeed      casual      registered      cnt                  0      1      2011-01-01      1      0      1      0      0      6      0      1      0.24      0.2879      0.81      0.0      3      13      16              1      2      2011-01-01      1      0      1      1      0      6      0      1      0.22      0.2727      0.80      0.0      8      32      40              2      3      2011-01-01      1      0      1      2      0      6      0      1      0.22      0.2727      0.80      0.0      5      27      32              3      4      2011-01-01      1      0      1      3      0      6      0      1      0.24      0.2879      0.75      0.0      3      10      13              4      5      2011-01-01      1      0      1      4      0      6      0      1      0.24      0.2879      0.75      0.0      0      1      1      Checking out the dataThis dataset has the number of riders for each hour of each day from January 1 2011 to December 31 2012. The number of riders is split between casual and registered, summed up in the cnt column. You can see the first few rows of the data above.Below is a plot showing the number of bike riders over the first 10 days or so in the data set. (Some days don’t have exactly 24 entries in the data set, so it’s not exactly 10 days.) You can see the hourly rentals here. This data is pretty complicated! The weekends have lower over all ridership and there are spikes when people are biking to and from work during the week. Looking at the data above, we also have information about temperature, humidity, and windspeed, all of these likely affecting the number of riders. You’ll be trying to capture all this with your model.rides[:24*10].plot(x='dteday', y='cnt')&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7c6fe39f28&gt;Dummy variablesHere we have some categorical variables like season, weather, month. To include these in our model, we’ll need to make binary dummy variables. This is simple to do with Pandas thanks to get_dummies().dummy_fields = ['season', 'weathersit', 'mnth', 'hr', 'weekday']for each in dummy_fields:    dummies = pd.get_dummies(rides[each], prefix=each, drop_first=False)    rides = pd.concat([rides, dummies], axis=1)fields_to_drop = ['instant', 'dteday', 'season', 'weathersit',                   'weekday', 'atemp', 'mnth', 'workingday', 'hr']data = rides.drop(fields_to_drop, axis=1)data.head()                  yr      holiday      temp      hum      windspeed      casual      registered      cnt      season_1      season_2      ...      hr_21      hr_22      hr_23      weekday_0      weekday_1      weekday_2      weekday_3      weekday_4      weekday_5      weekday_6                  0      0      0      0.24      0.81      0.0      3      13      16      1      0      ...      0      0      0      0      0      0      0      0      0      1              1      0      0      0.22      0.80      0.0      8      32      40      1      0      ...      0      0      0      0      0      0      0      0      0      1              2      0      0      0.22      0.80      0.0      5      27      32      1      0      ...      0      0      0      0      0      0      0      0      0      1              3      0      0      0.24      0.75      0.0      3      10      13      1      0      ...      0      0      0      0      0      0      0      0      0      1              4      0      0      0.24      0.75      0.0      0      1      1      1      0      ...      0      0      0      0      0      0      0      0      0      1      5 rows × 59 columnsScaling target variablesTo make training the network easier, we’ll standardize each of the continuous variables. That is, we’ll shift and scale the variables such that they have zero mean and a standard deviation of 1.The scaling factors are saved so we can go backwards when we use the network for predictions.quant_features = ['casual', 'registered', 'cnt', 'temp', 'hum', 'windspeed']# Store scalings in a dictionary so we can convert back laterscaled_features = {}for each in quant_features:    mean, std = data[each].mean(), data[each].std()    scaled_features[each] = [mean, std]    data.loc[:, each] = (data[each] - mean)/stdSplitting the data into training, testing, and validation setsWe’ll save the data for the last approximately 21 days to use as a test set after we’ve trained the network. We’ll use this set to make predictions and compare them with the actual number of riders.# Save data for approximately the last 21 days test_data = data[-21*24:]# Now remove the test data from the data set data = data[:-21*24]# Separate the data into features and targetstarget_fields = ['cnt', 'casual', 'registered']features, targets = data.drop(target_fields, axis=1), data[target_fields]test_features, test_targets = test_data.drop(target_fields, axis=1), test_data[target_fields]We’ll split the data into two sets, one for training and one for validating as the network is being trained. Since this is time series data, we’ll train on historical data, then try to predict on future data (the validation set).# Hold out the last 60 days or so of the remaining data as a validation settrain_features, train_targets = features[:-60*24], targets[:-60*24]val_features, val_targets = features[-60*24:], targets[-60*24:]Time to build the networkBelow you’ll build your network. We’ve built out the structure. You’ll implement both the forward pass and backwards pass through the network. You’ll also set the hyperparameters: the learning rate, the number of hidden units, and the number of training passes.The network has two layers, a hidden layer and an output layer. The hidden layer will use the sigmoid function for activations. The output layer has only one node and is used for the regression, the output of the node is the same as the input of the node. That is, the activation function is $f(x)=x$. A function that takes the input signal and generates an output signal, but takes into account the threshold, is called an activation function. We work through each layer of our network calculating the outputs for each neuron. All of the outputs from one layer become inputs to the neurons on the next layer. This process is called forward propagation.We use the weights to propagate signals forward from the input to the output layers in a neural network. We use the weights to also propagate error backwards from the output back into the network to update our weights. This is called backpropagation.  Hint: You’ll need the derivative of the output activation function ($f(x) = x$) for the backpropagation implementation. If you aren’t familiar with calculus, this function is equivalent to the equation $y = x$. What is the slope of that equation? That is the derivative of $f(x)$.Below, you have these tasks:  Implement the sigmoid function to use as the activation function. Set self.activation_function in __init__ to your sigmoid function.  Implement the forward pass in the train method.  Implement the backpropagation algorithm in the train method, including calculating the output error.  Implement the forward pass in the run method.############## In the my_answers.py file, fill out the TODO sections as specified#############from my_answers import NeuralNetworkdef MSE(y, Y):    return np.mean((y-Y)**2)Unit testsRun these unit tests to check the correctness of your network implementation. This will help you be sure your network was implemented correctly befor you starting trying to train it. These tests must all be successful to pass the project.import unittestinputs = np.array([[0.5, -0.2, 0.1]])targets = np.array([[0.4]])test_w_i_h = np.array([[0.1, -0.2],                       [0.4, 0.5],                       [-0.3, 0.2]])test_w_h_o = np.array([[0.3],                       [-0.1]])class TestMethods(unittest.TestCase):        ##########    # Unit tests for data loading    ##########        def test_data_path(self):        # Test that file path to dataset has been unaltered        self.assertTrue(data_path.lower() == 'bike-sharing-dataset/hour.csv')            def test_data_loaded(self):        # Test that data frame loaded        self.assertTrue(isinstance(rides, pd.DataFrame))        ##########    # Unit tests for network functionality    ##########    def test_activation(self):        network = NeuralNetwork(3, 2, 1, 0.5)        # Test that the activation function is a sigmoid        self.assertTrue(np.all(network.activation_function(0.5) == 1/(1+np.exp(-0.5))))    def test_train(self):        # Test that weights are updated correctly on training        network = NeuralNetwork(3, 2, 1, 0.5)        network.weights_input_to_hidden = test_w_i_h.copy()        network.weights_hidden_to_output = test_w_h_o.copy()                network.train(inputs, targets)        self.assertTrue(np.allclose(network.weights_hidden_to_output,                                     np.array([[ 0.37275328],                                               [-0.03172939]])))        self.assertTrue(np.allclose(network.weights_input_to_hidden,                                    np.array([[ 0.10562014, -0.20185996],                                               [0.39775194, 0.50074398],                                               [-0.29887597, 0.19962801]])))    def test_run(self):        # Test correctness of run method        network = NeuralNetwork(3, 2, 1, 0.5)        network.weights_input_to_hidden = test_w_i_h.copy()        network.weights_hidden_to_output = test_w_h_o.copy()        self.assertTrue(np.allclose(network.run(inputs), 0.09998924))suite = unittest.TestLoader().loadTestsFromModule(TestMethods())unittest.TextTestRunner().run(suite).....----------------------------------------------------------------------Ran 5 tests in 0.012sOK&lt;unittest.runner.TextTestResult run=5 errors=0 failures=0&gt;Training the networkHere you’ll set the hyperparameters for the network. The strategy here is to find hyperparameters such that the error on the training set is low, but you’re not overfitting to the data. If you train the network too long or have too many hidden nodes, it can become overly specific to the training set and will fail to generalize to the validation set. That is, the loss on the validation set will start increasing as the training set loss drops.You’ll also be using a method know as Stochastic Gradient Descent (SGD) to train the network. The idea is that for each training pass, you grab a random sample of the data instead of using the whole data set. You use many more training passes than with normal gradient descent, but each pass is much faster. This ends up training the network more efficiently. You’ll learn more about SGD later.Choose the number of iterationsThis is the number of batches of samples from the training data we’ll use to train the network. The more iterations you use, the better the model will fit the data. However, this process can have sharply diminishing returns and can waste computational resources if you use too many iterations.  You want to find a number here where the network has a low training loss, and the validation loss is at a minimum. The ideal number of iterations would be a level that stops shortly after the validation loss is no longer decreasing.Choose the learning rateThis scales the size of weight updates. If this is too big, the weights tend to explode and the network fails to fit the data. Normally a good choice to start at is 0.1; however, if you effectively divide the learning rate by n_records, try starting out with a learning rate of 1. In either case, if the network has problems fitting the data, try reducing the learning rate. Note that the lower the learning rate, the smaller the steps are in the weight updates and the longer it takes for the neural network to converge.Choose the number of hidden nodesIn a model where all the weights are optimized, the more hidden nodes you have, the more accurate the predictions of the model will be.  (A fully optimized model could have weights of zero, after all.) However, the more hidden nodes you have, the harder it will be to optimize the weights of the model, and the more likely it will be that suboptimal weights will lead to overfitting. With overfitting, the model will memorize the training data instead of learning the true pattern, and won’t generalize well to unseen data.Try a few different numbers and see how it affects the performance. You can look at the losses dictionary for a metric of the network performance. If the number of hidden units is too low, then the model won’t have enough space to learn and if it is too high there are too many options for the direction that the learning can take. The trick here is to find the right balance in number of hidden units you choose.  You’ll generally find that the best number of hidden nodes to use ends up being between the number of input and output nodes.import sys####################### Set the hyperparameters in you myanswers.py file #######################from my_answers import iterations, learning_rate, hidden_nodes, output_nodesN_i = train_features.shape[1]network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)losses = {'train':[], 'validation':[]}for ii in range(iterations):    # Go through a random batch of 128 records from the training data set    batch = np.random.choice(train_features.index, size=128)    X, y = train_features.ix[batch].values, train_targets.ix[batch]['cnt']                                 network.train(X, y)        # Printing out the training progress    train_loss = MSE(network.run(train_features).T, train_targets['cnt'].values)    val_loss = MSE(network.run(val_features).T, val_targets['cnt'].values)    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * ii/float(iterations)) \\                     + \"% ... Training loss: \" + str(train_loss)[:5] \\                     + \" ... Validation loss: \" + str(val_loss)[:5])    sys.stdout.flush()        losses['train'].append(train_loss)    losses['validation'].append(val_loss)Progress: 0.1% ... Training loss: 71.64 ... Validation loss: 18.54/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: .ix is deprecated. Please use.loc for label based indexing or.iloc for positional indexingSee the documentation here:http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecatedProgress: 100.0% ... Training loss: 0.076 ... Validation loss: 0.160plt.plot(losses['train'], label='Training loss')plt.plot(losses['validation'], label='Validation loss')plt.legend()_ = plt.ylim()Check out your predictionsHere, use the test data to view how well your network is modeling the data. If something is completely wrong here, make sure each step in your network is implemented correctly.fig, ax = plt.subplots(figsize=(8,4))mean, std = scaled_features['cnt']predictions = network.run(test_features).T*std + meanax.plot(predictions[0], label='Prediction')ax.plot((test_targets['cnt']*std + mean).values, label='Data')ax.set_xlim(right=len(predictions))ax.legend()dates = pd.to_datetime(rides.ix[test_data.index]['dteday'])dates = dates.apply(lambda d: d.strftime('%b %d'))ax.set_xticks(np.arange(len(dates))[12::24])_ = ax.set_xticklabels(dates[12::24], rotation=45)/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: .ix is deprecated. Please use.loc for label based indexing or.iloc for positional indexingSee the documentation here:http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated  # Remove the CWD from sys.path while we load stuff.",
        "url": "/deep%20learning/2020/05/30/predicting-bike-sharing-patterns/",
        "date": "May 30, 2020"
      }
      ,
    
      "food-2019-05-14-charming-evening-field": {
        "title": "Charming Evening Field",
        "author": "",
        "category": "",
        "content": "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.  Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt utlabore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi utaliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillumLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.",
        "url": "/food/2019/05/14/charming-Evening-Field/",
        "date": "May 14, 2019"
      }
      ,
    
      "food-2019-05-14-charming-evening-field-1": {
        "title": "Charming Evening Field",
        "author": "",
        "category": "",
        "content": "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.  Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt utlabore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi utaliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillumLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.",
        "url": "/food/2019/05/14/charming-Evening-Field.1/",
        "date": "May 14, 2019"
      }
      ,
    
      "food-2019-05-14-charming-evening-field-2": {
        "title": "Charming Evening Field",
        "author": "",
        "category": "",
        "content": "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.  Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt utlabore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi utaliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillumLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.",
        "url": "/food/2019/05/14/charming-Evening-Field.2/",
        "date": "May 14, 2019"
      }
      ,
    
      "food-2019-05-14-charming-evening-field-3": {
        "title": "Charming Evening Field",
        "author": "",
        "category": "",
        "content": "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.  Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt utlabore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi utaliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillumLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.",
        "url": "/food/2019/05/14/charming-Evening-Field.3/",
        "date": "May 14, 2019"
      }
      ,
    
      "food-2019-05-14-charming-evening-field-4": {
        "title": "Charming Evening Field",
        "author": "",
        "category": "",
        "content": "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.  Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt utlabore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi utaliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillumLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.",
        "url": "/food/2019/05/14/charming-Evening-Field.4/",
        "date": "May 14, 2019"
      }
      ,
    
      "food-2019-05-14-charming-evening-field-5": {
        "title": "Charming Evening Field",
        "author": "",
        "category": "",
        "content": "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.  Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt utlabore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi utaliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillumLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.",
        "url": "/food/2019/05/14/charming-Evening-Field.5/",
        "date": "May 14, 2019"
      }
      ,
    
      "food-2019-05-14-charming-evening-field-6": {
        "title": "Charming Evening Field",
        "author": "",
        "category": "",
        "content": "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.  Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt utlabore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi utaliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillumLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.",
        "url": "/food/2019/05/14/charming-Evening-Field.6/",
        "date": "May 14, 2019"
      }
      ,
    
      "food-2019-05-14-charming-evening-field-7": {
        "title": "Charming Evening Field",
        "author": "",
        "category": "",
        "content": "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.  Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt utlabore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi utaliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillumLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.",
        "url": "/food/2019/05/14/charming-Evening-Field.7/",
        "date": "May 14, 2019"
      }
      ,
    
      "food-2019-05-14-charming-evening-field-8": {
        "title": "Charming Evening Field",
        "author": "",
        "category": "",
        "content": "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.  Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt utlabore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi utaliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillumLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.",
        "url": "/food/2019/05/14/charming-Evening-Field.8/",
        "date": "May 14, 2019"
      }
      ,
    
      "food-2019-05-14-charming-evening-field-9": {
        "title": "Charming Evening Field",
        "author": "",
        "category": "",
        "content": "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.  Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt utlabore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi utaliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillumLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.",
        "url": "/food/2019/05/14/charming-Evening-Field.9/",
        "date": "May 14, 2019"
      }
      ,
    
      "food-2019-05-14-charming-evening-field-10": {
        "title": "Charming Evening Field",
        "author": "",
        "category": "",
        "content": "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.  Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt utlabore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi utaliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillumLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.",
        "url": "/food/2019/05/14/charming-Evening-Field.10/",
        "date": "May 14, 2019"
      }
      ,
    
      "food-2019-05-14-charming-evening-field-12": {
        "title": "Charming Evening Field",
        "author": "",
        "category": "",
        "content": "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.  Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt utlabore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi utaliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillumLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.",
        "url": "/food/2019/05/14/charming-Evening-Field.12/",
        "date": "May 14, 2019"
      }
      ,
    
      "food-2019-05-14-charming-evening-field-11": {
        "title": "Charming Evening Field",
        "author": "",
        "category": "",
        "content": "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.  Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt utlabore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi utaliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillumLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.",
        "url": "/food/2019/05/14/charming-Evening-Field.11/",
        "date": "May 14, 2019"
      }
      ,
    
      "lifestyle-2019-04-14-organize-your-life": {
        "title": "Organize Your Life With 10 Simple rule",
        "author": "",
        "category": "",
        "content": "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.  Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt utlabore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi utaliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillumLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.",
        "url": "/lifestyle/2019/04/14/organize-Your-Life/",
        "date": "Apr 14, 2019"
      }
      ,
    
      "philosophy-2019-04-14-organize-your-life-1": {
        "title": "Organize Your Life With 10 Simple rule",
        "author": "",
        "category": "",
        "content": "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.  Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt utlabore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi utaliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillumLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.",
        "url": "/philosophy/2019/04/14/organize-Your-Life.1/",
        "date": "Apr 14, 2019"
      }
      ,
    
      "fashion-2019-04-14-organize-your-life-2": {
        "title": "Organize Your Life With 10 Simple rule",
        "author": "",
        "category": "",
        "content": "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.  Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt utlabore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi utaliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillumLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.",
        "url": "/fashion/2019/04/14/organize-Your-Life.2/",
        "date": "Apr 14, 2019"
      }
      ,
    
      "article-2019-04-14-organize-your-life-3": {
        "title": "Organize Your Life With 10 Simple rule",
        "author": "",
        "category": "",
        "content": "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.  Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt utlabore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi utaliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillumLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.",
        "url": "/article/2019/04/14/organize-Your-Life.3/",
        "date": "Apr 14, 2019"
      }
      ,
    
      "nature-2019-04-14-organize-your-life-4": {
        "title": "Organize Your Life With 10 Simple rule",
        "author": "",
        "category": "",
        "content": "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.  Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt utlabore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi utaliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillumLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore etdolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip exea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiatnulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollitanim id est laborum. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremquelaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitaedicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quiaconsequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui doloremipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt utlabore et dolore magnam aliquam quaerat voluptatem.",
        "url": "/nature/2019/04/14/organize-Your-Life.4/",
        "date": "Apr 14, 2019"
      }
      
    
  };
</script>

  <footer class="bg-secondary">
  <div class="section">
    <div class="container">
      <div class="row">
        
        <div class="col-md-3 col-sm-6 mb-4 mb-md-0">
          <a href="/"><img src="/assets/images/s-logo.png" alt="Shrikant Naidu" class="img-fluid"></a>
        </div>
        
        
        <div class="col-md-3 col-sm-6 mb-4 mb-md-0">
          <h6>Address</h6>
          <ul class="list-unstyled">
            <li class="font-secondary text-dark">Mumbai</li>
            <li class="font-secondary text-dark"></li>
          </ul>
        </div>
        
        
        <div class="col-md-3 col-sm-6 mb-4 mb-md-0">
          <h6>Contact Info</h6>
          <ul class="list-unstyled">
            <li class="font-secondary text-dark">Tel: </li>
            <li class="font-secondary text-dark">Mail: shrikantnaidu777@gmail.com</li>
          </ul>
        </div>
        
        
        <div class="col-md-3 col-sm-6 mb-4 mb-md-0">
          <h6>Follow</h6>
          <ul class="list-inline d-inline-block">
            
            <li class="list-inline-item"><a href="https://www.linkedin.com/in/shrikant-naidu/" class="text-dark"><i class="ti-linkedin"></i></a></li>
            
            <li class="list-inline-item"><a href="https://github.com/shrikantnaidu" class="text-dark"><i class="ti-github"></i></a></li>
            
            <li class="list-inline-item"><a href="https://twitter.com/sk_dataholic" class="text-dark"><i class="ti-twitter-alt"></i></a></li>
            
            <li class="list-inline-item"><a href="https://www.instagram.com/sk.barcaholic/" class="text-dark"><i class="ti-instagram"></i></a></li>
            
          </ul>
        </div>
        
      </div>
    </div>
  </div>
  <div class="text-center pb-3"><p>Copyright 2023 Designed by <a href="https://themefisher.com">Themefisher</a> &amp; Developed by <a href="https://gethugothemes.com">Gethugothemes</a></p>
</div>
</footer>

  <!-- jQuery -->
  <script src="/assets/plugins/jQuery/jquery.min.js"></script>
  <!-- Bootstrap JS -->
  <script src="/assets/plugins/bootstrap/bootstrap.min.js"></script>
  <!-- slick slider -->
  <script src="/assets/plugins/slick/slick.min.js"></script>
  <!-- masonry -->
  <script src="/assets/plugins/masonry/masonry.js"></script>
  <!-- instafeed -->
  <script src="/assets/plugins/instafeed/instafeed.min.js"></script>
  <!-- smooth scroll -->
  <script src="/assets/plugins/smooth-scroll/smooth-scroll.js"></script>
  <!-- headroom -->
  <script src="/assets/plugins/headroom/headroom.js"></script>
  <!-- reading time -->
  <script src="/assets/plugins/reading-time/readingTime.min.js"></script>
  <!-- lunr.js -->
  <script src="/assets/plugins/search/lunr.min.js"></script>
  <!-- search -->
  <script src="/assets/plugins/search/search.js"></script>

  <!-- Main Script -->
  <script src="/assets/js/script.js"></script>
</body>

</html>